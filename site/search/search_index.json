{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"WildDetect Monorepo Documentation","text":"<p>Welcome to the comprehensive documentation for the WildDetect monorepo - a complete wildlife monitoring and conservation toolkit for aerial imagery analysis.</p>"},{"location":"#what-is-wilddetect","title":"What is WildDetect?","text":"<p>WildDetect is an integrated ecosystem of three specialized packages designed to streamline the entire wildlife detection workflow, from data management to model training and deployment:</p> <ul> <li>WilData - Data pipeline and management</li> <li>WildTrain - Model training and evaluation  </li> <li>WildDetect - Detection deployment and census analysis</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#complete-wildlife-detection-pipeline","title":"\ud83c\udfaf Complete Wildlife Detection Pipeline","text":"<ul> <li>Multi-species detection using state-of-the-art YOLO models</li> <li>Batch processing of large-scale aerial imagery</li> <li>Automated census campaigns with population statistics</li> <li>Geographic visualization and analysis</li> </ul>"},{"location":"#comprehensive-data-management","title":"\ud83d\udce6 Comprehensive Data Management","text":"<ul> <li>Import from COCO, YOLO, and Label Studio formats</li> <li>Data transformations (tiling, augmentation, filtering)</li> <li>DVC integration for dataset versioning</li> <li>ROI extraction for hard sample mining</li> </ul>"},{"location":"#flexible-training-framework","title":"\ud83e\udde0 Flexible Training Framework","text":"<ul> <li>Support for YOLO and MMDetection frameworks</li> <li>Classification and object detection training</li> <li>MLflow experiment tracking</li> <li>Hyperparameter optimization with Optuna</li> </ul>"},{"location":"#geographic-analysis","title":"\ud83c\udf0d Geographic Analysis","text":"<ul> <li>GPS metadata extraction and management</li> <li>Flight path analysis and coverage maps</li> <li>Interactive visualizations with FiftyOne</li> <li>Population density and distribution analysis</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#for-new-users","title":"For New Users","text":"<p>Start here to get up and running quickly:</p> <ul> <li>Installation Guide - Install all packages</li> <li>Quick Start - Your first detection</li> <li>Environment Setup - Configure your environment</li> </ul>"},{"location":"#for-researchers-conservationists","title":"For Researchers &amp; Conservationists","text":"<p>Learn how to use the tools for your wildlife monitoring needs:</p> <ul> <li>End-to-End Detection Tutorial - Complete workflow</li> <li>Census Campaign Guide - Run a census campaign</li> <li>Scripts Reference - Available scripts and tools</li> </ul>"},{"location":"#for-developers","title":"For Developers","text":"<p>Understand the architecture and extend the toolkit:</p> <ul> <li>Architecture Overview - System design and components</li> <li>Python API Reference - Programmatic usage</li> <li>Data Flow - How data moves through the system</li> </ul>"},{"location":"#for-data-scientists-ml-engineers","title":"For Data Scientists &amp; ML Engineers","text":"<p>Prepare datasets and train models:</p> <ul> <li>Dataset Preparation - Data pipeline tutorial</li> <li>Model Training - Train custom models</li> <li>Configuration Reference - Configuration files</li> </ul>"},{"location":"#package-overview","title":"Package Overview","text":""},{"location":"#wildata-data-pipeline","title":"\ud83d\uddc2\ufe0f WilData - Data Pipeline","text":"<p>The foundation for dataset management and preparation.</p> <p>Key Capabilities: - Import datasets from multiple formats (COCO, YOLO, Label Studio) - Apply transformations (tiling, augmentation, bbox clipping) - Create ROI datasets for classification - Update GPS metadata from CSV files - DVC integration for version control - REST API for programmatic access</p> <p>Learn more about WilData \u2192</p>"},{"location":"#wildtrain-training-framework","title":"\ud83c\udf93 WildTrain - Training Framework","text":"<p>Modular training system for detection and classification models.</p> <p>Key Capabilities: - YOLO and MMDetection framework support - PyTorch Lightning for classification - Hydra configuration management - MLflow experiment tracking - Model registration and versioning - Hyperparameter tuning</p> <p>Learn more about WildTrain \u2192</p>"},{"location":"#wilddetect-detection-analysis","title":"\ud83d\udd0d WildDetect - Detection &amp; Analysis","text":"<p>Production-ready detection and census system.</p> <p>Key Capabilities: - Multi-threaded detection pipelines - Raster (large image) detection support - Census campaign orchestration - Geographic analysis and visualization - FiftyOne integration - Comprehensive reporting (JSON, CSV)</p> <p>Learn more about WildDetect \u2192</p>"},{"location":"#common-workflows","title":"Common Workflows","text":""},{"location":"#detection-workflow","title":"Detection Workflow","text":"<pre><code>graph LR\n    A[Aerial Images] --&gt; B[WildDetect]\n    B --&gt; C[Detections]\n    C --&gt; D[Analysis]\n    D --&gt; E[Reports &amp; Maps]</code></pre>"},{"location":"#training-workflow","title":"Training Workflow","text":"<pre><code>graph LR\n    A[Annotations] --&gt; B[WilData]\n    B --&gt; C[Processed Dataset]\n    C --&gt; D[WildTrain]\n    D --&gt; E[Trained Model]\n    E --&gt; F[WildDetect]</code></pre>"},{"location":"#census-workflow","title":"Census Workflow","text":"<pre><code>graph LR\n    A[Flight Planning] --&gt; B[Image Capture]\n    B --&gt; C[WildDetect Census]\n    C --&gt; D[Statistics]\n    D --&gt; E[Geographic Viz]\n    E --&gt; F[Conservation Reports]</code></pre>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Tutorials: Step-by-step guides for common tasks</li> <li>API Reference: Complete command and function documentation</li> <li>Troubleshooting: Solutions to common issues</li> <li>GitHub Issues: Report bugs or request features</li> </ul>"},{"location":"#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.9 or higher</li> <li>GPU: CUDA-capable GPU recommended (optional)</li> <li>OS: Windows, Linux, macOS</li> <li>Memory: 16GB RAM minimum, 32GB recommended</li> <li>Storage: SSD recommended for large datasets</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! This is an open-source project designed for the conservation community.</p> <ul> <li>Submit bug reports and feature requests via GitHub Issues</li> <li>Contribute code via pull requests</li> <li>Share your use cases and results</li> <li>Help improve documentation</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE files in each package for details.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use WildDetect in your research, please cite:</p> <pre><code>@software{wildetect2025,\n  author = {Seydou, Fadel M.},\n  title = {WildDetect: Wildlife Detection and Census System for Aerial Imagery},\n  year = {2025},\n  url = {https://github.com/fadelmamar/wildetect}\n}\n</code></pre>"},{"location":"#support","title":"Support","text":"<p>For questions and support: - \ud83d\udce7 Email: [your-email@example.com] - \ud83d\udcac GitHub Discussions - \ud83d\udc1b GitHub Issues</p> <p>Ready to get started? Head to the Installation Guide to set up your environment.</p>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>Common issues and solutions for the WildDetect monorepo.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#uv-command-not-found","title":"uv command not found","text":"<p>Solution: <pre><code># Windows (PowerShell)\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Linux/macOS\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></p>"},{"location":"troubleshooting/#importerror-after-installation","title":"ImportError after installation","text":"<p>Solution: <pre><code># Ensure virtual environment is activated\n.venv\\Scripts\\activate  # Windows\nsource .venv/bin/activate  # Linux/macOS\n\n# Reinstall in development mode\ncd wildetect\nuv pip install -e .\n</code></pre></p>"},{"location":"troubleshooting/#gpu-and-cuda-issues","title":"GPU and CUDA Issues","text":""},{"location":"troubleshooting/#cuda-out-of-memory","title":"CUDA out of memory","text":"<p>Solutions: 1. Reduce batch size:    <pre><code>processing:\n  batch_size: 16  # Reduce from 32\n</code></pre></p> <ol> <li> <p>Reduce tile size:    <pre><code>processing:\n  tile_size: 640  # Reduce from 800\n</code></pre></p> </li> <li> <p>Clear GPU cache:    <pre><code>import torch\ntorch.cuda.empty_cache()\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#gpu-not-detected","title":"GPU not detected","text":"<p>Check CUDA: <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\n</code></pre></p> <p>Solutions: 1. Reinstall PyTorch with CUDA:    <pre><code>uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p> <ol> <li>Set device explicitly:    <pre><code>model:\n  device: \"cuda:0\"\n</code></pre></li> </ol>"},{"location":"troubleshooting/#windows-specific-issues","title":"Windows-Specific Issues","text":""},{"location":"troubleshooting/#processpool-not-supported","title":"ProcessPool not supported","text":"<p>Issue: <code>ProcessPoolExecutor</code> doesn't work on Windows</p> <p>Solution: Package automatically uses <code>ThreadPoolExecutor</code> on Windows</p>"},{"location":"troubleshooting/#path-issues","title":"Path issues","text":"<p>Use forward slashes or raw strings: <pre><code># Good\npath = \"D:/data/images\"\npath = r\"D:\\data\\images\"\n\n# Bad\npath = \"D:\\data\\images\"  # Backslashes can cause issues\n</code></pre></p>"},{"location":"troubleshooting/#mlflow-issues","title":"MLflow Issues","text":""},{"location":"troubleshooting/#cant-connect-to-mlflow-server","title":"Can't connect to MLflow server","text":"<p>Solutions: 1. Start MLflow server:    <pre><code>scripts\\launch_mlflow.bat\n</code></pre></p> <ol> <li> <p>Check environment variable:    <pre><code>echo %MLFLOW_TRACKING_URI%\n# Should be: http://localhost:5000\n</code></pre></p> </li> <li> <p>Set in <code>.env</code>:    <pre><code>MLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#model-not-found-in-registry","title":"Model not found in registry","text":"<p>Solutions: 1. List available models:    <pre><code>mlflow models list\n</code></pre></p> <ol> <li>Check model name and alias:    <pre><code>model:\n  mlflow_model_name: \"detector\"  # Check this is correct\n  mlflow_model_alias: \"production\"  # or version number\n</code></pre></li> </ol>"},{"location":"troubleshooting/#data-loading-issues","title":"Data Loading Issues","text":""},{"location":"troubleshooting/#images-not-found","title":"Images not found","text":"<p>Solutions: 1. Use absolute paths 2. Check file extensions match 3. Verify directory structure</p>"},{"location":"troubleshooting/#annotation-format-errors","title":"Annotation format errors","text":"<p>Solutions: 1. Validate COCO format:    <pre><code>from wildata.validation import validate_coco\nerrors = validate_coco(\"annotations.json\")\n</code></pre></p> <ol> <li>Check bbox coordinates</li> <li>Verify image IDs match</li> </ol>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#detection-is-slow","title":"Detection is slow","text":"<p>Solutions: 1. Use GPU if available 2. Increase batch size 3. Use multithreaded pipeline:    <pre><code>processing:\n  pipeline_type: \"multithreaded\"\n  num_data_workers: 4\n</code></pre></p>"},{"location":"troubleshooting/#high-memory-usage","title":"High memory usage","text":"<p>Solutions: 1. Enable streaming mode (WilData):    <pre><code>processing_mode: \"streaming\"\n</code></pre></p> <ol> <li>Process in smaller batches</li> <li>Clear cache between batches</li> </ol>"},{"location":"troubleshooting/#dvc-issues","title":"DVC Issues","text":""},{"location":"troubleshooting/#dvc-push-fails","title":"DVC push fails","text":"<p>Solutions: 1. Check remote configuration:    <pre><code>dvc remote list\n</code></pre></p> <ol> <li> <p>Verify credentials:    <pre><code>dvc remote modify myremote access_key_id YOUR_KEY\n</code></pre></p> </li> <li> <p>Test connection:    <pre><code>dvc remote list\ndvc status\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#label-studio-integration","title":"Label Studio Integration","text":""},{"location":"troubleshooting/#cant-connect-to-label-studio","title":"Can't connect to Label Studio","text":"<p>Solutions: 1. Start Label Studio:    <pre><code>scripts\\launch_labelstudio.bat\n</code></pre></p> <ol> <li>Check API key in <code>.env</code>:    <pre><code>LABEL_STUDIO_API_KEY=your_key\nLABEL_STUDIO_URL=http://localhost:8080\n</code></pre></li> </ol>"},{"location":"troubleshooting/#fiftyone-issues","title":"FiftyOne Issues","text":""},{"location":"troubleshooting/#fiftyone-app-wont-launch","title":"FiftyOne app won't launch","text":"<p>Solutions: 1. Check FiftyOne installation:    <pre><code>uv pip install fiftyone\n</code></pre></p> <ol> <li> <p>Clear FiftyOne database:    <pre><code>fiftyone app config database_dir\n</code></pre></p> </li> <li> <p>Use different port:    <pre><code>fiftyone app launch --port 5152\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#common-error-messages","title":"Common Error Messages","text":""},{"location":"troubleshooting/#no-module-named-wildetect","title":"\"No module named 'wildetect'\"","text":"<p>Solution: Install in development mode: <pre><code>cd wildetect\nuv pip install -e .\n</code></pre></p>"},{"location":"troubleshooting/#permission-denied","title":"\"Permission denied\"","text":"<p>Solution: Run as administrator or fix permissions: <pre><code>icacls \"D:\\data\" /grant %USERNAME%:F /t\n</code></pre></p>"},{"location":"troubleshooting/#port-already-in-use","title":"\"Port already in use\"","text":"<p>Solution: Kill process using port: <pre><code># Find process\nnetstat -ano | findstr :5000\n\n# Kill process (replace PID)\ntaskkill /PID &lt;PID&gt; /F\n</code></pre></p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<ol> <li>Check logs: Look in <code>logs/</code> directory</li> <li>Enable verbose mode: Add <code>--verbose</code> flag</li> <li>GitHub Issues: Report bugs</li> <li>Discussions: Ask questions in GitHub Discussions</li> </ol>"},{"location":"troubleshooting/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging:</p> <pre><code>logging:\n  log_level: \"DEBUG\"\n  verbose: true\n</code></pre> <p>Or in Python:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre> <p>Still having issues? Open an issue on GitHub with: - Error message - System info (<code>wildetect info</code>) - Steps to reproduce - Relevant configuration files</p>"},{"location":"api-reference/python-api/","title":"Python API Reference","text":"<p>Using WildDetect packages programmatically.</p>"},{"location":"api-reference/python-api/#wilddetect","title":"WildDetect","text":""},{"location":"api-reference/python-api/#detection-pipeline","title":"Detection Pipeline","text":"<pre><code>from wildetect.core.pipeline import DetectionPipeline\n\n# Initialize\npipeline = DetectionPipeline(\n    model_path=\"detector.pt\",\n    device=\"cuda\"\n)\n\n# Detect single image\nresult = pipeline.detect(\"image.jpg\")\n\n# Detect batch\nresults = pipeline.detect_batch(\"images/\")\n\n# Save results\npipeline.save_results(results, \"results.json\")\n</code></pre>"},{"location":"api-reference/python-api/#census-engine","title":"Census Engine","text":"<pre><code>from wildetect.core.census import CensusEngine, CensusConfig\n\n# Configure\nconfig = CensusConfig.from_yaml(\"config/census.yaml\")\n\n# Run census\nengine = CensusEngine(config)\ncensus_result = engine.run_census(\"survey_images/\")\n\n# Generate report\ncensus_result.save_report(\"report.pdf\")\n</code></pre>"},{"location":"api-reference/python-api/#wildata","title":"WilData","text":""},{"location":"api-reference/python-api/#data-pipeline","title":"Data Pipeline","text":"<pre><code>from wildata.pipeline import DataPipeline\n\n# Initialize\npipeline = DataPipeline(\"data\")\n\n# Import dataset\nresult = pipeline.import_dataset(\n    source_path=\"annotations.json\",\n    source_format=\"coco\",\n    dataset_name=\"my_dataset\"\n)\n\n# List datasets\ndatasets = pipeline.list_datasets()\n\n# Export\npipeline.export_dataset(\"my_dataset\", \"yolo\")\n</code></pre>"},{"location":"api-reference/python-api/#roi-adapter","title":"ROI Adapter","text":"<pre><code>from wildata.adapters import ROIAdapter\nimport json\n\n# Load COCO data\nwith open(\"annotations.json\") as f:\n    coco_data = json.load(f)\n\n# Create ROI adapter\nadapter = ROIAdapter(\n    coco_data,\n    roi_box_size=128,\n    random_roi_count=10\n)\n\n# Convert\nroi_data = adapter.convert()\n\n# Save\nadapter.save(roi_data, \"roi_dataset/\")\n</code></pre>"},{"location":"api-reference/python-api/#wildtrain","title":"WildTrain","text":""},{"location":"api-reference/python-api/#training-classification","title":"Training (Classification)","text":"<pre><code>from wildtrain.models import ImageClassifier\nfrom wildtrain.data import ClassificationDataModule\nimport pytorch_lightning as pl\n\n# Create model\nmodel = ImageClassifier(\n    architecture=\"resnet50\",\n    num_classes=10,\n    learning_rate=0.001\n)\n\n# Create data module\ndatamodule = ClassificationDataModule(\n    data_root=\"data/roi_dataset\",\n    batch_size=32\n)\n\n# Train\ntrainer = pl.Trainer(max_epochs=100, accelerator=\"gpu\")\ntrainer.fit(model, datamodule)\n</code></pre>"},{"location":"api-reference/python-api/#model-registry","title":"Model Registry","text":"<pre><code>from wildtrain.registry import ModelRegistry\n\n# Initialize\nregistry = ModelRegistry(\"http://localhost:5000\")\n\n# Register model\nversion = registry.register_model(\n    model_path=\"checkpoints/best.ckpt\",\n    model_name=\"wildlife_classifier\",\n    description=\"ResNet50 classifier\",\n    tags={\"accuracy\": \"0.95\"}\n)\n\n# Load model\nmodel = registry.load_model(\"wildlife_classifier\", version=\"latest\")\n\n# Promote to production\nregistry.promote_model(\"wildlife_classifier\", version, \"Production\")\n</code></pre> <p>For complete architecture details, see: - WildDetect Architecture - WilData Architecture - WildTrain Architecture</p>"},{"location":"api-reference/wildata-api/","title":"WilData REST API Reference","text":"<p>FastAPI-based REST API for WilData operations.</p>"},{"location":"api-reference/wildata-api/#getting-started","title":"Getting Started","text":""},{"location":"api-reference/wildata-api/#start-api-server","title":"Start API Server","text":"<pre><code>cd wildata\nscripts\\launch_api.bat\n</code></pre> <p>Access: - API: <code>http://localhost:8441</code> - Docs: <code>http://localhost:8441/docs</code> - Redoc: <code>http://localhost:8441/redoc</code></p>"},{"location":"api-reference/wildata-api/#endpoints","title":"Endpoints","text":""},{"location":"api-reference/wildata-api/#health-check","title":"Health Check","text":"<pre><code>GET /api/v1/health\n</code></pre>"},{"location":"api-reference/wildata-api/#import-dataset","title":"Import Dataset","text":"<pre><code>POST /api/v1/datasets/import\nContent-Type: application/json\n\n{\n  \"source_path\": \"/path/to/annotations.json\",\n  \"source_format\": \"coco\",\n  \"dataset_name\": \"my_dataset\",\n  \"root\": \"data\"\n}\n</code></pre>"},{"location":"api-reference/wildata-api/#list-datasets","title":"List Datasets","text":"<pre><code>GET /api/v1/datasets?root=data\n</code></pre>"},{"location":"api-reference/wildata-api/#create-roi-dataset","title":"Create ROI Dataset","text":"<pre><code>POST /api/v1/roi/create\nContent-Type: application/json\n\n{\n  \"source_path\": \"/path/to/data.json\",\n  \"source_format\": \"coco\",\n  \"dataset_name\": \"roi_dataset\",\n  \"roi_config\": {\n    \"roi_box_size\": 128,\n    \"random_roi_count\": 10\n  }\n}\n</code></pre>"},{"location":"api-reference/wildata-api/#job-status","title":"Job Status","text":"<pre><code>GET /api/v1/jobs/{job_id}\n</code></pre>"},{"location":"api-reference/wildata-api/#python-client-example","title":"Python Client Example","text":"<pre><code>import requests\n\n# Import dataset\nresponse = requests.post(\n    \"http://localhost:8441/api/v1/datasets/import\",\n    json={\n        \"source_path\": \"annotations.json\",\n        \"source_format\": \"coco\",\n        \"dataset_name\": \"my_dataset\"\n    }\n)\n\njob_id = response.json()[\"job_id\"]\n\n# Check job status\nstatus_response = requests.get(\n    f\"http://localhost:8441/api/v1/jobs/{job_id}\"\n)\n\nprint(status_response.json())\n</code></pre> <p>For complete API documentation, see: - WilData API Documentation - Interactive docs at <code>/docs</code> endpoint</p>"},{"location":"api-reference/wildata-cli/","title":"WilData CLI Reference","text":"<p>Complete command-line interface reference for WilData.</p>"},{"location":"api-reference/wildata-cli/#main-commands","title":"Main Commands","text":""},{"location":"api-reference/wildata-cli/#import-dataset","title":"import-dataset","text":"<p>Import dataset from various formats.</p> <pre><code>wildata import-dataset [SOURCE] [OPTIONS]\n</code></pre> <p>Options: - <code>-f, --format TEXT</code>: Source format (coco/yolo/ls) - <code>-n, --name TEXT</code>: Dataset name - <code>-c, --config PATH</code>: Config file path - <code>--root PATH</code>: Data root directory - <code>--split TEXT</code>: Split name (train/val/test) - <code>--enable-tiling</code>: Enable image tiling - <code>--tile-size INTEGER</code>: Tile size - <code>-v, --verbose</code>: Verbose output</p> <p>Examples: <pre><code># From config\nwildata import-dataset --config configs/import-config.yaml\n\n# Direct arguments\nwildata import-dataset data.json --format coco --name dataset1\n</code></pre></p>"},{"location":"api-reference/wildata-cli/#bulk-import-datasets","title":"bulk-import-datasets","text":"<p>Bulk import multiple datasets.</p> <pre><code>wildata bulk-import-datasets [OPTIONS]\n</code></pre> <p>Options: - <code>-c, --config PATH</code>: Bulk import config - <code>-n, --num-workers INTEGER</code>: Number of workers</p>"},{"location":"api-reference/wildata-cli/#create-roi-dataset","title":"create-roi-dataset","text":"<p>Create ROI classification dataset from detection annotations.</p> <pre><code>wildata create-roi-dataset [OPTIONS]\n</code></pre> <p>Options: - <code>-c, --config PATH</code>: ROI config file - <code>--roi-size INTEGER</code>: ROI box size - <code>--random-count INTEGER</code>: Background samples per image</p>"},{"location":"api-reference/wildata-cli/#dataset-list","title":"dataset list","text":"<p>List all datasets.</p> <pre><code>wildata dataset list [--root PATH]\n</code></pre>"},{"location":"api-reference/wildata-cli/#dataset-export","title":"dataset export","text":"<p>Export dataset to format.</p> <pre><code>wildata dataset export &lt;name&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--format TEXT</code>: Target format (coco/yolo) - <code>--output PATH</code>: Output directory</p>"},{"location":"api-reference/wildata-cli/#visualize-dataset","title":"visualize-dataset","text":"<p>Launch dataset visualization.</p> <pre><code>wildata visualize-dataset --dataset &lt;name&gt; --split &lt;split&gt;\n</code></pre>"},{"location":"api-reference/wildata-cli/#update-gps-from-csv","title":"update-gps-from-csv","text":"<p>Update image GPS metadata from CSV.</p> <pre><code>wildata update-gps-from-csv [OPTIONS]\n</code></pre> <p>Options: - <code>-c, --config PATH</code>: GPS update config - <code>--image-folder PATH</code>: Image folder - <code>--csv PATH</code>: CSV file path - <code>--output PATH</code>: Output directory</p> <p>For detailed script documentation, see WilData Scripts.</p>"},{"location":"api-reference/wildetect-cli/","title":"WildDetect CLI Reference","text":"<p>Complete command-line interface reference for WildDetect.</p>"},{"location":"api-reference/wildetect-cli/#main-commands","title":"Main Commands","text":"<pre><code>wildetect [COMMAND] [OPTIONS]\n</code></pre>"},{"location":"api-reference/wildetect-cli/#detect","title":"detect","text":"<p>Run wildlife detection on images.</p> <pre><code>wildetect detect &lt;images&gt; [OPTIONS]\n</code></pre> <p>Arguments: - <code>images</code>: Path to image file or directory</p> <p>Options: - <code>-m, --model TEXT</code>: Model path or MLflow model name - <code>-c, --config PATH</code>: Configuration file path - <code>-o, --output PATH</code>: Output directory - <code>--device TEXT</code>: Device (cuda/cpu/auto) - <code>--batch-size INTEGER</code>: Batch size - <code>--confidence FLOAT</code>: Confidence threshold - <code>--tile-size INTEGER</code>: Tile size for large images - <code>-v, --verbose</code>: Verbose output</p> <p>Examples: <pre><code># Basic detection\nwildetect detect images/ --model detector.pt\n\n# With config file\nwildetect detect images/ -c config/detection.yaml\n\n# Custom settings\nwildetect detect images/ --model detector.pt --batch-size 32 --confidence 0.7\n</code></pre></p>"},{"location":"api-reference/wildetect-cli/#census","title":"census","text":"<p>Run census campaign with analysis and reporting.</p> <pre><code>wildetect census &lt;campaign_name&gt; &lt;images&gt; [OPTIONS]\n</code></pre> <p>Arguments: - <code>campaign_name</code>: Census campaign name - <code>images</code>: Image directory</p> <p>Options: - <code>-c, --config PATH</code>: Configuration file (required) - <code>-o, --output PATH</code>: Output directory - <code>--species TEXT</code>: Target species (comma-separated) - <code>--generate-report</code>: Generate PDF report</p> <p>Examples: <pre><code>wildetect census summer_2024 images/ -c config/census.yaml\n</code></pre></p>"},{"location":"api-reference/wildetect-cli/#analyze","title":"analyze","text":"<p>Analyze detection results.</p> <pre><code>wildetect analyze &lt;results&gt; [OPTIONS]\n</code></pre> <p>Arguments: - <code>results</code>: Path to detection results JSON</p> <p>Options: - <code>-o, --output PATH</code>: Output directory - <code>--format TEXT</code>: Output format (json/csv/excel)</p>"},{"location":"api-reference/wildetect-cli/#fiftyone","title":"fiftyone","text":"<p>Manage FiftyOne datasets.</p> <pre><code>wildetect fiftyone [OPTIONS]\n</code></pre> <p>Options: - <code>--action TEXT</code>: Action (launch/info/export) - <code>--dataset TEXT</code>: Dataset name - <code>--port INTEGER</code>: Port number</p> <p>Examples: <pre><code># Launch viewer\nwildetect fiftyone --action launch --dataset my_dataset\n\n# Get dataset info\nwildetect fiftyone --action info --dataset my_dataset\n\n# Export\nwildetect fiftyone --action export --format coco --output export/\n</code></pre></p>"},{"location":"api-reference/wildetect-cli/#ui","title":"ui","text":"<p>Launch Streamlit web interface.</p> <pre><code>wildetect ui\n</code></pre>"},{"location":"api-reference/wildetect-cli/#info","title":"info","text":"<p>Show system and environment information.</p> <pre><code>wildetect info\n</code></pre> <p>For detailed script documentation, see WildDetect Scripts.</p>"},{"location":"api-reference/wildtrain-cli/","title":"WildTrain CLI Reference","text":"<p>Complete command-line interface reference for WildTrain.</p>"},{"location":"api-reference/wildtrain-cli/#main-commands","title":"Main Commands","text":""},{"location":"api-reference/wildtrain-cli/#train","title":"train","text":"<p>Train a model.</p> <pre><code>wildtrain train &lt;task&gt; [OPTIONS]\n</code></pre> <p>Arguments: - <code>task</code>: Task type (classifier/detector)</p> <p>Options: - <code>-c, --config PATH</code>: Config file (required) - <code>--resume PATH</code>: Resume from checkpoint - <code>--dry-run</code>: Dry run without training</p> <p>Examples: <pre><code># Train classifier\nwildtrain train classifier -c configs/classification/train.yaml\n\n# Train detector\nwildtrain train detector -c configs/detection/yolo.yaml\n</code></pre></p>"},{"location":"api-reference/wildtrain-cli/#eval","title":"eval","text":"<p>Evaluate a trained model.</p> <pre><code>wildtrain eval &lt;task&gt; [OPTIONS]\n</code></pre> <p>Arguments: - <code>task</code>: Task type (classifier/detector)</p> <p>Options: - <code>-c, --config PATH</code>: Config file (required) - <code>--checkpoint PATH</code>: Model checkpoint - <code>--split TEXT</code>: Dataset split (test/val)</p> <p>Examples: <pre><code>wildtrain eval classifier -c configs/classification/eval.yaml\nwildtrain eval detector -c configs/detection/yolo_eval.yaml\n</code></pre></p>"},{"location":"api-reference/wildtrain-cli/#register","title":"register","text":"<p>Register model to MLflow registry.</p> <pre><code>wildtrain register &lt;model_type&gt; &lt;config&gt;\n</code></pre> <p>Arguments: - <code>model_type</code>: Model type (classifier/detector) - <code>config</code>: Registration config file</p> <p>Example: <pre><code>wildtrain register detector configs/registration/detector_registration.yaml\n</code></pre></p>"},{"location":"api-reference/wildtrain-cli/#tune","title":"tune","text":"<p>Run hyperparameter tuning.</p> <pre><code>wildtrain tune &lt;task&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>-c, --config PATH</code>: Config file - <code>--n-trials INTEGER</code>: Number of trials</p> <p>Example: <pre><code>wildtrain tune classifier -c configs/classification/sweep.yaml --n-trials 50\n</code></pre></p>"},{"location":"api-reference/wildtrain-cli/#serve","title":"serve","text":"<p>Start inference server.</p> <pre><code>wildtrain serve [OPTIONS]\n</code></pre> <p>Options: - <code>-c, --config PATH</code>: Inference config - <code>--port INTEGER</code>: Server port - <code>--workers INTEGER</code>: Number of workers</p> <p>For detailed script documentation, see WildTrain Scripts.</p>"},{"location":"architecture/data-flow/","title":"Data Flow","text":"<p>This document describes how data flows through the WildDetect ecosystem, from raw annotations to final detection results and analysis.</p>"},{"location":"architecture/data-flow/#complete-pipeline-overview","title":"Complete Pipeline Overview","text":"<pre><code>flowchart TB\n    subgraph \"Stage 1: Data Collection\"\n        A[Raw Images]\n        B[Annotation Tools&lt;br/&gt;Label Studio/CVAT]\n        C[Annotations&lt;br/&gt;COCO/YOLO/LS]\n    end\n\n    subgraph \"Stage 2: Data Preparation (WilData)\"\n        D[Import &amp; Validate]\n        E[Transformations&lt;br/&gt;Tile/Augment/Clip]\n        F[Master Format&lt;br/&gt;Storage]\n        G[Export&lt;br/&gt;Train/Val/Test]\n    end\n\n    subgraph \"Stage 3: Model Training (WildTrain)\"\n        H[DataLoader]\n        I[Training Loop]\n        J[Validation]\n        K[MLflow Tracking]\n        L[Model Registry]\n    end\n\n    subgraph \"Stage 4: Deployment (WildDetect)\"\n        M[Load Model]\n        N[Detection Pipeline]\n        O[Detections]\n    end\n\n    subgraph \"Stage 5: Analysis\"\n        P[Census Statistics]\n        Q[Geographic Analysis]\n        R[Visualizations]\n        S[Reports]\n    end\n\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n    J --&gt; K\n    I --&gt; L\n    L --&gt; M\n    M --&gt; N\n    A --&gt; N\n    N --&gt; O\n    O --&gt; P\n    O --&gt; Q\n    Q --&gt; R\n    P --&gt; S\n\n    style F fill:#e3f2fd\n    style L fill:#fff3e0\n    style O fill:#e8f5e9\n    style S fill:#f3e5f5</code></pre>"},{"location":"architecture/data-flow/#stage-1-data-collection","title":"Stage 1: Data Collection","text":""},{"location":"architecture/data-flow/#raw-image-acquisition","title":"Raw Image Acquisition","text":"<p>Aerial images captured from drones or aircraft:</p> <pre><code>Input: Raw aerial images\nFormat: JPG, PNG, TIFF, GeoTIFF\nMetadata: GPS coordinates (EXIF), flight parameters\nSize: Varies (100MB - 10GB per image for rasters)\n</code></pre>"},{"location":"architecture/data-flow/#annotation-process","title":"Annotation Process","text":"<p>Images are annotated using labeling tools:</p> <p>Supported Tools: - Label Studio (recommended for collaboration) - CVAT (Computer Vision Annotation Tool) - Manual COCO/YOLO annotation</p> <p>Output Formats: - COCO JSON: <code>annotations.json</code> - YOLO: <code>labels/*.txt</code> + <code>data.yaml</code> - Label Studio: Export JSON</p>"},{"location":"architecture/data-flow/#example-label-studio-workflow","title":"Example: Label Studio Workflow","text":"<pre><code># 1. Setup Label Studio project\n# 2. Upload images\n# 3. Annotate with bounding boxes\n# 4. Export annotations\n\n# Example export structure\n{\n  \"annotations\": [\n    {\n      \"id\": 1,\n      \"image\": \"drone_001.jpg\",\n      \"annotations\": [\n        {\n          \"result\": [{\n            \"value\": {\n              \"x\": 10, \"y\": 20, \"width\": 50, \"height\": 60,\n              \"rectanglelabels\": [\"elephant\"]\n            }\n          }]\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"architecture/data-flow/#stage-2-data-preparation-wildata","title":"Stage 2: Data Preparation (WilData)","text":""},{"location":"architecture/data-flow/#import-process","title":"Import Process","text":"<pre><code>flowchart LR\n    A[Source&lt;br/&gt;Annotations] --&gt; B[Format&lt;br/&gt;Adapter]\n    B --&gt; C[Master&lt;br/&gt;Format]\n    C --&gt; D[Validation]\n    D --&gt; E{Valid?}\n    E --&gt;|Yes| F[Save]\n    E --&gt;|No| G[Error Report]\n    F --&gt; H[Master&lt;br/&gt;Storage]</code></pre>"},{"location":"architecture/data-flow/#format-conversion","title":"Format Conversion","text":"<p>All formats converted to unified master format:</p> <pre><code># COCO Input\n{\n  \"images\": [...],\n  \"annotations\": [...],\n  \"categories\": [...]\n}\n\n# \u2193 Converted to \u2193\n\n# Master Format\n{\n  \"info\": {\n    \"dataset_name\": \"my_dataset\",\n    \"source_format\": \"coco\",\n    \"created_at\": \"2024-01-01T00:00:00\"\n  },\n  \"images\": [\n    {\n      \"id\": 1,\n      \"file_name\": \"image.jpg\",\n      \"width\": 1920,\n      \"height\": 1080,\n      \"path\": \"data/images/train/image.jpg\"\n    }\n  ],\n  \"annotations\": [\n    {\n      \"id\": 1,\n      \"image_id\": 1,\n      \"category_id\": 1,\n      \"bbox\": [x, y, width, height],\n      \"area\": 12000,\n      \"confidence\": 1.0\n    }\n  ],\n  \"categories\": [\n    {\"id\": 1, \"name\": \"elephant\"}\n  ]\n}\n</code></pre>"},{"location":"architecture/data-flow/#transformation-pipeline","title":"Transformation Pipeline","text":"<p>Data transformations applied sequentially:</p>"},{"location":"architecture/data-flow/#1-bbox-clipping","title":"1. Bbox Clipping","text":"<pre><code># Before\nbbox = [x=-10, y=20, width=100, height=80]  # Outside image bounds\n\n# After clipping\nbbox = [x=0, y=20, width=90, height=80]  # Clipped to image\n</code></pre>"},{"location":"architecture/data-flow/#2-tiling","title":"2. Tiling","text":"<p>For large images:</p> <pre><code># Original: 8000x6000 image with 5 animals\n# \u2193\n# Tiles: 12 tiles of 800x800\n# - Tile (0,0): 1 animal\n# - Tile (1,0): 2 animals\n# - Tile (0,1): 1 animal\n# - etc.\n</code></pre>"},{"location":"architecture/data-flow/#3-augmentation","title":"3. Augmentation","text":"<p>Create variations for training:</p> <pre><code># Original image\n# \u2193\n# Augmented versions:\n# - Rotated +15\u00b0\n# - Rotated -15\u00b0\n# - Brightness adjusted\n# - etc.\n</code></pre>"},{"location":"architecture/data-flow/#export-for-training","title":"Export for Training","text":"<p>Convert to framework-specific format:</p> <p>YOLO Format: <pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 val/\n\u251c\u2500\u2500 labels/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 val/\n\u2514\u2500\u2500 data.yaml\n</code></pre></p> <p>COCO Format: <pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u251c\u2500\u2500 train.json\n\u2514\u2500\u2500 val.json\n</code></pre></p>"},{"location":"architecture/data-flow/#stage-3-model-training-wildtrain","title":"Stage 3: Model Training (WildTrain)","text":""},{"location":"architecture/data-flow/#data-loading","title":"Data Loading","text":"<pre><code># DataLoader creates batches\nfor batch in dataloader:\n    images, targets = batch\n    # images: tensor [B, 3, H, W]\n    # targets: list of dicts with 'boxes', 'labels'\n</code></pre>"},{"location":"architecture/data-flow/#training-loop","title":"Training Loop","text":"<pre><code>flowchart LR\n    A[Load Batch] --&gt; B[Forward Pass]\n    B --&gt; C[Calculate Loss]\n    C --&gt; D[Backpropagation]\n    D --&gt; E[Update Weights]\n    E --&gt; F{Epoch End?}\n    F --&gt;|No| A\n    F --&gt;|Yes| G[Validation]\n    G --&gt; H[Log Metrics]\n    H --&gt; I{Training Done?}\n    I --&gt;|No| A\n    I --&gt;|Yes| J[Save Model]</code></pre>"},{"location":"architecture/data-flow/#model-versioning","title":"Model Versioning","text":"<pre><code># Training produces:\n1. Model weights: model.pt\n2. Training metrics: logged to MLflow\n3. Model artifacts: configs, preprocessing params\n4. Model metadata: framework, version, dataset\n\n# Registered to MLflow:\nmodels:/detector_name/version\n</code></pre>"},{"location":"architecture/data-flow/#data-flow-in-training","title":"Data Flow in Training","text":"<pre><code># Epoch 1:\ntrain_images \u2192 model \u2192 predictions \u2192 loss \u2192 optimizer \u2192 updated_model\nval_images \u2192 updated_model \u2192 predictions \u2192 metrics \u2192 log\n\n# Epoch 2:\ntrain_images \u2192 updated_model \u2192 predictions \u2192 loss \u2192 optimizer \u2192 updated_model\nval_images \u2192 updated_model \u2192 predictions \u2192 metrics \u2192 log\n\n# ...\n\n# Epoch N:\ntrain_images \u2192 final_model \u2192 predictions \u2192 loss \u2192 optimizer \u2192 best_model\nval_images \u2192 best_model \u2192 predictions \u2192 metrics \u2192 save\n</code></pre>"},{"location":"architecture/data-flow/#stage-4-deployment-wilddetect","title":"Stage 4: Deployment (WildDetect)","text":""},{"location":"architecture/data-flow/#model-loading","title":"Model Loading","text":"<pre><code># Load from MLflow\nmodel = mlflow.pytorch.load_model(\"models:/detector/production\")\n\n# Or from file\nmodel = torch.load(\"detector.pt\")\n\n# Model ready for inference\n</code></pre>"},{"location":"architecture/data-flow/#detection-pipeline","title":"Detection Pipeline","text":"<pre><code>flowchart TB\n    A[Input Image] --&gt; B{Large Raster?}\n    B --&gt;|Yes| C[Tile Image]\n    B --&gt;|No| D[Preprocess]\n    C --&gt; E[Process Tiles]\n    E --&gt; F[Detect on Each Tile]\n    F --&gt; G[Stitch Results]\n    G --&gt; H[Apply NMS]\n    D --&gt; I[Detect]\n    I --&gt; H\n    H --&gt; J[Format Results]\n    J --&gt; K[Output Detections]</code></pre>"},{"location":"architecture/data-flow/#detection-output-format","title":"Detection Output Format","text":"<pre><code>{\n  \"image_path\": \"drone_001.jpg\",\n  \"image_size\": [1920, 1080],\n  \"processing_time\": 0.5,\n  \"detections\": [\n    {\n      \"class_name\": \"elephant\",\n      \"confidence\": 0.95,\n      \"bbox\": [100, 200, 150, 180],\n      \"bbox_normalized\": [0.052, 0.185, 0.078, 0.167]\n    },\n    {\n      \"class_name\": \"giraffe\",\n      \"confidence\": 0.89,\n      \"bbox\": [500, 300, 120, 200]\n    }\n  ],\n  \"metadata\": {\n    \"model_name\": \"detector_v1\",\n    \"model_version\": \"3\",\n    \"timestamp\": \"2024-01-01T12:00:00\"\n  }\n}\n</code></pre>"},{"location":"architecture/data-flow/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple images\nimages = [\"img1.jpg\", \"img2.jpg\", \"img3.jpg\"]\n\n# Parallel detection\nresults = []\nfor image in images:\n    result = pipeline.detect(image)\n    results.append(result)\n\n# Save all results\nsave_results(results, \"batch_results.json\")\n</code></pre>"},{"location":"architecture/data-flow/#stage-5-analysis","title":"Stage 5: Analysis","text":""},{"location":"architecture/data-flow/#census-statistics","title":"Census Statistics","text":"<p>Aggregate detections across campaign:</p> <pre><code># Input: All detection results\ntotal_detections = 1523 animals\n\n# Statistics by species:\n{\n  \"elephant\": {\n    \"count\": 423,\n    \"percentage\": 27.8,\n    \"avg_confidence\": 0.93\n  },\n  \"giraffe\": {\n    \"count\": 612,\n    \"percentage\": 40.2,\n    \"avg_confidence\": 0.89\n  },\n  \"zebra\": {\n    \"count\": 488,\n    \"percentage\": 32.0,\n    \"avg_confidence\": 0.91\n  }\n}\n\n# Density analysis:\nsurvey_area = 25 km\u00b2\ndensity = {\n  \"elephant\": 16.9 per km\u00b2,\n  \"giraffe\": 24.5 per km\u00b2,\n  \"zebra\": 19.5 per km\u00b2\n}\n</code></pre>"},{"location":"architecture/data-flow/#geographic-analysis","title":"Geographic Analysis","text":"<pre><code># Extract GPS coordinates from images\nimage_locations = [\n  (lat1, lon1),  # Image 1 location\n  (lat2, lon2),  # Image 2 location\n  ...\n]\n\n# Map detections to geographic space\ndetection_map = {\n  (lat1, lon1): [\"elephant\", \"giraffe\"],\n  (lat2, lon2): [\"zebra\", \"elephant\", \"elephant\"],\n  ...\n}\n\n# Analyze distribution\nhotspots = identify_hotspots(detection_map)\ncoverage = calculate_coverage(image_locations)\n</code></pre>"},{"location":"architecture/data-flow/#visualization-pipeline","title":"Visualization Pipeline","text":"<pre><code>flowchart LR\n    A[Detections] --&gt; B[Statistics]\n    A --&gt; C[Geographic Data]\n    B --&gt; D[Charts &amp; Graphs]\n    C --&gt; E[Maps]\n    D --&gt; F[Report]\n    E --&gt; F\n    A --&gt; G[FiftyOne]\n    G --&gt; H[Interactive Viewer]</code></pre>"},{"location":"architecture/data-flow/#data-storage-and-persistence","title":"Data Storage and Persistence","text":""},{"location":"architecture/data-flow/#directory-structure","title":"Directory Structure","text":"<pre><code>project/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/                    # Original images\n\u2502   \u251c\u2500\u2500 annotations/            # Original annotations\n\u2502   \u2514\u2500\u2500 datasets/               # Processed datasets\n\u2502       \u2514\u2500\u2500 my_dataset/\n\u2502           \u251c\u2500\u2500 images/\n\u2502           \u2502   \u251c\u2500\u2500 train/\n\u2502           \u2502   \u2514\u2500\u2500 val/\n\u2502           \u2514\u2500\u2500 annotations/\n\u2502               \u251c\u2500\u2500 train.json  # Master format\n\u2502               \u2514\u2500\u2500 val.json\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 checkpoints/            # Training checkpoints\n\u2502   \u2514\u2500\u2500 trained/                # Final models\n\u251c\u2500\u2500 results/\n\u2502   \u251c\u2500\u2500 detections/             # Detection outputs\n\u2502   \u251c\u2500\u2500 census/                 # Census reports\n\u2502   \u2514\u2500\u2500 visualizations/         # Maps, charts\n\u2514\u2500\u2500 mlruns/                     # MLflow tracking data\n</code></pre>"},{"location":"architecture/data-flow/#data-versioning-with-dvc","title":"Data Versioning with DVC","text":"<pre><code># Track data with DVC\ndvc add data/datasets/my_dataset\n\n# Creates .dvc file\ndata/datasets/my_dataset.dvc\n\n# Push to remote storage\ndvc push\n\n# On another machine\ndvc pull\n</code></pre>"},{"location":"architecture/data-flow/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/data-flow/#data-loading-optimization","title":"Data Loading Optimization","text":"<pre><code># Efficient data loading\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    num_workers=4,      # Parallel loading (threads on Windows)\n    pin_memory=True,    # Faster GPU transfer\n    prefetch_factor=2   # Prefetch batches\n)\n</code></pre>"},{"location":"architecture/data-flow/#memory-management","title":"Memory Management","text":"<pre><code># For large images\nwith rasterio.open(large_image) as src:\n    # Process in windows\n    for window in tile_windows:\n        tile = src.read(window=window)\n        process(tile)\n        del tile  # Free memory\n</code></pre>"},{"location":"architecture/data-flow/#caching","title":"Caching","text":"<pre><code># Cache loaded models\n@lru_cache(maxsize=1)\ndef load_model(model_path):\n    return torch.load(model_path)\n\n# Cache detection results\nresults_cache = {}\nif image_hash in results_cache:\n    return results_cache[image_hash]\n</code></pre>"},{"location":"architecture/data-flow/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"architecture/data-flow/#validation-checkpoints","title":"Validation Checkpoints","text":"<pre><code>flowchart TB\n    A[Input Data] --&gt; B{Valid Format?}\n    B --&gt;|No| C[Error Report]\n    B --&gt;|Yes| D{Images Exist?}\n    D --&gt;|No| C\n    D --&gt;|Yes| E{Bboxes Valid?}\n    E --&gt;|No| C\n    E --&gt;|Yes| F[Process Data]</code></pre>"},{"location":"architecture/data-flow/#recovery-mechanisms","title":"Recovery Mechanisms","text":"<pre><code># Checkpoint-based recovery\nfor i, image in enumerate(images):\n    try:\n        result = detect(image)\n        save_checkpoint(i, result)\n    except Exception as e:\n        logger.error(f\"Failed on image {i}: {e}\")\n        if should_continue:\n            continue\n        else:\n            # Resume from last checkpoint\n            resume_from_checkpoint(i)\n</code></pre>"},{"location":"architecture/data-flow/#integration-points","title":"Integration Points","text":""},{"location":"architecture/data-flow/#wildata-wildtrain","title":"WilData \u2194 WildTrain","text":"<pre><code># WilData exports dataset\nwildata.export_dataset(\"my_dataset\", format=\"yolo\", output=\"data/yolo\")\n\n# WildTrain loads dataset\ndatamodule = DataModule(data_root=\"data/yolo\")\ntrainer.fit(model, datamodule)\n</code></pre>"},{"location":"architecture/data-flow/#wildtrain-wilddetect","title":"WildTrain \u2194 WildDetect","text":"<pre><code># WildTrain registers model\nmlflow.pytorch.log_model(model, \"model\")\nmlflow.register_model(\"runs:/.../model\", \"detector\")\n\n# WildDetect loads model\npipeline = DetectionPipeline(mlflow_model_name=\"detector\")\n</code></pre>"},{"location":"architecture/data-flow/#wilddetect-fiftyone","title":"WildDetect \u2194 FiftyOne","text":"<pre><code># WildDetect creates FiftyOne dataset\nfo_dataset = create_fiftyone_dataset(detections)\n\n# Launch viewer\nsession = fo.launch_app(fo_dataset)\n</code></pre>"},{"location":"architecture/data-flow/#example-complete-workflow","title":"Example: Complete Workflow","text":"<pre><code># 1. Import annotations (WilData)\nfrom wildata import DataPipeline\n\npipeline = DataPipeline(\"data\")\npipeline.import_dataset(\n    source_path=\"annotations.json\",\n    source_format=\"coco\",\n    dataset_name=\"training_data\",\n    transformations={\"enable_tiling\": True}\n)\n\n# 2. Train model (WildTrain)\nfrom wildtrain import Trainer\n\ntrainer = Trainer.from_config(\"configs/yolo.yaml\")\nmodel = trainer.train()\nmodel_uri = trainer.register_model(\"wildlife_detector\")\n\n# 3. Run detection (WildDetect)\nfrom wildetect import DetectionPipeline\n\ndetector = DetectionPipeline(mlflow_model_uri=model_uri)\nresults = detector.detect_batch(\"survey_images/\")\n\n# 4. Analyze results (WildDetect)\nfrom wildetect import CensusEngine\n\ncensus = CensusEngine.from_detections(results)\ncensus.generate_report(\"census_report.pdf\")\n</code></pre>"},{"location":"architecture/data-flow/#next-steps","title":"Next Steps","text":"<ul> <li>WilData Architecture \u2192</li> <li>WildTrain Architecture \u2192</li> <li>WildDetect Architecture \u2192</li> <li>End-to-End Tutorial \u2192</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>The WildDetect monorepo is designed as a modular ecosystem of three interconnected packages, each serving a specific purpose in the wildlife detection and analysis pipeline.</p>"},{"location":"architecture/overview/#monorepo-structure","title":"Monorepo Structure","text":"<pre><code>wildetect/                 # Monorepo root\n\u251c\u2500\u2500 wildata/              # \ud83d\udce6 Data management package\n\u251c\u2500\u2500 wildtrain/            # \ud83c\udf93 Model training package  \n\u251c\u2500\u2500 src/wildetect/        # \ud83d\udd0d Detection and analysis package\n\u251c\u2500\u2500 config/               # Shared configurations\n\u251c\u2500\u2500 scripts/              # Batch scripts\n\u251c\u2500\u2500 docs/                 # Documentation\n\u2514\u2500\u2500 mkdocs.yml           # Documentation config\n</code></pre>"},{"location":"architecture/overview/#package-relationships","title":"Package Relationships","text":"<p>The three packages have a clear dependency hierarchy:</p> <pre><code>graph TD\n    A[WilData&lt;br/&gt;Data Pipeline] --&gt; B[WildTrain&lt;br/&gt;Model Training]\n    B --&gt; C[WildDetect&lt;br/&gt;Detection &amp; Analysis]\n    A -.optional.-&gt; C\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#e8f5e9</code></pre>"},{"location":"architecture/overview/#dependency-flow","title":"Dependency Flow","text":"<ol> <li>WilData (Foundation)</li> <li>Standalone package</li> <li>No dependencies on other packages</li> <li> <p>Provides data management primitives</p> </li> <li> <p>WildTrain (Training)</p> </li> <li>Depends on WilData for dataset loading</li> <li>Can be used independently for model training</li> <li> <p>Outputs models for WildDetect</p> </li> <li> <p>WildDetect (Application)</p> </li> <li>Depends on WildTrain for model structures</li> <li>Optionally uses WilData for data handling</li> <li>Top-level application package</li> </ol>"},{"location":"architecture/overview/#core-principles","title":"Core Principles","text":""},{"location":"architecture/overview/#1-separation-of-concerns","title":"1. Separation of Concerns","text":"<p>Each package has a single, well-defined responsibility:</p> <ul> <li>WilData: \"How do I manage and transform data?\"</li> <li>WildTrain: \"How do I train and evaluate models?\"</li> <li>WildDetect: \"How do I detect wildlife and analyze results?\"</li> </ul>"},{"location":"architecture/overview/#2-modularity","title":"2. Modularity","text":"<p>Packages can be used independently:</p> <pre><code># Use WilData alone for data management\nfrom wildata import DataPipeline\n\n# Use WildTrain alone for training\nfrom wildtrain import Trainer\n\n# Use WildDetect for detection\nfrom wildetect import DetectionPipeline\n</code></pre>"},{"location":"architecture/overview/#3-configuration-driven","title":"3. Configuration-Driven","text":"<p>All behavior is configurable via YAML files:</p> <pre><code># Each package has its own configs\nwildetect/config/         # Detection configs\nwildata/configs/          # Data configs\nwildtrain/configs/        # Training configs\n</code></pre>"},{"location":"architecture/overview/#4-clean-architecture","title":"4. Clean Architecture","text":"<p>Each package follows clean architecture principles:</p> <pre><code>src/package/\n\u251c\u2500\u2500 core/          # Business logic (domain)\n\u251c\u2500\u2500 adapters/      # External interfaces\n\u251c\u2500\u2500 cli/           # Command-line interface\n\u251c\u2500\u2500 api/           # REST API (if applicable)\n\u2514\u2500\u2500 ui/            # User interface (if applicable)\n</code></pre>"},{"location":"architecture/overview/#package-overview","title":"Package Overview","text":""},{"location":"architecture/overview/#wildata-data-management","title":"\ud83d\udce6 WilData - Data Management","text":"<p>Purpose: Unified data pipeline for object detection datasets</p> <p>Key Features: - Multi-format import/export (COCO, YOLO, Label Studio) - Data transformations (tiling, augmentation, clipping) - ROI dataset creation - DVC integration for versioning - REST API for remote operations</p> <p>Use Cases: - Import annotations from labeling tools - Prepare datasets for training - Create ROI datasets for hard sample mining - Version control large datasets</p> <p>Learn more \u2192</p>"},{"location":"architecture/overview/#wildtrain-model-training","title":"\ud83c\udf93 WildTrain - Model Training","text":"<p>Purpose: Modular training framework for detection and classification</p> <p>Key Features: - Multiple frameworks (YOLO, MMDetection, PyTorch Lightning) - Hydra configuration management - MLflow experiment tracking - Hyperparameter optimization (Optuna) - Model registration and versioning</p> <p>Use Cases: - Train custom detection models - Train classification models - Hyperparameter tuning - Model evaluation and comparison - Export models for deployment</p> <p>Learn more \u2192</p>"},{"location":"architecture/overview/#wilddetect-detection-analysis","title":"\ud83d\udd0d WildDetect - Detection &amp; Analysis","text":"<p>Purpose: Production detection system with census capabilities</p> <p>Key Features: - Multi-threaded detection pipelines - Large raster image support - Census campaign orchestration - Geographic analysis and visualization - FiftyOne integration - Comprehensive reporting</p> <p>Use Cases: - Run detection on aerial imagery - Conduct wildlife census campaigns - Generate population statistics - Create geographic visualizations - Export results for analysis</p> <p>Learn more \u2192</p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#complete-workflow","title":"Complete Workflow","text":"<pre><code>graph TB\n    subgraph \"1. Data Preparation (WilData)\"\n        A[Raw Annotations&lt;br/&gt;COCO/YOLO/LS] --&gt; B[WilData Import]\n        B --&gt; C[Transformations&lt;br/&gt;Tile/Augment]\n        C --&gt; D[Processed Dataset]\n    end\n\n    subgraph \"2. Model Training (WildTrain)\"\n        D --&gt; E[WildTrain]\n        E --&gt; F[Training Loop]\n        F --&gt; G[Trained Model]\n        G --&gt; H[MLflow Registry]\n    end\n\n    subgraph \"3. Deployment (WildDetect)\"\n        H --&gt; I[WildDetect]\n        J[Aerial Images] --&gt; I\n        I --&gt; K[Detections]\n        K --&gt; L[Analysis &amp; Reports]\n    end\n\n    style A fill:#e3f2fd\n    style D fill:#e3f2fd\n    style G fill:#fff3e0\n    style L fill:#e8f5e9</code></pre>"},{"location":"architecture/overview/#example-complete-pipeline","title":"Example: Complete Pipeline","text":"<pre><code># 1. WilData: Prepare dataset\nfrom wildata import DataPipeline\n\npipeline = DataPipeline(\"data\")\npipeline.import_dataset(\n    source_path=\"annotations.json\",\n    source_format=\"coco\",\n    dataset_name=\"training_data\"\n)\n\n# 2. WildTrain: Train model\nfrom wildtrain import Trainer\n\ntrainer = Trainer(config=\"configs/yolo.yaml\")\nmodel = trainer.train()\nmodel_uri = trainer.register_model(\"detector_v1\")\n\n# 3. WildDetect: Run detection\nfrom wildetect import DetectionPipeline\n\ndetector = DetectionPipeline(model_uri=model_uri)\nresults = detector.detect_batch(\"aerial_images/\")\ndetector.generate_report(results, \"census_report.json\")\n</code></pre> <p>See detailed data flow \u2192</p>"},{"location":"architecture/overview/#technology-stack","title":"Technology Stack","text":""},{"location":"architecture/overview/#core-technologies","title":"Core Technologies","text":"Component Technology Purpose Language Python 3.9+ Primary language CLI Typer Command-line interfaces Config Hydra/OmegaConf Configuration management Detection YOLO, MMDetection Object detection Training PyTorch Lightning Model training API FastAPI REST API (WilData) UI Streamlit Web interfaces Visualization FiftyOne Dataset visualization Tracking MLflow Experiment tracking Versioning DVC Data versioning"},{"location":"architecture/overview/#key-libraries","title":"Key Libraries","text":""},{"location":"architecture/overview/#data-processing","title":"Data Processing","text":"<ul> <li>Pillow: Image processing</li> <li>OpenCV: Computer vision operations</li> <li>Rasterio: Geospatial raster data</li> <li>Pandas: Tabular data manipulation</li> <li>Albumentations: Data augmentation</li> </ul>"},{"location":"architecture/overview/#machine-learning","title":"Machine Learning","text":"<ul> <li>PyTorch: Deep learning framework</li> <li>Ultralytics: YOLO implementation</li> <li>MMDetection: Detection framework</li> <li>Torchvision: Vision utilities</li> </ul>"},{"location":"architecture/overview/#utilities","title":"Utilities","text":"<ul> <li>Pydantic: Data validation</li> <li>Rich: Terminal formatting</li> <li>TQDM: Progress bars</li> <li>PyYAML: YAML parsing</li> </ul>"},{"location":"architecture/overview/#design-patterns","title":"Design Patterns","text":""},{"location":"architecture/overview/#1-factory-pattern","title":"1. Factory Pattern","text":"<p>Used for creating detectors, trainers, and datasets:</p> <pre><code># Detector factory\ndetector = DetectorFactory.create(\n    framework=\"yolo\",\n    model_path=\"model.pt\"\n)\n\n# Dataset factory\ndataset = DatasetFactory.create(\n    format=\"coco\",\n    path=\"annotations.json\"\n)\n</code></pre>"},{"location":"architecture/overview/#2-strategy-pattern","title":"2. Strategy Pattern","text":"<p>Used for different processing strategies:</p> <pre><code># Different detection strategies\npipeline = DetectionPipeline(\n    strategy=\"raster\"  # or \"simple\", \"multithreaded\", etc.\n)\n</code></pre>"},{"location":"architecture/overview/#3-adapter-pattern","title":"3. Adapter Pattern","text":"<p>Used for format conversions:</p> <pre><code># COCO to YOLO adapter\ncoco_data = COCODataset(path)\nyolo_adapter = YOLOAdapter(coco_data)\nyolo_data = yolo_adapter.convert()\n</code></pre>"},{"location":"architecture/overview/#4-pipeline-pattern","title":"4. Pipeline Pattern","text":"<p>Used for data transformations:</p> <pre><code># Transformation pipeline\npipeline = TransformationPipeline([\n    BBoxClippingTransform(),\n    TilingTransform(tile_size=800),\n    AugmentationTransform()\n])\ntransformed = pipeline.apply(dataset)\n</code></pre>"},{"location":"architecture/overview/#configuration-management","title":"Configuration Management","text":""},{"location":"architecture/overview/#hierarchical-configuration","title":"Hierarchical Configuration","text":"<p>Each package uses a hierarchical configuration system:</p> <pre><code># configs/main.yaml\ndefaults:\n  - model: yolo\n  - data: coco\n  - training: default\n\n# Override with CLI\npython main.py model=custom data.batch_size=64\n</code></pre>"},{"location":"architecture/overview/#configuration-sources","title":"Configuration Sources","text":"<ol> <li>Default configs: Sensible defaults in code</li> <li>YAML files: User configurations</li> <li>Environment variables: <code>.env</code> files</li> <li>CLI arguments: Command-line overrides</li> </ol> <p>Priority: CLI &gt; Env Vars &gt; YAML &gt; Defaults</p>"},{"location":"architecture/overview/#error-handling","title":"Error Handling","text":""},{"location":"architecture/overview/#centralized-error-management","title":"Centralized Error Management","text":"<pre><code>from wildetect.core.exceptions import (\n    DetectionError,\n    ModelLoadError,\n    ConfigurationError\n)\n\ntry:\n    detector.detect(image)\nexcept ModelLoadError as e:\n    logger.error(f\"Failed to load model: {e}\")\nexcept DetectionError as e:\n    logger.error(f\"Detection failed: {e}\")\n</code></pre>"},{"location":"architecture/overview/#validation","title":"Validation","text":"<p>All inputs are validated using Pydantic:</p> <pre><code>from pydantic import BaseModel, validator\n\nclass DetectionConfig(BaseModel):\n    batch_size: int\n    tile_size: int\n\n    @validator('batch_size')\n    def validate_batch_size(cls, v):\n        if v &lt;= 0:\n            raise ValueError(\"batch_size must be positive\")\n        return v\n</code></pre>"},{"location":"architecture/overview/#logging-and-monitoring","title":"Logging and Monitoring","text":""},{"location":"architecture/overview/#structured-logging","title":"Structured Logging","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\nlogger.info(\"Processing image\", extra={\n    \"image_path\": path,\n    \"tile_size\": 800,\n    \"batch_size\": 32\n})\n</code></pre>"},{"location":"architecture/overview/#experiment-tracking","title":"Experiment Tracking","text":"<pre><code>import mlflow\n\nwith mlflow.start_run():\n    mlflow.log_params(config)\n    mlflow.log_metrics({\"accuracy\": 0.95})\n    mlflow.log_artifact(\"model.pt\")\n</code></pre>"},{"location":"architecture/overview/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/overview/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/              # Unit tests\n\u2502   \u251c\u2500\u2500 test_core/\n\u2502   \u2514\u2500\u2500 test_adapters/\n\u251c\u2500\u2500 integration/       # Integration tests\n\u2502   \u2514\u2500\u2500 test_pipelines/\n\u2514\u2500\u2500 e2e/              # End-to-end tests\n    \u2514\u2500\u2500 test_workflows/\n</code></pre>"},{"location":"architecture/overview/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest tests/ -v\n\n# Run specific package tests\nuv run pytest tests/test_detection_pipeline.py -v\n\n# With coverage\nuv run pytest --cov=wildetect tests/\n</code></pre>"},{"location":"architecture/overview/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/overview/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li>Multi-threading: For I/O-bound operations (Windows-compatible)</li> <li>Batch Processing: Process multiple images together</li> <li>Caching: Cache loaded models and configurations</li> <li>Memory Management: Efficient image loading and cleanup</li> <li>GPU Utilization: Maximize GPU usage with appropriate batch sizes</li> </ol>"},{"location":"architecture/overview/#scalability","title":"Scalability","text":"<ul> <li>Horizontal: Process multiple images in parallel</li> <li>Vertical: Use larger models and batch sizes</li> <li>Distributed: Deploy inference servers for remote processing</li> </ul>"},{"location":"architecture/overview/#security-considerations","title":"Security Considerations","text":"<ul> <li>API Keys: Stored in <code>.env</code>, never in code</li> <li>File Paths: Validated before processing</li> <li>Input Validation: All inputs validated with Pydantic</li> <li>Dependency Management: Regular security updates</li> </ul>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<p>Explore individual package architectures:</p> <ul> <li>WilData Architecture \u2192</li> <li>WildTrain Architecture \u2192</li> <li>WildDetect Architecture \u2192</li> <li>Data Flow Details \u2192</li> </ul> <p>Or dive into specific topics:</p> <ul> <li>Scripts Reference</li> <li>Configuration Reference</li> <li>API Documentation</li> </ul>"},{"location":"architecture/wildata/","title":"WilData Architecture","text":"<p>WilData is the data management foundation of the WildDetect ecosystem, providing a unified pipeline for importing, transforming, and exporting object detection datasets.</p>"},{"location":"architecture/wildata/#overview","title":"Overview","text":"<p>Purpose: Unified data pipeline and management system for computer vision datasets</p> <p>Key Responsibilities: - Multi-format dataset import/export - Data transformations and augmentation - ROI dataset creation - DVC integration for versioning - REST API for programmatic access</p>"},{"location":"architecture/wildata/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TB\n    subgraph \"Input Layer\"\n        A[COCO Format]\n        B[YOLO Format]\n        C[Label Studio]\n    end\n\n    subgraph \"Core Pipeline\"\n        D[Format Adapters]\n        E[Master Format]\n        F[Transformation Pipeline]\n        G[Validation Layer]\n    end\n\n    subgraph \"Storage Layer\"\n        H[File System]\n        I[DVC Storage]\n    end\n\n    subgraph \"Output Layer\"\n        J[COCO Export]\n        K[YOLO Export]\n        L[ROI Dataset]\n    end\n\n    subgraph \"API Layer\"\n        M[REST API]\n        N[CLI]\n        O[Python API]\n    end\n\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    G --&gt; I\n    E --&gt; J\n    E --&gt; K\n    E --&gt; L\n\n    M --&gt; D\n    N --&gt; D\n    O --&gt; D\n\n    style E fill:#e1f5ff\n    style F fill:#fff4e1\n    style H fill:#e8f5e9</code></pre>"},{"location":"architecture/wildata/#core-components","title":"Core Components","text":""},{"location":"architecture/wildata/#1-format-adapters","title":"1. Format Adapters","text":"<p>Convert between different annotation formats.</p>"},{"location":"architecture/wildata/#coco-adapter","title":"COCO Adapter","text":"<pre><code># src/wildata/adapters/coco_adapter.py\nclass COCOAdapter:\n    \"\"\"Adapter for COCO format datasets.\"\"\"\n\n    def load(self, path: Path) -&gt; MasterDataset:\n        \"\"\"Load COCO annotations into master format.\"\"\"\n\n    def save(self, dataset: MasterDataset, path: Path):\n        \"\"\"Export master format to COCO.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#yolo-adapter","title":"YOLO Adapter","text":"<pre><code># src/wildata/adapters/yolo_adapter.py\nclass YOLOAdapter:\n    \"\"\"Adapter for YOLO format datasets.\"\"\"\n\n    def load(self, path: Path) -&gt; MasterDataset:\n        \"\"\"Load YOLO annotations into master format.\"\"\"\n\n    def save(self, dataset: MasterDataset, path: Path):\n        \"\"\"Export master format to YOLO.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#label-studio-adapter","title":"Label Studio Adapter","text":"<pre><code># src/wildata/adapters/ls_adapter.py\nclass LabelStudioAdapter:\n    \"\"\"Adapter for Label Studio export format.\"\"\"\n\n    def load(self, path: Path) -&gt; MasterDataset:\n        \"\"\"Load Label Studio annotations.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#2-master-format","title":"2. Master Format","text":"<p>Internal unified representation for all datasets.</p> <pre><code># src/wildata/core/master_format.py\n@dataclass\nclass MasterDataset:\n    \"\"\"Unified dataset representation.\"\"\"\n    images: List[ImageInfo]\n    annotations: List[Annotation]\n    categories: List[Category]\n    metadata: Dict[str, Any]\n\n@dataclass\nclass ImageInfo:\n    id: int\n    file_name: str\n    width: int\n    height: int\n    path: Path\n    metadata: Optional[Dict] = None\n\n@dataclass\nclass Annotation:\n    id: int\n    image_id: int\n    category_id: int\n    bbox: List[float]  # [x, y, width, height]\n    area: float\n    segmentation: Optional[List] = None\n</code></pre>"},{"location":"architecture/wildata/#3-transformation-pipeline","title":"3. Transformation Pipeline","text":"<p>Apply transformations to datasets.</p>"},{"location":"architecture/wildata/#bbox-clipping","title":"Bbox Clipping","text":"<pre><code># src/wildata/transforms/bbox_clipping.py\nclass BBoxClippingTransform:\n    \"\"\"Clip bounding boxes to image boundaries.\"\"\"\n\n    def __init__(self, tolerance: int = 5, skip_invalid: bool = False):\n        self.tolerance = tolerance\n        self.skip_invalid = skip_invalid\n\n    def apply(self, dataset: MasterDataset) -&gt; MasterDataset:\n        \"\"\"Apply clipping to all bboxes.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#tiling-transform","title":"Tiling Transform","text":"<pre><code># src/wildata/transforms/tiling.py\nclass TilingTransform:\n    \"\"\"Tile large images into smaller patches.\"\"\"\n\n    def __init__(\n        self,\n        tile_size: int = 512,\n        stride: int = 416,\n        min_visibility: float = 0.1\n    ):\n        self.tile_size = tile_size\n        self.stride = stride\n        self.min_visibility = min_visibility\n\n    def apply(self, dataset: MasterDataset) -&gt; MasterDataset:\n        \"\"\"Tile all images in dataset.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#augmentation-transform","title":"Augmentation Transform","text":"<pre><code># src/wildata/transforms/augmentation.py\nclass AugmentationTransform:\n    \"\"\"Apply data augmentation.\"\"\"\n\n    def __init__(\n        self,\n        rotation_range: Tuple[float, float] = (-45, 45),\n        probability: float = 1.0,\n        num_transforms: int = 2\n    ):\n        self.rotation_range = rotation_range\n        self.probability = probability\n        self.num_transforms = num_transforms\n\n    def apply(self, dataset: MasterDataset) -&gt; MasterDataset:\n        \"\"\"Augment dataset.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#4-roi-adapter","title":"4. ROI Adapter","text":"<p>Extract regions of interest for classification datasets.</p> <pre><code># src/wildata/adapters/roi_adapter.py\nclass ROIAdapter:\n    \"\"\"Convert detection datasets to ROI classification datasets.\"\"\"\n\n    def __init__(\n        self,\n        roi_box_size: int = 128,\n        min_roi_size: int = 32,\n        random_roi_count: int = 10,\n        background_class: str = \"background\"\n    ):\n        self.roi_box_size = roi_box_size\n        self.min_roi_size = min_roi_size\n        self.random_roi_count = random_roi_count\n        self.background_class = background_class\n\n    def convert(self, coco_data: dict) -&gt; ROIDataset:\n        \"\"\"Convert COCO dataset to ROI dataset.\"\"\"\n        # Extract ROIs from bboxes\n        # Generate background samples\n        # Create classification dataset\n</code></pre> <p>Use Cases: - Hard sample mining - Error analysis - Training ROI-based classifiers - Creating balanced classification datasets</p>"},{"location":"architecture/wildata/#5-data-pipeline","title":"5. Data Pipeline","text":"<p>Main orchestrator for data operations.</p> <pre><code># src/wildata/pipeline/data_pipeline.py\nclass DataPipeline:\n    \"\"\"Main data pipeline orchestrator.\"\"\"\n\n    def __init__(self, root: str = \"data\", enable_dvc: bool = False):\n        self.root = Path(root)\n        self.enable_dvc = enable_dvc\n        self.dvc_manager = DVCManager() if enable_dvc else None\n\n    def import_dataset(\n        self,\n        source_path: str,\n        source_format: str,\n        dataset_name: str,\n        transformations: Optional[TransformConfig] = None,\n        track_with_dvc: bool = False\n    ) -&gt; ImportResult:\n        \"\"\"Import dataset with optional transformations.\"\"\"\n        # 1. Load using appropriate adapter\n        # 2. Validate data\n        # 3. Apply transformations\n        # 4. Save to master format\n        # 5. Track with DVC if enabled\n\n    def export_dataset(\n        self,\n        dataset_name: str,\n        target_format: str,\n        output_path: Optional[str] = None\n    ) -&gt; ExportResult:\n        \"\"\"Export dataset to target format.\"\"\"\n\n    def list_datasets(self) -&gt; List[DatasetInfo]:\n        \"\"\"List all available datasets.\"\"\"\n\n    def get_dataset_info(self, dataset_name: str) -&gt; DatasetInfo:\n        \"\"\"Get detailed dataset information.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#6-dvc-manager","title":"6. DVC Manager","text":"<p>Handle data versioning with DVC.</p> <pre><code># src/wildata/pipeline/dvc_manager.py\nclass DVCManager:\n    \"\"\"DVC integration for data versioning.\"\"\"\n\n    def setup(self, storage_type: DVCStorageType, storage_path: str):\n        \"\"\"Initialize DVC remote.\"\"\"\n\n    def track(self, path: Path) -&gt; bool:\n        \"\"\"Add path to DVC tracking.\"\"\"\n\n    def push(self) -&gt; bool:\n        \"\"\"Push data to remote.\"\"\"\n\n    def pull(self, dataset_name: Optional[str] = None) -&gt; bool:\n        \"\"\"Pull data from remote.\"\"\"\n\n    def status(self) -&gt; DVCStatus:\n        \"\"\"Get DVC status.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#rest-api","title":"REST API","text":"<p>FastAPI-based API for remote operations.</p>"},{"location":"architecture/wildata/#api-structure","title":"API Structure","text":"<pre><code># src/wildata/api/main.py\napp = FastAPI(title=\"WilData API\")\n\n# Routers\napp.include_router(datasets_router, prefix=\"/api/v1/datasets\")\napp.include_router(roi_router, prefix=\"/api/v1/roi\")\napp.include_router(gps_router, prefix=\"/api/v1/gps\")\napp.include_router(jobs_router, prefix=\"/api/v1/jobs\")\napp.include_router(health_router, prefix=\"/api/v1/health\")\n</code></pre>"},{"location":"architecture/wildata/#background-jobs","title":"Background Jobs","text":"<p>Long-running operations handled asynchronously:</p> <pre><code># src/wildata/api/services/job_queue.py\nclass JobQueue:\n    \"\"\"Background job queue for async operations.\"\"\"\n\n    def submit(self, job_type: str, **kwargs) -&gt; str:\n        \"\"\"Submit job and return job_id.\"\"\"\n\n    def get_status(self, job_id: str) -&gt; JobStatus:\n        \"\"\"Get job status.\"\"\"\n\n    def cancel(self, job_id: str) -&gt; bool:\n        \"\"\"Cancel running job.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#cli-interface","title":"CLI Interface","text":"<p>Command-line interface built with Typer.</p> <pre><code># src/wildata/cli/main.py\napp = typer.Typer()\n\n@app.command()\ndef import_dataset(\n    source_path: str,\n    format: str = typer.Option(..., \"--format\", \"-f\"),\n    name: str = typer.Option(..., \"--name\", \"-n\"),\n    config: Optional[str] = typer.Option(None, \"--config\", \"-c\")\n):\n    \"\"\"Import dataset from source format.\"\"\"\n\n@app.command()\ndef export_dataset(\n    dataset_name: str,\n    format: str,\n    output: Optional[str] = None\n):\n    \"\"\"Export dataset to target format.\"\"\"\n\n@app.command()\ndef create_roi(\n    source_path: str,\n    config: str = typer.Option(..., \"--config\", \"-c\")\n):\n    \"\"\"Create ROI dataset.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#configuration-system","title":"Configuration System","text":""},{"location":"architecture/wildata/#import-configuration","title":"Import Configuration","text":"<pre><code># configs/import-config-example.yaml\nsource_path: \"annotations.json\"\nsource_format: \"coco\"  # coco, yolo, ls\ndataset_name: \"my_dataset\"\n\nroot: \"data\"\nsplit_name: \"train\"  # train, val, test\nprocessing_mode: \"batch\"  # streaming, batch\n\n# Transformations\ntransformations:\n  enable_bbox_clipping: true\n  bbox_clipping:\n    tolerance: 5\n    skip_invalid: false\n\n  enable_augmentation: false\n  augmentation:\n    rotation_range: [-45, 45]\n    probability: 1.0\n    num_transforms: 2\n\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n    min_visibility: 0.7\n    max_negative_tiles_in_negative_image: 2\n\n# ROI Configuration\nroi_config:\n  random_roi_count: 10\n  roi_box_size: 128\n  min_roi_size: 32\n  background_class: \"background\"\n  save_format: \"jpg\"\n  quality: 95\n</code></pre>"},{"location":"architecture/wildata/#data-storage","title":"Data Storage","text":""},{"location":"architecture/wildata/#directory-structure","title":"Directory Structure","text":"<pre><code>data/\n\u251c\u2500\u2500 datasets/\n\u2502   \u251c\u2500\u2500 dataset_name/\n\u2502   \u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 val/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 test/\n\u2502   \u2502   \u2514\u2500\u2500 annotations/\n\u2502   \u2502       \u251c\u2500\u2500 train.json        # Master format\n\u2502   \u2502       \u251c\u2500\u2500 val.json\n\u2502   \u2502       \u2514\u2500\u2500 test.json\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 exports/\n\u2502   \u251c\u2500\u2500 coco/\n\u2502   \u2514\u2500\u2500 yolo/\n\u2514\u2500\u2500 .dvc/                         # DVC metadata\n</code></pre>"},{"location":"architecture/wildata/#master-format-storage","title":"Master Format Storage","text":"<p>Datasets are stored in an extended COCO-like format:</p> <pre><code>{\n  \"info\": {\n    \"dataset_name\": \"my_dataset\",\n    \"created_at\": \"2024-01-01T00:00:00\",\n    \"source_format\": \"coco\",\n    \"transformations_applied\": [\"tiling\", \"clipping\"]\n  },\n  \"images\": [...],\n  \"annotations\": [...],\n  \"categories\": [...]\n}\n</code></pre>"},{"location":"architecture/wildata/#validation","title":"Validation","text":"<p>All data is validated at import:</p> <pre><code># src/wildata/core/validation.py\nclass DatasetValidator:\n    \"\"\"Validate dataset integrity.\"\"\"\n\n    def validate_coco(self, data: dict) -&gt; ValidationResult:\n        \"\"\"Validate COCO format.\"\"\"\n        # Check required fields\n        # Validate bbox coordinates\n        # Check image references\n        # Validate category IDs\n\n    def validate_yolo(self, data_yaml: Path) -&gt; ValidationResult:\n        \"\"\"Validate YOLO format.\"\"\"\n\n    def validate_master(self, dataset: MasterDataset) -&gt; ValidationResult:\n        \"\"\"Validate master format.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#performance-optimization","title":"Performance Optimization","text":""},{"location":"architecture/wildata/#streaming-mode","title":"Streaming Mode","text":"<p>For large datasets:</p> <pre><code># Process datasets in streaming mode\npipeline.import_dataset(\n    source_path=\"large_dataset.json\",\n    source_format=\"coco\",\n    dataset_name=\"large\",\n    processing_mode=\"streaming\"  # Process in chunks\n)\n</code></pre>"},{"location":"architecture/wildata/#parallel-processing","title":"Parallel Processing","text":"<p>Use threading for I/O-bound operations:</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    futures = [\n        executor.submit(process_image, img)\n        for img in images\n    ]\n</code></pre>"},{"location":"architecture/wildata/#use-cases","title":"Use Cases","text":""},{"location":"architecture/wildata/#1-import-and-transform","title":"1. Import and Transform","text":"<pre><code>from wildata import DataPipeline\n\npipeline = DataPipeline(\"data\")\n\n# Import with transformations\nresult = pipeline.import_dataset(\n    source_path=\"annotations.json\",\n    source_format=\"coco\",\n    dataset_name=\"processed\",\n    transformations={\n        \"enable_tiling\": True,\n        \"tiling\": {\"tile_size\": 800, \"stride\": 640}\n    }\n)\n</code></pre>"},{"location":"architecture/wildata/#2-create-roi-dataset","title":"2. Create ROI Dataset","text":"<pre><code>from wildata.adapters import ROIAdapter\n\n# Load COCO data\nwith open(\"annotations.json\") as f:\n    coco_data = json.load(f)\n\n# Convert to ROI dataset\nroi_adapter = ROIAdapter(\n    roi_box_size=128,\n    random_roi_count=10\n)\nroi_dataset = roi_adapter.convert(coco_data)\nroi_adapter.save(roi_dataset, output_dir=\"roi_dataset\")\n</code></pre>"},{"location":"architecture/wildata/#3-dvc-workflow","title":"3. DVC Workflow","text":"<pre><code># Setup DVC\nwildata dvc setup --storage-type s3 --storage-path s3://bucket/data\n\n# Import with tracking\nwildata import-dataset data.json --format coco --name ds --track-dvc\n\n# Push to remote\nwildata dvc push\n\n# On another machine\nwildata dvc pull ds\n</code></pre>"},{"location":"architecture/wildata/#next-steps","title":"Next Steps","text":"<ul> <li>WildTrain Architecture \u2192</li> <li>WildDetect Architecture \u2192</li> <li>Data Flow Details \u2192</li> <li>WilData CLI Reference \u2192</li> <li>WilData Scripts \u2192</li> </ul>"},{"location":"architecture/wildetect/","title":"WildDetect Architecture","text":"<p>WildDetect is the top-level application package that provides production-ready wildlife detection, census analysis, and geographic visualization capabilities.</p>"},{"location":"architecture/wildetect/#overview","title":"Overview","text":"<p>Purpose: Production detection system with census and analysis capabilities</p> <p>Key Responsibilities: - Wildlife detection on aerial imagery - Census campaign orchestration - Geographic analysis and visualization - Population statistics and reporting - Integration with FiftyOne and Label Studio</p>"},{"location":"architecture/wildetect/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TB\n    subgraph \"Input Layer\"\n        A[Aerial Images]\n        B[MLflow Models]\n        C[Configuration]\n    end\n\n    subgraph \"Detection Core\"\n        D[Model Loader]\n        E[Detection Pipeline]\n        F[Tiling Engine]\n        G[NMS &amp; Stitching]\n    end\n\n    subgraph \"Processing Strategies\"\n        H[Simple Pipeline]\n        I[Multi-threaded]\n        J[Raster Pipeline]\n    end\n\n    subgraph \"Analysis Layer\"\n        K[Census Engine]\n        L[Statistics]\n        M[Geographic Analysis]\n    end\n\n    subgraph \"Visualization Layer\"\n        N[FiftyOne Integration]\n        O[Geographic Maps]\n        P[Reports]\n    end\n\n    subgraph \"Output Layer\"\n        Q[Detections JSON/CSV]\n        R[Census Reports]\n        S[Visualizations]\n    end\n\n    A --&gt; E\n    B --&gt; D\n    C --&gt; E\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    E --&gt; H\n    E --&gt; I\n    E --&gt; J\n    G --&gt; K\n    K --&gt; L\n    K --&gt; M\n    L --&gt; P\n    M --&gt; O\n    K --&gt; N\n    G --&gt; Q\n    K --&gt; R\n    O --&gt; S\n\n    style E fill:#e1f5ff\n    style K fill:#fff4e1\n    style Q fill:#e8f5e9</code></pre>"},{"location":"architecture/wildetect/#core-components","title":"Core Components","text":""},{"location":"architecture/wildetect/#1-detection-pipelines","title":"1. Detection Pipelines","text":"<p>Multiple pipeline strategies for different use cases.</p>"},{"location":"architecture/wildetect/#base-pipeline-interface","title":"Base Pipeline Interface","text":"<pre><code># src/wildetect/core/pipeline/base.py\nfrom abc import ABC, abstractmethod\n\nclass DetectionPipeline(ABC):\n    \"\"\"Base detection pipeline interface.\"\"\"\n\n    def __init__(\n        self,\n        model_path: Optional[str] = None,\n        mlflow_model_name: Optional[str] = None,\n        device: str = \"cuda\"\n    ):\n        self.device = device\n        self.model = self._load_model(model_path, mlflow_model_name)\n\n    @abstractmethod\n    def detect(self, image_path: str) -&gt; DetectionResult:\n        \"\"\"Detect objects in single image.\"\"\"\n        pass\n\n    @abstractmethod\n    def detect_batch(self, image_paths: List[str]) -&gt; List[DetectionResult]:\n        \"\"\"Detect objects in batch of images.\"\"\"\n        pass\n\n    def _load_model(self, model_path, mlflow_model_name):\n        \"\"\"Load model from file or MLflow.\"\"\"\n        if mlflow_model_name:\n            return self._load_from_mlflow(mlflow_model_name)\n        return self._load_from_file(model_path)\n</code></pre>"},{"location":"architecture/wildetect/#simple-pipeline","title":"Simple Pipeline","text":"<pre><code># src/wildetect/core/pipeline/simple.py\nclass SimplePipeline(DetectionPipeline):\n    \"\"\"Simple sequential pipeline.\"\"\"\n\n    def detect_batch(self, image_paths: List[str]) -&gt; List[DetectionResult]:\n        \"\"\"Process images sequentially.\"\"\"\n        results = []\n        for image_path in tqdm(image_paths):\n            result = self.detect(image_path)\n            results.append(result)\n        return results\n</code></pre>"},{"location":"architecture/wildetect/#multi-threaded-pipeline","title":"Multi-threaded Pipeline","text":"<pre><code># src/wildetect/core/pipeline/multithreaded.py\nfrom concurrent.futures import ThreadPoolExecutor\nfrom queue import Queue\n\nclass MultiThreadedPipeline(DetectionPipeline):\n    \"\"\"Multi-threaded pipeline for parallel processing.\"\"\"\n\n    def __init__(self, num_data_workers: int = 2, queue_size: int = 64, **kwargs):\n        super().__init__(**kwargs)\n        self.num_data_workers = num_data_workers\n        self.queue_size = queue_size\n\n    def detect_batch(self, image_paths: List[str]) -&gt; List[DetectionResult]:\n        \"\"\"Process images using thread pool.\"\"\"\n        with ThreadPoolExecutor(max_workers=self.num_data_workers) as executor:\n            results = list(executor.map(self.detect, image_paths))\n        return results\n</code></pre>"},{"location":"architecture/wildetect/#raster-pipeline","title":"Raster Pipeline","text":"<pre><code># src/wildetect/core/pipeline/raster.py\nclass RasterPipeline(DetectionPipeline):\n    \"\"\"Pipeline for large raster images (GeoTIFF, etc.).\"\"\"\n\n    def __init__(\n        self,\n        tile_size: int = 800,\n        overlap_ratio: float = 0.2,\n        gsd: Optional[float] = None,  # Ground Sample Distance\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.tile_size = tile_size\n        self.overlap_ratio = overlap_ratio\n        self.gsd = gsd\n        self.tiler = RasterTiler(tile_size, overlap_ratio)\n        self.stitcher = DetectionStitcher(nms_threshold=0.5)\n\n    def detect(self, raster_path: str) -&gt; DetectionResult:\n        \"\"\"Detect objects in large raster image.\"\"\"\n        # 1. Tile the raster\n        tiles = self.tiler.tile_raster(raster_path)\n\n        # 2. Detect on each tile\n        tile_detections = []\n        for tile in tqdm(tiles, desc=\"Processing tiles\"):\n            detections = self._detect_tile(tile)\n            tile_detections.append(detections)\n\n        # 3. Stitch detections together\n        stitched = self.stitcher.stitch(tile_detections, tiles)\n\n        # 4. Convert to geographic coordinates if GSD provided\n        if self.gsd:\n            stitched = self._convert_to_geographic(stitched)\n\n        return stitched\n</code></pre>"},{"location":"architecture/wildetect/#2-tiling-engine","title":"2. Tiling Engine","text":"<p>Handle large image tiling and stitching.</p> <pre><code># src/wildetect/core/tiling/tiler.py\nimport rasterio\nfrom rasterio.windows import Window\n\nclass RasterTiler:\n    \"\"\"Tile large raster images.\"\"\"\n\n    def __init__(self, tile_size: int = 800, overlap_ratio: float = 0.2):\n        self.tile_size = tile_size\n        self.overlap = int(tile_size * overlap_ratio)\n        self.stride = tile_size - self.overlap\n\n    def tile_raster(self, raster_path: str) -&gt; List[TileInfo]:\n        \"\"\"Generate tiles from raster.\"\"\"\n        tiles = []\n\n        with rasterio.open(raster_path) as src:\n            width, height = src.width, src.height\n\n            for y in range(0, height, self.stride):\n                for x in range(0, width, self.stride):\n                    # Calculate window\n                    w = min(self.tile_size, width - x)\n                    h = min(self.tile_size, height - y)\n\n                    window = Window(x, y, w, h)\n\n                    # Read tile\n                    tile_data = src.read(window=window)\n\n                    tiles.append(TileInfo(\n                        data=tile_data,\n                        window=window,\n                        x_offset=x,\n                        y_offset=y,\n                        transform=src.window_transform(window)\n                    ))\n\n        return tiles\n</code></pre> <pre><code># src/wildetect/core/tiling/stitcher.py\nclass DetectionStitcher:\n    \"\"\"Stitch tile detections together.\"\"\"\n\n    def __init__(self, nms_threshold: float = 0.5):\n        self.nms_threshold = nms_threshold\n\n    def stitch(\n        self,\n        tile_detections: List[List[Detection]],\n        tiles: List[TileInfo]\n    ) -&gt; List[Detection]:\n        \"\"\"Stitch detections from tiles.\"\"\"\n        # 1. Convert tile coordinates to global coordinates\n        global_detections = []\n        for detections, tile in zip(tile_detections, tiles):\n            for det in detections:\n                # Adjust bbox to global coordinates\n                det.bbox[0] += tile.x_offset\n                det.bbox[1] += tile.y_offset\n                global_detections.append(det)\n\n        # 2. Apply NMS to remove duplicates at boundaries\n        final_detections = self._apply_nms(global_detections)\n\n        return final_detections\n\n    def _apply_nms(self, detections: List[Detection]) -&gt; List[Detection]:\n        \"\"\"Apply Non-Maximum Suppression.\"\"\"\n        # Group by class\n        by_class = {}\n        for det in detections:\n            if det.class_name not in by_class:\n                by_class[det.class_name] = []\n            by_class[det.class_name].append(det)\n\n        # Apply NMS per class\n        final = []\n        for class_name, dets in by_class.items():\n            nms_dets = self._nms_class(dets)\n            final.extend(nms_dets)\n\n        return final\n</code></pre>"},{"location":"architecture/wildetect/#3-census-system","title":"3. Census System","text":"<p>Orchestrate wildlife census campaigns.</p> <pre><code># src/wildetect/core/census/census_engine.py\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n@dataclass\nclass CensusConfig:\n    campaign_name: str\n    target_species: List[str]\n    flight_specs: FlightSpecs\n    coverage_area: Optional[Polygon] = None\n\nclass CensusEngine:\n    \"\"\"Census campaign orchestrator.\"\"\"\n\n    def __init__(self, config: CensusConfig, detection_pipeline: DetectionPipeline):\n        self.config = config\n        self.pipeline = detection_pipeline\n        self.statistics = StatisticsCalculator()\n        self.geographic = GeographicAnalyzer()\n\n    def run_census(self, image_dir: str) -&gt; CensusResult:\n        \"\"\"Run complete census campaign.\"\"\"\n        # 1. Discover images\n        images = self._discover_images(image_dir)\n\n        # 2. Run detection\n        detections = self.pipeline.detect_batch(images)\n\n        # 3. Calculate statistics\n        stats = self.statistics.calculate(detections, self.config.target_species)\n\n        # 4. Geographic analysis\n        geographic = self.geographic.analyze(detections, self.config.flight_specs)\n\n        # 5. Generate report\n        report = self._generate_report(stats, geographic)\n\n        return CensusResult(\n            campaign_name=self.config.campaign_name,\n            statistics=stats,\n            geographic=geographic,\n            detections=detections,\n            report=report\n        )\n</code></pre>"},{"location":"architecture/wildetect/#4-statistics-calculator","title":"4. Statistics Calculator","text":"<p>Compute population statistics.</p> <pre><code># src/wildetect/core/census/statistics.py\nclass StatisticsCalculator:\n    \"\"\"Calculate census statistics.\"\"\"\n\n    def calculate(\n        self,\n        detections: List[DetectionResult],\n        target_species: List[str]\n    ) -&gt; CensusStatistics:\n        \"\"\"Calculate comprehensive statistics.\"\"\"\n        stats = CensusStatistics()\n\n        # Total counts per species\n        stats.total_counts = self._count_by_species(detections)\n\n        # Counts per image\n        stats.counts_per_image = self._counts_per_image(detections)\n\n        # Density analysis\n        stats.density = self._calculate_density(detections)\n\n        # Confidence distribution\n        stats.confidence_dist = self._confidence_distribution(detections)\n\n        # Species co-occurrence\n        stats.co_occurrence = self._species_co_occurrence(detections)\n\n        return stats\n\n    def _count_by_species(self, detections: List[DetectionResult]) -&gt; Dict[str, int]:\n        \"\"\"Count detections per species.\"\"\"\n        counts = {}\n        for result in detections:\n            for det in result.detections:\n                counts[det.class_name] = counts.get(det.class_name, 0) + 1\n        return counts\n</code></pre>"},{"location":"architecture/wildetect/#5-geographic-analyzer","title":"5. Geographic Analyzer","text":"<p>Analyze geographic distribution.</p> <pre><code># src/wildetect/core/geographic/analyzer.py\nfrom shapely.geometry import Point, Polygon\nfrom shapely.ops import unary_union\n\nclass GeographicAnalyzer:\n    \"\"\"Analyze geographic distribution of detections.\"\"\"\n\n    def analyze(\n        self,\n        detections: List[DetectionResult],\n        flight_specs: FlightSpecs\n    ) -&gt; GeographicAnalysis:\n        \"\"\"Perform geographic analysis.\"\"\"\n        # 1. Extract GPS coordinates\n        gps_data = self._extract_gps(detections)\n\n        # 2. Calculate coverage area\n        coverage = self._calculate_coverage(gps_data)\n\n        # 3. Create distribution map\n        distribution = self._create_distribution_map(detections, gps_data)\n\n        # 4. Flight path analysis\n        flight_path = self._analyze_flight_path(gps_data)\n\n        # 5. Hotspot detection\n        hotspots = self._detect_hotspots(detections, gps_data)\n\n        return GeographicAnalysis(\n            coverage_area=coverage,\n            distribution_map=distribution,\n            flight_path=flight_path,\n            hotspots=hotspots,\n            total_area_surveyed=coverage.area\n        )\n</code></pre>"},{"location":"architecture/wildetect/#6-fiftyone-integration","title":"6. FiftyOne Integration","text":"<p>Integrate with FiftyOne for visualization.</p> <pre><code># src/wildetect/core/fiftyone/manager.py\nimport fiftyone as fo\n\nclass FiftyOneManager:\n    \"\"\"Manage FiftyOne datasets.\"\"\"\n\n    def create_dataset(\n        self,\n        name: str,\n        detections: List[DetectionResult],\n        image_dir: str\n    ) -&gt; fo.Dataset:\n        \"\"\"Create FiftyOne dataset from detections.\"\"\"\n        # Create dataset\n        dataset = fo.Dataset(name)\n\n        # Add samples\n        samples = []\n        for result in detections:\n            sample = fo.Sample(filepath=result.image_path)\n\n            # Add detections\n            detections_fo = []\n            for det in result.detections:\n                detections_fo.append(\n                    fo.Detection(\n                        label=det.class_name,\n                        bounding_box=self._convert_bbox(det.bbox),\n                        confidence=det.confidence\n                    )\n                )\n\n            sample[\"detections\"] = fo.Detections(detections=detections_fo)\n            samples.append(sample)\n\n        dataset.add_samples(samples)\n        return dataset\n\n    def launch_app(self, dataset_name: str, port: int = 5151):\n        \"\"\"Launch FiftyOne app.\"\"\"\n        dataset = fo.load_dataset(dataset_name)\n        session = fo.launch_app(dataset, port=port)\n        return session\n</code></pre>"},{"location":"architecture/wildetect/#cli-interface","title":"CLI Interface","text":"<pre><code># src/wildetect/cli/detect.py\nimport typer\n\napp = typer.Typer()\n\n@app.command()\ndef detect(\n    images: str = typer.Argument(..., help=\"Image path or directory\"),\n    model: Optional[str] = typer.Option(None, \"--model\", \"-m\"),\n    config: Optional[str] = typer.Option(None, \"--config\", \"-c\"),\n    output: str = typer.Option(\"results/\", \"--output\", \"-o\")\n):\n    \"\"\"Run wildlife detection.\"\"\"\n\n@app.command()\ndef census(\n    campaign_name: str = typer.Argument(...),\n    images: str = typer.Argument(...),\n    config: str = typer.Option(..., \"--config\", \"-c\"),\n    output: str = typer.Option(\"results/\", \"--output\", \"-o\")\n):\n    \"\"\"Run census campaign.\"\"\"\n\n@app.command()\ndef analyze(\n    results: str = typer.Argument(..., help=\"Detection results JSON\"),\n    output: str = typer.Option(\"analysis/\", \"--output\", \"-o\")\n):\n    \"\"\"Analyze detection results.\"\"\"\n\n@app.command()\ndef visualize(\n    results: str = typer.Argument(...),\n    output: str = typer.Option(\"visualizations/\", \"--output\", \"-o\")\n):\n    \"\"\"Create visualizations.\"\"\"\n</code></pre>"},{"location":"architecture/wildetect/#configuration-files","title":"Configuration Files","text":""},{"location":"architecture/wildetect/#detection-configuration","title":"Detection Configuration","text":"<pre><code># config/detection.yaml\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"raster\"  # simple, multithreaded, raster\n  queue_size: 64\n  num_data_workers: 2\n  nms_threshold: 0.5\n\nflight_specs:\n  sensor_height: 24  # mm\n  focal_length: 35.0  # mm\n  flight_height: 180.0  # meters\n  gsd: 2.38  # cm/px (Ground Sample Distance)\n\noutput:\n  directory: \"results\"\n  dataset_name: \"detections_fiftyone\"  # null to disable FiftyOne\n  save_visualizations: true\n</code></pre>"},{"location":"architecture/wildetect/#census-configuration","title":"Census Configuration","text":"<pre><code># config/census.yaml\ncampaign:\n  name: \"Summer_2024_Survey\"\n  target_species: [\"elephant\", \"giraffe\", \"zebra\"]\n  area_name: \"Serengeti_North\"\n\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n\nflight_specs:\n  flight_height: 120.0\n  gsd: 2.38\n\nanalysis:\n  calculate_density: true\n  detect_hotspots: true\n  create_maps: true\n\noutput:\n  directory: \"census_results\"\n  generate_pdf_report: true\n</code></pre>"},{"location":"architecture/wildetect/#use-cases","title":"Use Cases","text":""},{"location":"architecture/wildetect/#1-simple-detection","title":"1. Simple Detection","text":"<pre><code>from wildetect import DetectionPipeline\n\npipeline = DetectionPipeline(model_path=\"detector.pt\", device=\"cuda\")\nresults = pipeline.detect_batch(\"images/\")\n\n# Save results\npipeline.save_results(results, \"results.json\")\n</code></pre>"},{"location":"architecture/wildetect/#2-census-campaign","title":"2. Census Campaign","text":"<pre><code>from wildetect import CensusEngine, CensusConfig\n\nconfig = CensusConfig.from_yaml(\"config/census.yaml\")\nengine = CensusEngine(config)\n\ncensus_result = engine.run_census(\"survey_images/\")\n\n# Generate report\ncensus_result.save_report(\"census_report.pdf\")\n</code></pre>"},{"location":"architecture/wildetect/#3-large-raster-detection","title":"3. Large Raster Detection","text":"<pre><code>from wildetect import RasterPipeline\n\npipeline = RasterPipeline(\n    model_path=\"detector.pt\",\n    tile_size=800,\n    overlap_ratio=0.2,\n    gsd=2.38  # cm/px\n)\n\n# Detect on large GeoTIFF\nresults = pipeline.detect(\"large_ortho.tif\")\n</code></pre>"},{"location":"architecture/wildetect/#next-steps","title":"Next Steps","text":"<ul> <li>Data Flow Details \u2192</li> <li>Detection Tutorial \u2192</li> <li>Census Tutorial \u2192</li> <li>WildDetect Scripts \u2192</li> </ul>"},{"location":"architecture/wildtrain/","title":"WildTrain Architecture","text":"<p>WildTrain is a modular training framework that supports both object detection (YOLO, MMDetection) and classification (PyTorch Lightning) with integrated experiment tracking and model management.</p>"},{"location":"architecture/wildtrain/#overview","title":"Overview","text":"<p>Purpose: Flexible model training and evaluation framework</p> <p>Key Responsibilities: - Model training (detection and classification) - Experiment tracking with MLflow - Hyperparameter optimization - Model evaluation and metrics - Model registration and versioning</p>"},{"location":"architecture/wildtrain/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TB\n    subgraph \"Configuration Layer\"\n        A[Hydra Config]\n        B[YAML Files]\n    end\n\n    subgraph \"Data Layer\"\n        C[WilData Integration]\n        D[DataModule]\n        E[DataLoaders]\n    end\n\n    subgraph \"Model Layer\"\n        F[YOLO Models]\n        G[MMDet Models]\n        H[Classification Models]\n    end\n\n    subgraph \"Training Layer\"\n        I[Trainer]\n        J[Training Loop]\n        K[Validation]\n    end\n\n    subgraph \"Tracking Layer\"\n        L[MLflow]\n        M[Metrics]\n        N[Artifacts]\n    end\n\n    subgraph \"Output Layer\"\n        O[Trained Models]\n        P[Model Registry]\n        Q[Checkpoints]\n    end\n\n    A --&gt; I\n    B --&gt; A\n    C --&gt; D\n    D --&gt; E\n    E --&gt; J\n    F --&gt; J\n    G --&gt; J\n    H --&gt; J\n    I --&gt; J\n    J --&gt; K\n    K --&gt; M\n    M --&gt; L\n    J --&gt; O\n    O --&gt; P\n    O --&gt; Q\n    L --&gt; N\n\n    style I fill:#e1f5ff\n    style L fill:#fff4e1\n    style P fill:#e8f5e9</code></pre>"},{"location":"architecture/wildtrain/#core-components","title":"Core Components","text":""},{"location":"architecture/wildtrain/#1-model-architectures","title":"1. Model Architectures","text":""},{"location":"architecture/wildtrain/#yolo-detection","title":"YOLO Detection","text":"<pre><code># src/wildtrain/models/detection/yolo_model.py\nfrom ultralytics import YOLO\n\nclass YOLODetector:\n    \"\"\"YOLO-based object detector.\"\"\"\n\n    def __init__(self, model_size: str = \"n\"):\n        self.model = YOLO(f\"yolo11{model_size}.pt\")\n\n    def train(self, data_yaml: str, **kwargs):\n        \"\"\"Train YOLO model.\"\"\"\n        results = self.model.train(\n            data=data_yaml,\n            epochs=kwargs.get('epochs', 100),\n            imgsz=kwargs.get('imgsz', 640),\n            batch=kwargs.get('batch', 16)\n        )\n        return results\n\n    def validate(self, data_yaml: str):\n        \"\"\"Validate model.\"\"\"\n        return self.model.val(data=data_yaml)\n</code></pre>"},{"location":"architecture/wildtrain/#mmdetection","title":"MMDetection","text":"<pre><code># src/wildtrain/models/detection/mmdet_model.py\nfrom mmdet.apis import init_detector, train_detector\nfrom mmdet.apis import inference_detector\n\nclass MMDetDetector:\n    \"\"\"MMDetection-based detector.\"\"\"\n\n    def __init__(self, config: str, checkpoint: Optional[str] = None):\n        self.config = config\n        self.model = init_detector(config, checkpoint) if checkpoint else None\n\n    def train(self, config_file: str, work_dir: str):\n        \"\"\"Train MMDet model.\"\"\"\n        from mmcv import Config\n        cfg = Config.fromfile(config_file)\n        train_detector(self.model, cfg, distributed=False)\n</code></pre>"},{"location":"architecture/wildtrain/#classification-models","title":"Classification Models","text":"<pre><code># src/wildtrain/models/classification/classifier.py\nimport pytorch_lightning as pl\nimport torchvision.models as models\n\nclass ImageClassifier(pl.LightningModule):\n    \"\"\"PyTorch Lightning classifier.\"\"\"\n\n    def __init__(\n        self,\n        architecture: str = \"resnet50\",\n        num_classes: int = 10,\n        learning_rate: float = 0.001\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Load pretrained model\n        self.model = getattr(models, architecture)(pretrained=True)\n\n        # Replace final layer\n        in_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(in_features, num_classes)\n\n        self.criterion = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n</code></pre>"},{"location":"architecture/wildtrain/#2-data-modules","title":"2. Data Modules","text":""},{"location":"architecture/wildtrain/#detection-datamodule","title":"Detection DataModule","text":"<pre><code># src/wildtrain/data/detection_datamodule.py\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader\n\nclass DetectionDataModule(LightningDataModule):\n    \"\"\"DataModule for detection datasets.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 32,\n        num_workers: int = 4\n    ):\n        super().__init__()\n        self.data_root = data_root\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n    def setup(self, stage: Optional[str] = None):\n        # Load datasets using WilData\n        from wildata import DataPipeline\n        pipeline = DataPipeline(self.data_root)\n\n        if stage == \"fit\":\n            self.train_dataset = pipeline.load_dataset(\"train\")\n            self.val_dataset = pipeline.load_dataset(\"val\")\n\n        if stage == \"test\":\n            self.test_dataset = pipeline.load_dataset(\"test\")\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            shuffle=True\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers\n        )\n</code></pre>"},{"location":"architecture/wildtrain/#classification-datamodule","title":"Classification DataModule","text":"<pre><code># src/wildtrain/data/classification_datamodule.py\nclass ClassificationDataModule(LightningDataModule):\n    \"\"\"DataModule for classification datasets.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        image_size: int = 224,\n        batch_size: int = 32,\n        num_workers: int = 4\n    ):\n        super().__init__()\n        self.data_root = data_root\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n        # Define transforms\n        self.train_transform = transforms.Compose([\n            transforms.Resize((image_size, image_size)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n</code></pre>"},{"location":"architecture/wildtrain/#3-training-orchestration","title":"3. Training Orchestration","text":""},{"location":"architecture/wildtrain/#main-trainer","title":"Main Trainer","text":"<pre><code># src/wildtrain/trainers/trainer.py\nfrom hydra.utils import instantiate\nimport mlflow\n\nclass Trainer:\n    \"\"\"Main training orchestrator.\"\"\"\n\n    def __init__(self, config: DictConfig):\n        self.config = config\n        self.model = instantiate(config.model)\n        self.datamodule = instantiate(config.data)\n\n    def train(self):\n        \"\"\"Execute training loop.\"\"\"\n        # Setup MLflow\n        mlflow.set_experiment(self.config.experiment_name)\n\n        with mlflow.start_run():\n            # Log parameters\n            mlflow.log_params(OmegaConf.to_container(self.config))\n\n            # Create PyTorch Lightning trainer\n            pl_trainer = pl.Trainer(\n                max_epochs=self.config.training.epochs,\n                accelerator=self.config.training.accelerator,\n                devices=self.config.training.devices,\n                callbacks=self._create_callbacks()\n            )\n\n            # Train\n            pl_trainer.fit(self.model, self.datamodule)\n\n            # Log metrics\n            metrics = pl_trainer.callback_metrics\n            mlflow.log_metrics({k: v.item() for k, v in metrics.items()})\n\n            # Save model\n            model_path = \"trained_model\"\n            pl_trainer.save_checkpoint(model_path)\n            mlflow.pytorch.log_model(self.model, \"model\")\n\n        return self.model\n\n    def _create_callbacks(self):\n        \"\"\"Create training callbacks.\"\"\"\n        from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\n        return [\n            ModelCheckpoint(\n                monitor='val_loss',\n                mode='min',\n                save_top_k=3\n            ),\n            EarlyStopping(\n                monitor='val_loss',\n                patience=10,\n                mode='min'\n            )\n        ]\n</code></pre>"},{"location":"architecture/wildtrain/#4-evaluation-system","title":"4. Evaluation System","text":""},{"location":"architecture/wildtrain/#metrics-computation","title":"Metrics Computation","text":"<pre><code># src/wildtrain/evaluation/metrics.py\nfrom torchmetrics import Accuracy, Precision, Recall, F1Score\n\nclass ClassificationMetrics:\n    \"\"\"Compute classification metrics.\"\"\"\n\n    def __init__(self, num_classes: int):\n        self.accuracy = Accuracy(num_classes=num_classes)\n        self.precision = Precision(num_classes=num_classes, average='macro')\n        self.recall = Recall(num_classes=num_classes, average='macro')\n        self.f1 = F1Score(num_classes=num_classes, average='macro')\n\n    def compute(self, predictions, targets):\n        \"\"\"Compute all metrics.\"\"\"\n        return {\n            'accuracy': self.accuracy(predictions, targets),\n            'precision': self.precision(predictions, targets),\n            'recall': self.recall(predictions, targets),\n            'f1': self.f1(predictions, targets)\n        }\n</code></pre>"},{"location":"architecture/wildtrain/#detection-metrics","title":"Detection Metrics","text":"<pre><code># src/wildtrain/evaluation/detection_metrics.py\nfrom torchmetrics.detection import MeanAveragePrecision\n\nclass DetectionMetrics:\n    \"\"\"Compute detection metrics.\"\"\"\n\n    def __init__(self):\n        self.map_metric = MeanAveragePrecision()\n\n    def compute(self, predictions, targets):\n        \"\"\"Compute mAP and related metrics.\"\"\"\n        self.map_metric.update(predictions, targets)\n        results = self.map_metric.compute()\n\n        return {\n            'mAP': results['map'],\n            'mAP_50': results['map_50'],\n            'mAP_75': results['map_75']\n        }\n</code></pre>"},{"location":"architecture/wildtrain/#5-hyperparameter-optimization","title":"5. Hyperparameter Optimization","text":""},{"location":"architecture/wildtrain/#optuna-integration","title":"Optuna Integration","text":"<pre><code># src/wildtrain/tuning/optuna_tuner.py\nimport optuna\nfrom optuna.integration import PyTorchLightningPruningCallback\n\nclass OptunaTuner:\n    \"\"\"Hyperparameter tuning with Optuna.\"\"\"\n\n    def __init__(self, config: DictConfig):\n        self.config = config\n        self.study = optuna.create_study(\n            direction=\"minimize\",\n            study_name=config.study_name,\n            storage=config.storage\n        )\n\n    def objective(self, trial: optuna.Trial) -&gt; float:\n        \"\"\"Optimization objective.\"\"\"\n        # Suggest hyperparameters\n        lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n\n        # Update config\n        self.config.model.learning_rate = lr\n        self.config.data.batch_size = batch_size\n\n        # Train model\n        trainer = Trainer(self.config)\n        model = trainer.train()\n\n        # Return validation loss\n        return trainer.pl_trainer.callback_metrics['val_loss'].item()\n\n    def tune(self, n_trials: int = 50):\n        \"\"\"Run hyperparameter tuning.\"\"\"\n        self.study.optimize(self.objective, n_trials=n_trials)\n\n        print(f\"Best trial: {self.study.best_trial.params}\")\n        return self.study.best_params\n</code></pre>"},{"location":"architecture/wildtrain/#6-model-registration","title":"6. Model Registration","text":""},{"location":"architecture/wildtrain/#mlflow-model-registry","title":"MLflow Model Registry","text":"<pre><code># src/wildtrain/registry/model_registry.py\nimport mlflow\nfrom mlflow.tracking import MlflowClient\n\nclass ModelRegistry:\n    \"\"\"Manage models in MLflow registry.\"\"\"\n\n    def __init__(self, tracking_uri: str):\n        mlflow.set_tracking_uri(tracking_uri)\n        self.client = MlflowClient()\n\n    def register_model(\n        self,\n        model_path: str,\n        model_name: str,\n        description: Optional[str] = None,\n        tags: Optional[Dict] = None\n    ) -&gt; str:\n        \"\"\"Register model to MLflow.\"\"\"\n        # Log model\n        with mlflow.start_run():\n            mlflow.pytorch.log_model(model_path, \"model\")\n            run_id = mlflow.active_run().info.run_id\n\n        # Register\n        model_uri = f\"runs:/{run_id}/model\"\n        mv = mlflow.register_model(model_uri, model_name)\n\n        # Add description and tags\n        if description:\n            self.client.update_model_version(\n                name=model_name,\n                version=mv.version,\n                description=description\n            )\n\n        if tags:\n            for key, value in tags.items():\n                self.client.set_model_version_tag(\n                    name=model_name,\n                    version=mv.version,\n                    key=key,\n                    value=value\n                )\n\n        return mv.version\n\n    def load_model(self, model_name: str, version: Optional[str] = None):\n        \"\"\"Load model from registry.\"\"\"\n        if version:\n            model_uri = f\"models:/{model_name}/{version}\"\n        else:\n            model_uri = f\"models:/{model_name}/latest\"\n\n        return mlflow.pytorch.load_model(model_uri)\n\n    def promote_model(self, model_name: str, version: str, stage: str):\n        \"\"\"Promote model to production stage.\"\"\"\n        self.client.transition_model_version_stage(\n            name=model_name,\n            version=version,\n            stage=stage  # \"Staging\", \"Production\", \"Archived\"\n        )\n</code></pre>"},{"location":"architecture/wildtrain/#configuration-system","title":"Configuration System","text":""},{"location":"architecture/wildtrain/#hydra-configuration","title":"Hydra Configuration","text":"<p>WildTrain uses Hydra for flexible configuration management.</p> <pre><code># src/wildtrain/main.py\nimport hydra\nfrom omegaconf import DictConfig\n\n@hydra.main(config_path=\"configs\", config_name=\"main\", version_base=\"1.3\")\ndef main(cfg: DictConfig):\n    trainer = Trainer(cfg)\n    trainer.train()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"architecture/wildtrain/#configuration-structure","title":"Configuration Structure","text":"<pre><code># configs/main.yaml\ndefaults:\n  - model: yolo\n  - data: detection\n  - training: default\n  - _self_\n\nexperiment_name: wildlife_detection\nseed: 42\n\n# Override from CLI:\n# python main.py model=custom data.batch_size=64\n</code></pre>"},{"location":"architecture/wildtrain/#model-configs","title":"Model Configs","text":"<pre><code># configs/detection/yolo.yaml\nmodel:\n  framework: \"yolo\"\n  size: \"n\"  # n, s, m, l, x\n  pretrained: true\n\ntraining:\n  epochs: 100\n  imgsz: 640\n  batch: 16\n  optimizer: \"AdamW\"\n  lr0: 0.001\n</code></pre>"},{"location":"architecture/wildtrain/#cli-interface","title":"CLI Interface","text":"<pre><code># src/wildtrain/cli/train.py\nimport typer\napp = typer.Typer()\n\n@app.command()\ndef train(\n    task: str = typer.Option(..., help=\"Task type: classifier or detector\"),\n    config: str = typer.Option(..., \"-c\", \"--config\"),\n    override: List[str] = typer.Option(None, \"-o\", \"--override\")\n):\n    \"\"\"Train a model.\"\"\"\n    # Load config with overrides\n    # Start training\n\n@app.command()\ndef evaluate(\n    task: str,\n    config: str,\n    checkpoint: str\n):\n    \"\"\"Evaluate a trained model.\"\"\"\n\n@app.command()\ndef tune(\n    config: str,\n    n_trials: int = 50\n):\n    \"\"\"Run hyperparameter tuning.\"\"\"\n</code></pre>"},{"location":"architecture/wildtrain/#use-cases","title":"Use Cases","text":""},{"location":"architecture/wildtrain/#1-train-yolo-detector","title":"1. Train YOLO Detector","text":"<pre><code># Using CLI\nwildtrain train detector -c configs/detection/yolo.yaml\n\n# Using Python\nfrom wildtrain import Trainer\ntrainer = Trainer.from_config(\"configs/detection/yolo.yaml\")\nmodel = trainer.train()\n</code></pre>"},{"location":"architecture/wildtrain/#2-train-classifier","title":"2. Train Classifier","text":"<pre><code>import pytorch_lightning as pl\nfrom wildtrain import ImageClassifier, ClassificationDataModule\n\n# Create model and data\nmodel = ImageClassifier(architecture=\"resnet50\", num_classes=10)\ndatamodule = ClassificationDataModule(data_root=\"data/classification\")\n\n# Create trainer\ntrainer = pl.Trainer(max_epochs=50, accelerator=\"gpu\")\n\n# Train\ntrainer.fit(model, datamodule)\n</code></pre>"},{"location":"architecture/wildtrain/#3-hyperparameter-tuning","title":"3. Hyperparameter Tuning","text":"<pre><code>from wildtrain.tuning import OptunaTuner\n\ntuner = OptunaTuner(config)\nbest_params = tuner.tune(n_trials=50)\n\nprint(f\"Best hyperparameters: {best_params}\")\n</code></pre>"},{"location":"architecture/wildtrain/#4-model-registration","title":"4. Model Registration","text":"<pre><code>from wildtrain.registry import ModelRegistry\n\nregistry = ModelRegistry(tracking_uri=\"http://localhost:5000\")\n\n# Register model\nversion = registry.register_model(\n    model_path=\"checkpoints/best.ckpt\",\n    model_name=\"wildlife_detector\",\n    description=\"YOLO model trained on aerial images\",\n    tags={\"framework\": \"yolo\", \"dataset\": \"wildlife_v1\"}\n)\n\n# Promote to production\nregistry.promote_model(\"wildlife_detector\", version, \"Production\")\n</code></pre>"},{"location":"architecture/wildtrain/#next-steps","title":"Next Steps","text":"<ul> <li>WildDetect Architecture \u2192</li> <li>Data Flow Details \u2192</li> <li>Training Tutorial \u2192</li> <li>WildTrain Scripts \u2192</li> </ul>"},{"location":"configs/wildata/","title":"WilData Configuration Reference","text":"<p>Documentation for all WilData configuration files used in data management operations.</p>"},{"location":"configs/wildata/#configuration-files","title":"Configuration Files","text":"File Purpose import-config-example.yaml Dataset import configuration bulk-import-*.yaml Bulk import configurations roi-create-config.yaml ROI dataset creation bulk-roi-create-config.yaml Bulk ROI creation gps-update-config-example.yaml GPS metadata update label_studio_config.xml Label Studio interface"},{"location":"configs/wildata/#import-config-exampleyaml","title":"import-config-example.yaml","text":"<p>Purpose: Configure dataset import with transformations</p> <p>Location: <code>wildata/configs/import-config-example.yaml</code></p>"},{"location":"configs/wildata/#complete-configuration","title":"Complete Configuration","text":"<pre><code># Required: Source Information\nsource_path: \"D:/annotations/dataset.json\"\nsource_format: \"coco\"  # coco, yolo, ls (Label Studio)\ndataset_name: \"my_dataset\"\n\n# Pipeline Configuration\nroot: \"D:/data\"\nsplit_name: \"train\"  # train, val, test\nenable_dvc: false\nprocessing_mode: \"batch\"  # batch, streaming\ntrack_with_dvc: false\nbbox_tolerance: 5\n\n# Label Studio Options (for ls format)\ndotenv_path: \".env\"\nls_xml_config: \"configs/label_studio_config.xml\"\nls_parse_config: false\n\n# ROI Configuration\ndisable_roi: false\nroi_config:\n  random_roi_count: 2              # Background samples per image\n  roi_box_size: 384                # ROI size (pixels)\n  min_roi_size: 32                 # Minimum object size\n  dark_threshold: 0.7              # Dark image threshold\n  background_class: \"background\"\n  save_format: \"jpg\"               # jpg, png\n  quality: 95                      # JPEG quality\n  sample_background: true          # Sample background regions\n\n# Transformation Pipeline\ntransformations:\n  # Bbox Clipping\n  enable_bbox_clipping: true\n  bbox_clipping:\n    tolerance: 5                   # Pixels outside image allowed\n    skip_invalid: false            # Skip invalid bboxes\n\n  # Data Augmentation\n  enable_augmentation: false\n  augmentation:\n    rotation_range: [-45, 45]     # Rotation degrees\n    probability: 1.0               # Augmentation probability\n    brightness_range: [-0.2, 0.4]\n    scale: [1.0, 2.0]\n    translate: [-0.1, 0.2]\n    shear: [-5, 5]\n    contrast_range: [-0.2, 0.4]\n    noise_std: [0.01, 0.1]\n    seed: 41\n    num_transforms: 2              # Augmentations per image\n\n  # Image Tiling\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640                    # Tile stride\n    min_visibility: 0.7            # Min object visibility\n    max_negative_tiles_in_negative_image: 2\n    negative_positive_ratio: 1.0\n    dark_threshold: 0.7\n</code></pre>"},{"location":"configs/wildata/#bulk-import-configs","title":"bulk-import Configs","text":"<p>Purpose: Configure batch import of multiple datasets</p> <p>Files: - <code>bulk-import-train.yaml</code> - <code>bulk-import-val.yaml</code> - <code>bulk-import-config-example.yaml</code></p>"},{"location":"configs/wildata/#configuration-format","title":"Configuration Format","text":"<pre><code>source_paths:\n  - \"D:/annotations/dataset1.json\"\n  - \"D:/annotations/dataset2.json\"\n  - \"D:/annotations/dataset3.json\"\n\nsource_format: \"coco\"\nroot: \"D:/data\"\nsplit_name: \"train\"\n\n# Shared settings (same as import-config)\nprocessing_mode: \"batch\"\nbbox_tolerance: 5\n\ntransformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n</code></pre>"},{"location":"configs/wildata/#roi-create-configyaml","title":"roi-create-config.yaml","text":"<p>Purpose: Configure ROI dataset creation</p> <p>Location: <code>wildata/configs/roi-create-config.yaml</code></p>"},{"location":"configs/wildata/#configuration","title":"Configuration","text":"<pre><code>source_path: \"annotations.json\"\nsource_format: \"coco\"\ndataset_name: \"roi_dataset\"\n\nroot: \"data\"\nsplit_name: \"val\"  # Usually val or test\nbbox_tolerance: 5\n\nroi_config:\n  roi_box_size: 128              # ROI crop size\n  min_roi_size: 32               # Min object size to extract\n  random_roi_count: 10           # Background samples per image\n  dark_threshold: 0.7\n  background_class: \"background\"\n  save_format: \"jpg\"\n  quality: 95\n  padding: 10                    # Padding around object\n  sample_background: true\n\n  # Advanced options\n  aspect_ratio_range: [0.5, 2.0]  # Valid aspect ratios\n  min_object_area: 32             # Min area (pixels\u00b2)\n\nls_xml_config: null\nls_parse_config: false\ndraw_original_bboxes: false\n</code></pre>"},{"location":"configs/wildata/#bulk-roi-create-configyaml","title":"bulk-roi-create-config.yaml","text":"<p>Purpose: Bulk ROI dataset creation</p>"},{"location":"configs/wildata/#configuration_1","title":"Configuration","text":"<pre><code>source_paths:\n  - \"dataset1.json\"\n  - \"dataset2.json\"\n\nsource_format: \"coco\"\nsplit_name: \"val\"\n\nroi_config:\n  roi_box_size: 128\n  random_roi_count: 5\n  background_class: \"background\"\n</code></pre>"},{"location":"configs/wildata/#gps-update-config-exampleyaml","title":"gps-update-config-example.yaml","text":"<p>Purpose: Update image GPS from CSV</p> <p>Location: <code>wildata/configs/gps-update-config-example.yaml</code></p>"},{"location":"configs/wildata/#configuration_2","title":"Configuration","text":"<pre><code>image_folder: \"D:/images/\"\ncsv_path: \"gps_coordinates.csv\"\noutput_dir: \"D:/images_with_gps/\"\n\n# CSV Parsing\nskip_rows: 0\nfilename_col: \"filename\"\nlat_col: \"latitude\"\nlon_col: \"longitude\"\nalt_col: \"altitude\"\n\n# Options\noverwrite_existing: false        # Overwrite existing GPS\ncreate_backup: true              # Backup original files\nvalidate_coordinates: true       # Validate GPS coordinates\n</code></pre>"},{"location":"configs/wildata/#csv-format","title":"CSV Format","text":"<pre><code>filename,latitude,longitude,altitude\nimage001.jpg,40.7128,-74.0060,10.5\nimage002.jpg,40.7589,-73.9851,15.2\n</code></pre>"},{"location":"configs/wildata/#label_studio_configxml","title":"label_studio_config.xml","text":"<p>Purpose: Label Studio annotation interface configuration</p> <p>Location: <code>wildata/configs/label_studio_config.xml</code></p>"},{"location":"configs/wildata/#example-configuration","title":"Example Configuration","text":"<pre><code>&lt;View&gt;\n  &lt;Image name=\"image\" value=\"$image\"/&gt;\n  &lt;RectangleLabels name=\"label\" toName=\"image\"&gt;\n    &lt;Label value=\"elephant\" background=\"red\"/&gt;\n    &lt;Label value=\"giraffe\" background=\"blue\"/&gt;\n    &lt;Label value=\"zebra\" background=\"green\"/&gt;\n    &lt;Label value=\"buffalo\" background=\"yellow\"/&gt;\n  &lt;/RectangleLabels&gt;\n&lt;/View&gt;\n</code></pre>"},{"location":"configs/wildata/#configuration-examples","title":"Configuration Examples","text":""},{"location":"configs/wildata/#import-coco-with-tiling","title":"Import COCO with Tiling","text":"<pre><code>source_path: \"D:/coco/annotations.json\"\nsource_format: \"coco\"\ndataset_name: \"wildlife_tiled\"\n\nroot: \"D:/data\"\nsplit_name: \"train\"\n\ntransformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n    min_visibility: 0.7\n</code></pre>"},{"location":"configs/wildata/#import-label-studio","title":"Import Label Studio","text":"<pre><code>source_path: \"D:/label_studio/export.json\"\nsource_format: \"ls\"\ndataset_name: \"annotated_data\"\n\nls_xml_config: \"configs/label_studio_config.xml\"\nls_parse_config: true\n\nroi_config:\n  roi_box_size: 128\n  random_roi_count: 5\n</code></pre>"},{"location":"configs/wildata/#import-with-full-pipeline","title":"Import with Full Pipeline","text":"<pre><code>source_path: \"raw_annotations.json\"\nsource_format: \"coco\"\ndataset_name: \"processed_dataset\"\n\ntransformations:\n  enable_bbox_clipping: true\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n  enable_augmentation: true\n  augmentation:\n    num_transforms: 2\n    probability: 0.8\n\nroi_config:\n  roi_box_size: 384\n  random_roi_count: 10\n</code></pre>"},{"location":"configs/wildata/#best-practices","title":"Best Practices","text":"<ol> <li>Use absolute paths for cross-platform compatibility</li> <li>Enable bbox_clipping to fix annotation errors</li> <li>Tile large images for better training</li> <li>Sample background ROIs for balanced datasets</li> <li>Version control configuration changes</li> </ol>"},{"location":"configs/wildata/#next-steps","title":"Next Steps","text":"<ul> <li>WilData Scripts</li> <li>Dataset Preparation Tutorial</li> <li>WilData API Reference</li> </ul>"},{"location":"configs/wildetect/","title":"WildDetect Configuration Reference","text":"<p>This page documents all configuration files used by WildDetect for detection, census, and related operations.</p>"},{"location":"configs/wildetect/#overview","title":"Overview","text":"<p>Configuration files are located in the <code>config/</code> directory and use YAML format for easy editing.</p>"},{"location":"configs/wildetect/#configuration-files","title":"Configuration Files","text":"File Purpose detection.yaml Main detection configuration census.yaml Census campaign configuration benchmark.yaml Performance benchmarking visualization.yaml Visualization settings extract-gps.yaml GPS extraction configuration detector_registration.yaml Model registration config.yaml Main application config class_mapping.json Class ID to name mapping"},{"location":"configs/wildetect/#detectionyaml","title":"detection.yaml","text":"<p>Purpose: Configure wildlife detection pipeline parameters.</p> <p>Location: <code>config/detection.yaml</code></p>"},{"location":"configs/wildetect/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code># Model Configuration\nmodel:\n  mlflow_model_name: \"detector\"          # Model name in MLflow registry\n  mlflow_model_alias: \"production\"       # Model version alias\n  mlflow_model_version: null             # Specific version (overrides alias)\n  model_path: null                       # Direct path to model file (alternative to MLflow)\n  device: \"cuda\"                         # Device: \"cuda\", \"cpu\", \"auto\"\n\n# Input Sources (choose one)\nimage_paths:                             # List of specific image paths\n  - \"path/to/image1.jpg\"\n  - \"path/to/image2.tif\"\n\nimage_dir: null                          # Directory containing images\n\n# EXIF GPS Update (optional)\nexif_gps_update:\n  image_folder: null                     # Folder with images to update\n  csv_path: null                         # CSV with GPS coordinates\n  skip_rows: 4                           # Rows to skip in CSV\n  filename_col: \"filename\"               # Column name for filenames\n  lat_col: \"latitude\"                    # Column name for latitude\n  lon_col: \"longitude\"                   # Column name for longitude\n  alt_col: \"altitude\"                    # Column name for altitude\n\n# Label Studio Integration (optional)\nlabelstudio:\n  url: null                              # Label Studio URL\n  api_key: null                          # API key\n  project_id: null                       # Project ID\n  download_resources: false              # Download images from LS\n\n# Processing Configuration\nprocessing:\n  batch_size: 32                         # Batch size for inference\n  tile_size: 800                         # Tile size for large images (pixels)\n  overlap_ratio: 0.2                     # Overlap ratio between tiles (0.0-1.0)\n  pipeline_type: \"raster\"                # Pipeline: \"raster\", \"multithreaded\", \"simple\", \"default\"\n  queue_size: 64                         # Queue size for multithreaded pipeline\n  num_data_workers: 1                    # Number of data loading workers\n  num_inference_workers: 1               # Number of inference workers (multiprocessing)\n  prefetch_factor: 2                     # Batches to prefetch per worker\n  pin_memory: true                       # Pin memory for faster GPU transfer\n  nms_threshold: 0.5                     # NMS threshold for detection stitching\n  max_errors: 5                          # Max errors before stopping\n  confidence_threshold: 0.5              # Minimum confidence for detections\n\n# Flight Specifications\nflight_specs:\n  sensor_height: 24                      # Camera sensor height (mm)\n  focal_length: 35.0                     # Lens focal length (mm)\n  flight_height: 180.0                   # Flight altitude (meters)\n  gsd: 2.38                              # Ground Sample Distance (cm/pixel) - REQUIRED for raster\n\n# Inference Service (optional)\ninference_service:\n  url: null                              # URL for external inference service\n  # Example: \"http://localhost:4141/predict\"\n  timeout: 60                            # Request timeout (seconds)\n\n# Profiling Configuration\nprofiling:\n  enable: false                          # Enable profiling\n  memory_profile: false                  # Profile memory usage\n  line_profile: false                    # Line-by-line profiling\n  gpu_profile: false                     # GPU memory profiling\n\n# Output Configuration\noutput:\n  directory: \"results\"                   # Output directory for results\n  dataset_name: null                     # FiftyOne dataset name (null to disable)\n  save_visualizations: true              # Save visualization images\n  save_crops: false                      # Save detection crops\n  export_formats: [\"json\", \"csv\"]        # Export formats\n\n# Logging Configuration\nlogging:\n  verbose: false                         # Verbose logging\n  log_file: null                         # Log file path (null for default)\n  log_level: \"INFO\"                      # Log level: DEBUG, INFO, WARNING, ERROR\n</code></pre>"},{"location":"configs/wildetect/#example-configurations","title":"Example Configurations","text":""},{"location":"configs/wildetect/#basic-detection","title":"Basic Detection","text":"<pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nimage_dir: \"D:/images/survey/\"\n\nprocessing:\n  batch_size: 32\n  pipeline_type: \"simple\"\n\noutput:\n  directory: \"results/detections\"\n</code></pre>"},{"location":"configs/wildetect/#raster-detection-large-geotiff","title":"Raster Detection (Large GeoTIFF)","text":"<pre><code>model:\n  mlflow_model_name: \"detector\"\n  device: \"cuda\"\n\nimage_paths:\n  - \"D:/orthomosaics/ortho_large.tif\"\n\nprocessing:\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"raster\"\n  nms_threshold: 0.5\n\nflight_specs:\n  gsd: 2.38  # Required for raster\n\noutput:\n  directory: \"results/raster_detections\"\n</code></pre>"},{"location":"configs/wildetect/#censusyaml","title":"census.yaml","text":"<p>Purpose: Configure wildlife census campaigns with statistics and analysis.</p> <p>Location: <code>config/census.yaml</code></p>"},{"location":"configs/wildetect/#complete-parameter-reference_1","title":"Complete Parameter Reference","text":"<pre><code># Campaign Information\ncampaign:\n  name: \"Summer_2024_Survey\"             # Campaign name\n  target_species: [\"elephant\", \"giraffe\", \"zebra\"]  # Target species list\n  area_name: \"Serengeti_North\"           # Survey area name\n  start_date: \"2024-06-01\"               # Campaign start date\n  end_date: \"2024-06-15\"                 # Campaign end date\n  pilot_name: null                       # Pilot name\n  notes: null                            # Additional notes\n\n# Model Configuration (same as detection.yaml)\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\n# Image Sources\nimage_dir: \"D:/census_images/\"           # Directory with survey images\n\n# Processing (same as detection.yaml)\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"raster\"\n  nms_threshold: 0.5\n\n# Flight Specifications\nflight_specs:\n  sensor_height: 24\n  focal_length: 35.0\n  flight_height: 120.0                   # Flight altitude (meters)\n  gsd: 2.38                              # Ground Sample Distance (cm/pixel)\n\n# Analysis Configuration\nanalysis:\n  calculate_density: true                # Calculate population density\n  density_unit: \"per_km2\"                # Density unit: \"per_km2\", \"per_hectare\"\n  detect_hotspots: true                  # Identify concentration hotspots\n  hotspot_radius: 500                    # Hotspot radius (meters)\n  create_maps: true                      # Generate geographic maps\n  coverage_analysis: true                # Analyze survey coverage\n  species_distribution: true             # Species distribution analysis\n  co_occurrence_analysis: true           # Species co-occurrence\n\n# Statistics Configuration\nstatistics:\n  confidence_bins: [0.5, 0.7, 0.9]       # Confidence bins for analysis\n  size_bins: [50, 100, 200]              # Object size bins (pixels)\n  group_size_analysis: true              # Analyze group sizes\n\n# Visualization Settings\nvisualization:\n  create_heatmaps: true                  # Create density heatmaps\n  create_distribution_maps: true         # Create distribution maps\n  create_flight_path_map: true           # Show flight path\n  overlay_detections: true               # Overlay detections on maps\n  color_by_species: true                 # Color code by species\n\n# Output Configuration\noutput:\n  directory: \"census_results\"            # Output directory\n  dataset_name: \"census_2024\"            # FiftyOne dataset name\n  generate_pdf_report: true              # Generate PDF report\n  generate_excel: true                   # Generate Excel statistics\n  save_individual_reports: true          # Save per-image reports\n\n# Reporting\nreport:\n  include_methodology: true              # Include methodology section\n  include_confidence_analysis: true      # Include confidence analysis\n  include_temporal_analysis: false       # Include time-based analysis\n  executive_summary: true                # Include executive summary\n  detailed_statistics: true              # Include detailed stats\n</code></pre>"},{"location":"configs/wildetect/#example-configurations_1","title":"Example Configurations","text":""},{"location":"configs/wildetect/#basic-census","title":"Basic Census","text":"<pre><code>campaign:\n  name: \"Quick_Survey_2024\"\n  target_species: [\"elephant\"]\n\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n\nimage_dir: \"D:/survey/\"\n\nanalysis:\n  calculate_density: true\n  create_maps: true\n\noutput:\n  directory: \"census_results\"\n  generate_pdf_report: true\n</code></pre>"},{"location":"configs/wildetect/#comprehensive-census","title":"Comprehensive Census","text":"<pre><code>campaign:\n  name: \"Annual_Census_2024\"\n  target_species: [\"elephant\", \"giraffe\", \"zebra\", \"buffalo\"]\n  area_name: \"Protected_Area_North\"\n  start_date: \"2024-06-01\"\n  pilot_name: \"John Doe\"\n\nmodel:\n  mlflow_model_name: \"detector\"\n  device: \"cuda\"\n\nimage_dir: \"D:/annual_census/images/\"\n\nflight_specs:\n  flight_height: 150.0\n  gsd: 3.0\n\nanalysis:\n  calculate_density: true\n  detect_hotspots: true\n  species_distribution: true\n  co_occurrence_analysis: true\n\noutput:\n  directory: \"census_2024\"\n  generate_pdf_report: true\n  generate_excel: true\n</code></pre>"},{"location":"configs/wildetect/#benchmarkyaml","title":"benchmark.yaml","text":"<p>Purpose: Configure performance benchmarking tests.</p> <p>Location: <code>config/benchmark.yaml</code></p>"},{"location":"configs/wildetect/#parameter-reference","title":"Parameter Reference","text":"<pre><code>benchmark:\n  test_images: [\"test1.jpg\", \"test2.tif\"]  # Images to benchmark\n  iterations: 10                            # Number of iterations\n  warmup_iterations: 2                      # Warmup runs\n  measure_memory: true                      # Measure memory usage\n  measure_gpu: true                         # Measure GPU utilization\n\nmodels:\n  - name: \"yolo11n\"\n    path: \"models/yolo11n.pt\"\n  - name: \"yolo11s\"\n    path: \"models/yolo11s.pt\"\n\nconfigurations:\n  - batch_size: 1\n  - batch_size: 8\n  - batch_size: 32\n</code></pre>"},{"location":"configs/wildetect/#visualizationyaml","title":"visualization.yaml","text":"<p>Purpose: Configure visualization settings.</p> <p>Location: <code>config/visualization.yaml</code></p>"},{"location":"configs/wildetect/#parameter-reference_1","title":"Parameter Reference","text":"<pre><code>visualization:\n  bbox_color: \"red\"                      # Bounding box color\n  bbox_thickness: 2                      # Box line thickness\n  show_labels: true                      # Show class labels\n  show_confidence: true                  # Show confidence scores\n  font_size: 12                          # Font size for labels\n  dpi: 300                               # Output DPI for images\n\nmaps:\n  basemap: \"OpenStreetMap\"               # Basemap provider\n  zoom_level: 12                         # Default zoom level\n  marker_size: 8                         # Marker size\n</code></pre>"},{"location":"configs/wildetect/#extract-gpsyaml","title":"extract-gps.yaml","text":"<p>Purpose: Configure GPS coordinate extraction from images.</p> <p>Location: <code>config/extract-gps.yaml</code></p>"},{"location":"configs/wildetect/#parameter-reference_2","title":"Parameter Reference","text":"<pre><code>gps_extraction:\n  image_directory: \"D:/images/\"          # Directory with images\n  output_file: \"gps_coordinates.csv\"     # Output CSV file\n  recursive: true                        # Search subdirectories\n  include_images_without_gps: false      # Include images without GPS\n\nformat:\n  decimal_places: 6                      # GPS coordinate precision\n  date_format: \"%Y-%m-%d %H:%M:%S\"       # Date format\n</code></pre>"},{"location":"configs/wildetect/#detector_registrationyaml","title":"detector_registration.yaml","text":"<p>Purpose: Configure model registration to MLflow.</p> <p>Location: <code>config/detector_registration.yaml</code></p>"},{"location":"configs/wildetect/#parameter-reference_3","title":"Parameter Reference","text":"<pre><code>registration:\n  model_path: \"models/best.pt\"           # Path to model weights\n  model_name: \"wildlife_detector\"        # Model name in registry\n  model_type: \"detector\"                 # \"detector\" or \"classifier\"\n\n  description: |\n    YOLO11n model trained on aerial wildlife images\n    Dataset: Wildlife Aerial v2.0\n    Training date: 2024-01-15\n\n  tags:\n    framework: \"yolo\"\n    version: \"11n\"\n    dataset: \"wildlife_v2\"\n    map50: \"0.89\"\n    map50_95: \"0.76\"\n\n  aliases:\n    - \"production\"\n    - \"latest\"\n    - \"v2.0\"\n\n  artifacts:\n    - \"configs/training_config.yaml\"\n    - \"logs/training.log\"\n</code></pre>"},{"location":"configs/wildetect/#configyaml","title":"config.yaml","text":"<p>Purpose: Main application configuration.</p> <p>Location: <code>config/config.yaml</code></p>"},{"location":"configs/wildetect/#parameter-reference_4","title":"Parameter Reference","text":"<pre><code>app:\n  name: \"WildDetect\"\n  version: \"1.0.0\"\n\nmlflow:\n  tracking_uri: \"http://localhost:5000\"\n  experiment_name: \"wildlife_detection\"\n\npaths:\n  data_root: \"D:/data/\"\n  models_root: \"D:/models/\"\n  results_root: \"D:/results/\"\n\ndefaults:\n  device: \"cuda\"\n  batch_size: 32\n  confidence_threshold: 0.5\n</code></pre>"},{"location":"configs/wildetect/#class_mappingjson","title":"class_mapping.json","text":"<p>Purpose: Map class IDs to class names.</p> <p>Location: <code>config/class_mapping.json</code></p>"},{"location":"configs/wildetect/#format","title":"Format","text":"<pre><code>{\n  \"0\": \"elephant\",\n  \"1\": \"giraffe\",\n  \"2\": \"zebra\",\n  \"3\": \"buffalo\",\n  \"4\": \"wildebeest\"\n}\n</code></pre>"},{"location":"configs/wildetect/#configuration-best-practices","title":"Configuration Best Practices","text":""},{"location":"configs/wildetect/#1-use-environment-variables","title":"1. Use Environment Variables","text":"<p>For sensitive data:</p> <pre><code>mlflow:\n  tracking_uri: ${MLFLOW_TRACKING_URI}\n\nlabelstudio:\n  api_key: ${LABEL_STUDIO_API_KEY}\n</code></pre>"},{"location":"configs/wildetect/#2-create-config-variants","title":"2. Create Config Variants","text":"<p>For different scenarios:</p> <pre><code>config/\n\u251c\u2500\u2500 detection.yaml          # Default\n\u251c\u2500\u2500 detection_dev.yaml      # Development\n\u251c\u2500\u2500 detection_prod.yaml     # Production\n\u2514\u2500\u2500 detection_test.yaml     # Testing\n</code></pre>"},{"location":"configs/wildetect/#3-document-custom-settings","title":"3. Document Custom Settings","text":"<p>Add comments to configs:</p> <pre><code>processing:\n  batch_size: 16  # Reduced for 8GB GPU\n  tile_size: 640  # Smaller tiles for memory efficiency\n</code></pre>"},{"location":"configs/wildetect/#4-version-control","title":"4. Version Control","text":"<p>Track configuration changes:</p> <pre><code>git add config/\ngit commit -m \"Update detection config for new model\"\n</code></pre>"},{"location":"configs/wildetect/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configs/wildetect/#invalid-configuration","title":"Invalid Configuration","text":"<p>Issue: Config validation fails</p> <p>Solutions: 1. Check YAML syntax (indentation, colons) 2. Verify required fields are present 3. Check data types match expected types 4. Use YAML validator online</p>"},{"location":"configs/wildetect/#path-not-found","title":"Path Not Found","text":"<p>Issue: Image paths or model paths not found</p> <p>Solutions: 1. Use absolute paths 2. Check path separators (use forward slashes) 3. Verify files exist 4. Check permissions</p>"},{"location":"configs/wildetect/#model-loading-fails","title":"Model Loading Fails","text":"<p>Issue: Can't load model from MLflow</p> <p>Solutions: 1. Verify MLflow server is running 2. Check <code>mlflow_model_name</code> is correct 3. Verify model exists in registry 4. Check <code>MLFLOW_TRACKING_URI</code> environment variable</p>"},{"location":"configs/wildetect/#next-steps","title":"Next Steps","text":"<ul> <li>WildDetect Scripts</li> <li>Detection Tutorial</li> <li>Census Tutorial</li> <li>CLI Reference</li> </ul>"},{"location":"configs/wildtrain/","title":"WildTrain Configuration Reference","text":"<p>Documentation for all WildTrain configuration files used in model training.</p>"},{"location":"configs/wildtrain/#configuration-structure","title":"Configuration Structure","text":"<p>WildTrain uses Hydra for hierarchical configuration management.</p> <pre><code>configs/\n\u251c\u2500\u2500 classification/          # Classification configs\n\u2502   \u251c\u2500\u2500 classification_train.yaml\n\u2502   \u251c\u2500\u2500 classification_eval.yaml\n\u2502   \u251c\u2500\u2500 classification_sweep.yaml\n\u2502   \u2514\u2500\u2500 classification_pipeline_config.yaml\n\u251c\u2500\u2500 detection/              # Detection configs\n\u2502   \u251c\u2500\u2500 yolo_configs/\n\u2502   \u2502   \u251c\u2500\u2500 yolo.yaml\n\u2502   \u2502   \u251c\u2500\u2500 yolo_eval.yaml\n\u2502   \u2502   \u2514\u2500\u2500 data/demo.yaml\n\u2502   \u2514\u2500\u2500 mmdet_configs/\n\u2502       \u251c\u2500\u2500 mmdet.yaml\n\u2502       \u2514\u2500\u2500 [various model configs]\n\u251c\u2500\u2500 datapreparation/        # Data prep configs\n\u2502   \u251c\u2500\u2500 import-config-example.yaml\n\u2502   \u2514\u2500\u2500 savmap.yaml\n\u251c\u2500\u2500 registration/           # Model registration\n\u2502   \u251c\u2500\u2500 classifier_registration_example.yaml\n\u2502   \u2514\u2500\u2500 detector_registration_example.yaml\n\u251c\u2500\u2500 main.yaml              # Main config\n\u2514\u2500\u2500 inference.yaml         # Inference config\n</code></pre>"},{"location":"configs/wildtrain/#classification-configs","title":"Classification Configs","text":""},{"location":"configs/wildtrain/#classification_trainyaml","title":"classification_train.yaml","text":"<p>Purpose: Configure classification model training</p> <pre><code>model:\n  architecture: \"resnet50\"\n  num_classes: 10\n  pretrained: true\n  learning_rate: 0.001\n  weight_decay: 0.0001\n  dropout: 0.5\n\ndata:\n  root_data_directory: \"D:/data/roi_dataset\"\n  split: \"train\"\n  batch_size: 32\n  num_workers: 4\n  image_size: 224\n\ntraining:\n  max_epochs: 100\n  accelerator: \"gpu\"\n  devices: 1\n  precision: 16\n  gradient_clip_val: 1.0\n\ncallbacks:\n  early_stopping:\n    monitor: \"val_loss\"\n    patience: 10\n  model_checkpoint:\n    monitor: \"val_acc\"\n    mode: \"max\"\n    save_top_k: 3\n\nmlflow:\n  experiment_name: \"classification\"\n  tracking_uri: \"http://localhost:5000\"\n</code></pre>"},{"location":"configs/wildtrain/#classification_evalyaml","title":"classification_eval.yaml","text":"<pre><code>model:\n  checkpoint_path: \"checkpoints/best.ckpt\"\n  mlflow_model_name: \"classifier\"\n\ndata:\n  root_data_directory: \"D:/data/roi_dataset\"\n  split: \"test\"\n  batch_size: 64\n\nevaluation:\n  save_predictions: true\n  generate_confusion_matrix: true\n</code></pre>"},{"location":"configs/wildtrain/#detection-configs","title":"Detection Configs","text":""},{"location":"configs/wildtrain/#yolo-configuration","title":"YOLO Configuration","text":"<p>File: <code>configs/detection/yolo_configs/yolo.yaml</code></p> <pre><code>model:\n  framework: \"yolo\"\n  size: \"n\"  # n, s, m, l, x\n  pretrained: true\n\ndata:\n  data_yaml: \"D:/data/wildlife/data.yaml\"\n\ntraining:\n  epochs: 100\n  imgsz: 640\n  batch: 16\n  optimizer: \"AdamW\"\n  lr0: 0.001\n  device: 0\n\naugmentation:\n  hsv_h: 0.015\n  hsv_s: 0.7\n  flipud: 0.0\n  fliplr: 0.5\n  mosaic: 1.0\n\nmlflow:\n  experiment_name: \"yolo_detection\"\n</code></pre> <p>YOLO Data YAML: <pre><code># data/wildlife/data.yaml\ntrain: ./images/train\nval: ./images/val\nnc: 3\nnames: ['elephant', 'giraffe', 'zebra']\n</code></pre></p>"},{"location":"configs/wildtrain/#mmdetection-configuration","title":"MMDetection Configuration","text":"<p>File: <code>configs/detection/mmdet_configs/mmdet.yaml</code></p> <pre><code>model:\n  framework: \"mmdet\"\n  config_file: \"configs/detection/mmdet_configs/faster_rcnn.py\"\n\ndata:\n  data_root: \"D:/data/coco_format\"\n  ann_file_train: \"train.json\"\n  ann_file_val: \"val.json\"\n\ntraining:\n  work_dir: \"work_dirs/faster_rcnn\"\n  max_epochs: 12\n</code></pre>"},{"location":"configs/wildtrain/#registration-configs","title":"Registration Configs","text":""},{"location":"configs/wildtrain/#classifier-registration","title":"Classifier Registration","text":"<p>File: <code>configs/registration/classifier_registration_example.yaml</code></p> <pre><code>model_path: \"checkpoints/best.ckpt\"\nmodel_name: \"wildlife_classifier\"\nmodel_type: \"classifier\"\n\ndescription: \"ResNet50 classifier for wildlife ROI\"\n\ntags:\n  architecture: \"resnet50\"\n  dataset: \"wildlife_roi_v1\"\n  accuracy: \"0.95\"\n\naliases:\n  - \"production\"\n</code></pre>"},{"location":"configs/wildtrain/#detector-registration","title":"Detector Registration","text":"<p>File: <code>configs/registration/detector_registration_example.yaml</code></p> <pre><code>model_path: \"runs/detect/train/weights/best.pt\"\nmodel_name: \"wildlife_detector\"\nmodel_type: \"detector\"\n\ndescription: \"YOLO11n for aerial wildlife\"\n\ntags:\n  framework: \"yolo\"\n  map50: \"0.89\"\n\naliases:\n  - \"production\"\n</code></pre>"},{"location":"configs/wildtrain/#inference-configuration","title":"Inference Configuration","text":"<p>File: <code>configs/inference.yaml</code></p> <pre><code>server:\n  host: \"0.0.0.0\"\n  port: 8000\n  workers: 2\n\nmodel:\n  mlflow_model_name: \"wildlife_detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\ninference:\n  batch_size: 8\n  confidence_threshold: 0.5\n  nms_threshold: 0.45\n</code></pre>"},{"location":"configs/wildtrain/#main-configuration","title":"Main Configuration","text":"<p>File: <code>configs/main.yaml</code></p> <pre><code>defaults:\n  - model: yolo\n  - data: detection\n  - training: default\n\nexperiment_name: \"wildlife_detection\"\nseed: 42\n</code></pre>"},{"location":"configs/wildtrain/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":""},{"location":"configs/wildtrain/#classification_sweepyaml","title":"classification_sweep.yaml","text":"<pre><code>method: bayes  # grid, random, bayes\nmetric:\n  name: val_acc\n  goal: maximize\n\nparameters:\n  learning_rate:\n    min: 0.0001\n    max: 0.01\n  batch_size:\n    values: [16, 32, 64]\n  architecture:\n    values: [\"resnet18\", \"resnet50\"]\n</code></pre>"},{"location":"configs/wildtrain/#best-practices","title":"Best Practices","text":"<ol> <li>Use MLflow for experiment tracking</li> <li>Save checkpoints frequently</li> <li>Enable early stopping to prevent overfitting</li> <li>Use mixed precision (precision: 16) for faster training</li> <li>Version your configs with git</li> </ol>"},{"location":"configs/wildtrain/#next-steps","title":"Next Steps","text":"<ul> <li>WildTrain Scripts</li> <li>Training Tutorial</li> <li>WildTrain CLI</li> </ul>"},{"location":"getting-started/environment-setup/","title":"Environment Setup","text":"<p>This guide covers setting up your environment for working with the WildDetect monorepo, including configuration files, environment variables, and external services.</p>"},{"location":"getting-started/environment-setup/#directory-structure","title":"Directory Structure","text":"<p>Create the following directory structure for your project:</p> <pre><code>your-project/\n\u251c\u2500\u2500 wildetect/          # Main package (cloned repo)\n\u251c\u2500\u2500 data/              # Data storage root\n\u2502   \u251c\u2500\u2500 raw/           # Original data\n\u2502   \u251c\u2500\u2500 processed/     # Processed datasets\n\u2502   \u2514\u2500\u2500 exports/       # Exported datasets\n\u251c\u2500\u2500 models/            # Trained models\n\u2502   \u251c\u2500\u2500 detectors/\n\u2502   \u2514\u2500\u2500 classifiers/\n\u251c\u2500\u2500 results/           # Detection results\n\u2502   \u251c\u2500\u2500 detections/\n\u2502   \u251c\u2500\u2500 census/\n\u2502   \u2514\u2500\u2500 visualizations/\n\u2514\u2500\u2500 mlruns/            # MLflow experiment tracking\n</code></pre>"},{"location":"getting-started/environment-setup/#environment-variables","title":"Environment Variables","text":""},{"location":"getting-started/environment-setup/#create-env-file","title":"Create .env File","text":"<p>Create a <code>.env</code> file in the root directory of each package:</p>"},{"location":"getting-started/environment-setup/#wilddetect-env","title":"WildDetect <code>.env</code>","text":"<pre><code># MLflow Configuration\nMLFLOW_TRACKING_URI=http://localhost:5000\nMLFLOW_EXPERIMENT_NAME=wilddetect\nMODEL_REGISTRY_PATH=models/\n\n# Data Paths\nDATA_ROOT=D:/data/\nRESULTS_ROOT=D:/results/\n\n# Label Studio (Optional)\nLABEL_STUDIO_URL=http://localhost:8080\nLABEL_STUDIO_API_KEY=your_api_key_here\nLABEL_STUDIO_PROJECT_ID=1\n\n# FiftyOne (Optional)\nFIFTYONE_DATABASE_DIR=D:/fiftyone/\nFIFTYONE_DEFAULT_DATASET_DIR=D:/data/fiftyone/\n\n# Inference Server\nINFERENCE_SERVER_HOST=0.0.0.0\nINFERENCE_SERVER_PORT=4141\n\n# GPU Configuration\nCUDA_VISIBLE_DEVICES=0\nPYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n\n# Logging\nLOG_LEVEL=INFO\nLOG_FILE=logs/wildetect.log\n</code></pre>"},{"location":"getting-started/environment-setup/#wildata-env","title":"WilData <code>.env</code>","text":"<pre><code># API Configuration\nWILDATA_API_HOST=0.0.0.0\nWILDATA_API_PORT=8441\nWILDATA_API_DEBUG=false\n\n# Data Storage\nDATA_ROOT=D:/data/\nDVC_REMOTE_URL=s3://my-bucket/datasets  # or local path\n\n# DVC Configuration\nDVC_CACHE_DIR=D:/.dvc/cache/\n\n# Label Studio Integration\nLABEL_STUDIO_URL=http://localhost:8080\nLABEL_STUDIO_API_KEY=your_api_key_here\n\n# Processing\nMAX_WORKERS=4\nBATCH_SIZE=32\n</code></pre>"},{"location":"getting-started/environment-setup/#wildtrain-env","title":"WildTrain <code>.env</code>","text":"<pre><code># MLflow Configuration\nMLFLOW_TRACKING_URI=http://localhost:5000\nMLFLOW_EXPERIMENT_NAME=wildtrain\n\n# Training Paths\nDATA_ROOT=D:/data/\nMODEL_OUTPUT_DIR=D:/models/\nCHECKPOINT_DIR=D:/checkpoints/\n\n# Hyperparameter Tuning\nOPTUNA_STORAGE=sqlite:///optuna.db\nN_TRIALS=50\n\n# Distributed Training (Optional)\nMASTER_ADDR=localhost\nMASTER_PORT=12355\nWORLD_SIZE=1\nRANK=0\n\n# GPU Configuration\nCUDA_VISIBLE_DEVICES=0,1  # Multiple GPUs\n</code></pre>"},{"location":"getting-started/environment-setup/#loading-environment-variables","title":"Loading Environment Variables","text":"<p>The packages automatically load <code>.env</code> files when using scripts:</p> <pre><code># Scripts automatically load .env\nscripts\\run_detection.bat\n\n# Or manually in Python\nfrom dotenv import load_dotenv\nload_dotenv()\n</code></pre>"},{"location":"getting-started/environment-setup/#external-services-setup","title":"External Services Setup","text":""},{"location":"getting-started/environment-setup/#mlflow-tracking-server","title":"MLflow Tracking Server","text":"<p>MLflow is used for experiment tracking and model registry.</p>"},{"location":"getting-started/environment-setup/#1-start-mlflow-server","title":"1. Start MLflow Server","text":"<pre><code># Launch using script\nscripts\\launch_mlflow.bat\n\n# Or manually\nmlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow.db\n</code></pre>"},{"location":"getting-started/environment-setup/#2-access-mlflow-ui","title":"2. Access MLflow UI","text":"<p>Open browser to: <code>http://localhost:5000</code></p>"},{"location":"getting-started/environment-setup/#3-configure-in-code","title":"3. Configure in Code","text":"<pre><code>import mlflow\n\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"my_experiment\")\n</code></pre>"},{"location":"getting-started/environment-setup/#label-studio-optional","title":"Label Studio (Optional)","text":"<p>For data annotation and labeling.</p>"},{"location":"getting-started/environment-setup/#1-install-label-studio","title":"1. Install Label Studio","text":"<pre><code>uv pip install label-studio\n</code></pre>"},{"location":"getting-started/environment-setup/#2-start-server","title":"2. Start Server","text":"<pre><code># Launch using script\nscripts\\launch_labelstudio.bat\n\n# Or manually\nlabel-studio start --port 8080\n</code></pre>"},{"location":"getting-started/environment-setup/#3-create-project","title":"3. Create Project","text":"<ol> <li>Navigate to <code>http://localhost:8080</code></li> <li>Create new project</li> <li>Upload images</li> <li>Configure labeling interface (use provided XML configs)</li> </ol>"},{"location":"getting-started/environment-setup/#4-get-api-key","title":"4. Get API Key","text":"<ol> <li>Go to Account &amp; Settings</li> <li>Copy your API token</li> <li>Add to <code>.env</code> file</li> </ol>"},{"location":"getting-started/environment-setup/#fiftyone-dataset-visualization","title":"FiftyOne (Dataset Visualization)","text":"<p>Interactive dataset viewer and analyzer.</p>"},{"location":"getting-started/environment-setup/#1-install-fiftyone","title":"1. Install FiftyOne","text":"<pre><code>uv pip install fiftyone\n</code></pre>"},{"location":"getting-started/environment-setup/#2-launch-viewer","title":"2. Launch Viewer","text":"<pre><code># Launch using script\nscripts\\launch_fiftyone.bat\n\n# Or using CLI\nwildetect fiftyone --action launch --dataset my_dataset\n</code></pre>"},{"location":"getting-started/environment-setup/#3-configure-database","title":"3. Configure Database","text":"<pre><code># Set database directory\nfiftyone config database_dir D:/fiftyone/db\n\n# Set default dataset directory\nfiftyone config default_dataset_dir D:/data/fiftyone\n</code></pre>"},{"location":"getting-started/environment-setup/#dvc-data-version-control","title":"DVC (Data Version Control)","text":"<p>For versioning large datasets.</p>"},{"location":"getting-started/environment-setup/#1-initialize-dvc","title":"1. Initialize DVC","text":"<pre><code>cd wildata\nscripts\\dvc-setup.bat\n\n# Or manually\ndvc init\ndvc remote add -d myremote s3://my-bucket/datasets\n</code></pre>"},{"location":"getting-started/environment-setup/#2-configure-remote-storage","title":"2. Configure Remote Storage","text":"Local StorageAWS S3Google Cloud StorageAzure Blob <pre><code>dvc remote add -d local D:/dvc-storage\n</code></pre> <pre><code>dvc remote add -d s3remote s3://my-bucket/datasets\n\n# Set credentials\ndvc remote modify s3remote access_key_id YOUR_KEY\ndvc remote modify s3remote secret_access_key YOUR_SECRET\n</code></pre> <pre><code>dvc remote add -d gcs gs://my-bucket/datasets\n\n# Set credentials\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json\n</code></pre> <pre><code>dvc remote add -d azure azure://container/path\n\n# Set credentials\nexport AZURE_STORAGE_CONNECTION_STRING=your_connection_string\n</code></pre>"},{"location":"getting-started/environment-setup/#3-track-data","title":"3. Track Data","text":"<pre><code># Add data to DVC\ndvc add data/raw/\n\n# Commit changes\ngit add data/raw.dvc .gitignore\ngit commit -m \"Add raw data\"\n\n# Push to remote\ndvc push\n</code></pre>"},{"location":"getting-started/environment-setup/#configuration-files","title":"Configuration Files","text":""},{"location":"getting-started/environment-setup/#wilddetect-configurations","title":"WildDetect Configurations","text":"<p>Location: <code>config/</code></p>"},{"location":"getting-started/environment-setup/#detectionyaml","title":"detection.yaml","text":"<p>Main detection configuration. Edit based on your needs:</p> <pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  overlap_ratio: 0.2\n</code></pre> <p>See Detection Config Reference for all options.</p>"},{"location":"getting-started/environment-setup/#censusyaml","title":"census.yaml","text":"<p>Census campaign configuration:</p> <pre><code>campaign:\n  name: \"Summer_2024\"\n  target_species: [\"elephant\", \"giraffe\", \"zebra\"]\n\nflight_specs:\n  flight_height: 120.0\n  gsd: 2.38\n</code></pre> <p>See Census Config Reference.</p>"},{"location":"getting-started/environment-setup/#wildata-configurations","title":"WilData Configurations","text":"<p>Location: <code>wildata/configs/</code></p>"},{"location":"getting-started/environment-setup/#import-config-exampleyaml","title":"import-config-example.yaml","text":"<p>Dataset import configuration:</p> <pre><code>source_path: \"annotations.json\"\nsource_format: \"coco\"\ndataset_name: \"my_dataset\"\n\ntransformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n</code></pre> <p>See Import Config Reference.</p>"},{"location":"getting-started/environment-setup/#wildtrain-configurations","title":"WildTrain Configurations","text":"<p>Location: <code>wildtrain/configs/</code></p>"},{"location":"getting-started/environment-setup/#training-config","title":"Training Config","text":"<p>For model training:</p> <pre><code># configs/classification/classification_train.yaml\nmodel:\n  architecture: \"resnet50\"\n  num_classes: 10\n\ntraining:\n  epochs: 100\n  batch_size: 32\n  learning_rate: 0.001\n</code></pre> <p>See Training Configs.</p>"},{"location":"getting-started/environment-setup/#gpu-configuration","title":"GPU Configuration","text":""},{"location":"getting-started/environment-setup/#cuda-setup","title":"CUDA Setup","text":""},{"location":"getting-started/environment-setup/#check-cuda-availability","title":"Check CUDA Availability","text":"<pre><code>python -c \"import torch; print(f'CUDA: {torch.cuda.is_available()}')\"\npython -c \"import torch; print(f'Device: {torch.cuda.get_device_name(0)}')\"\n</code></pre>"},{"location":"getting-started/environment-setup/#set-gpu-device","title":"Set GPU Device","text":"<p>In <code>.env</code>: <pre><code>CUDA_VISIBLE_DEVICES=0  # Use first GPU\nCUDA_VISIBLE_DEVICES=0,1  # Use first two GPUs\n</code></pre></p> <p>In config files: <pre><code>device: \"cuda\"  # Use default GPU\ndevice: \"cuda:0\"  # Specific GPU\ndevice: \"cpu\"  # Force CPU\n</code></pre></p>"},{"location":"getting-started/environment-setup/#memory-management","title":"Memory Management","text":"<p>For large models or images:</p> <pre><code># In .env\nPYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n\n# Or in Python\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n</code></pre>"},{"location":"getting-started/environment-setup/#cpu-only-setup","title":"CPU-Only Setup","text":"<p>If you don't have a GPU:</p> <ol> <li>Install CPU-only PyTorch (see Installation)</li> <li>Set <code>device: \"cpu\"</code> in all config files</li> <li>Reduce batch sizes for memory efficiency</li> </ol>"},{"location":"getting-started/environment-setup/#testing-your-setup","title":"Testing Your Setup","text":""},{"location":"getting-started/environment-setup/#run-system-info","title":"Run System Info","text":"<pre><code>wildetect info\n</code></pre> <p>This will display: - Python version - Package versions - CUDA availability - GPU information - Memory available</p>"},{"location":"getting-started/environment-setup/#test-detection","title":"Test Detection","text":"<pre><code># Test with a single image\nwildetect detect test_image.jpg --model model.pt --output test_results/\n</code></pre>"},{"location":"getting-started/environment-setup/#test-data-import","title":"Test Data Import","text":"<pre><code># Test data import\nwildata import-dataset test_annotations.json --format coco --name test_dataset\n</code></pre>"},{"location":"getting-started/environment-setup/#test-training","title":"Test Training","text":"<pre><code># Test training setup\ncd wildtrain\nwildtrain train classifier -c configs/classification/classification_train.yaml --dry-run\n</code></pre>"},{"location":"getting-started/environment-setup/#ide-setup","title":"IDE Setup","text":""},{"location":"getting-started/environment-setup/#vscode-configuration","title":"VSCode Configuration","text":"<p>Create <code>.vscode/settings.json</code>:</p> <pre><code>{\n  \"python.defaultInterpreterPath\": \".venv/Scripts/python.exe\",\n  \"python.formatting.provider\": \"black\",\n  \"python.linting.enabled\": true,\n  \"python.linting.ruffEnabled\": true,\n  \"python.testing.pytestEnabled\": true,\n  \"python.testing.pytestArgs\": [\n    \"tests\",\n    \"-v\"\n  ],\n  \"files.exclude\": {\n    \"**/__pycache__\": true,\n    \"**/*.pyc\": true\n  }\n}\n</code></pre>"},{"location":"getting-started/environment-setup/#ruff-configuration","title":"Ruff Configuration","text":"<p>The project uses ruff for linting. Configuration is in <code>pyproject.toml</code>:</p> <pre><code># Run ruff on all files\nuv run ruff check src/ tests/\n\n# Auto-fix issues\nuv run ruff check --fix src/ tests/\n</code></pre>"},{"location":"getting-started/environment-setup/#directory-permissions-windows","title":"Directory Permissions (Windows)","text":"<p>Ensure you have write permissions for: - Data directories - Model directories - Results directories - Log directories</p> <p>Run PowerShell as Administrator if needed:</p> <pre><code># Grant full control to current user\nicacls \"D:\\data\" /grant %USERNAME%:F /t\n</code></pre>"},{"location":"getting-started/environment-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/environment-setup/#common-issues","title":"Common Issues","text":"MLflow server won't start <p>Check if port 5000 is already in use: <pre><code>netstat -ano | findstr :5000\n</code></pre></p> <p>Use a different port: <pre><code>mlflow server --port 5001\n</code></pre></p> DVC push fails <p>Verify remote credentials: <pre><code>dvc remote list\ndvc remote modify --local myremote access_key_id YOUR_KEY\n</code></pre></p> Out of memory errors <p>Reduce batch size and tile size: <pre><code>processing:\n  batch_size: 16  # Reduced\n  tile_size: 640  # Reduced\n</code></pre></p> Import errors <p>Verify virtual environment is activated: <pre><code>which python  # Should point to .venv\n</code></pre></p>"},{"location":"getting-started/environment-setup/#next-steps","title":"Next Steps","text":"<p>Now that your environment is set up:</p> <ol> <li>\u2705 Test your setup with the commands above</li> <li>\ud83d\udcda Follow the Quick Start Guide</li> <li>\ud83c\udfaf Try an End-to-End Detection tutorial</li> </ol> <p>Environment ready? Head to the Quick Start Guide to run your first detection!</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will help you install all three packages in the WildDetect monorepo: WilData, WildTrain, and WildDetect.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.9, 3.10, or 3.11</li> <li>Package Manager: uv (recommended) or pip</li> <li>Git: For cloning repositories</li> <li>GPU (optional): CUDA-capable GPU for faster inference and training</li> </ul>"},{"location":"getting-started/installation/#operating-system","title":"Operating System","text":"<ul> <li>Windows 10/11</li> <li>Linux (Ubuntu 20.04+ recommended)</li> <li>macOS (Intel or Apple Silicon)</li> </ul> <p>Windows Users</p> <p>This monorepo is developed and tested on Windows. All scripts use <code>.bat</code> format for Windows compatibility.</p>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":"Method 1: Install from Source (Recommended)Method 2: Install from GitHubMethod 3: Using uv sync <p>You can install packages directly from GitHub:</p> <pre><code># Install WilData\nuv pip install git+https://github.com/fadelmamar/wildata\n\n# Install WildTrain\nuv pip install git+https://github.com/fadelmamar/wildtrain\n\n# Install WildDetect\nuv pip install git+https://github.com/fadelmamar/wildetect\n</code></pre> <p>If packages have <code>uv.lock</code> files:</p> <pre><code># In each package directory\ncd wildata\nuv sync\n\ncd ../wildtrain\nuv sync\n\ncd ../\nuv sync\n</code></pre>"},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/fadelmamar/wildetect.git\ncd wildetect\n</code></pre>"},{"location":"getting-started/installation/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<p>Using <code>uv</code> (recommended): <pre><code>uv venv --python 3.10\n\n# Activate on Windows\n.venv\\Scripts\\activate\n\n# Activate on Linux/macOS\nsource .venv/bin/activate\n</code></pre></p>"},{"location":"getting-started/installation/#3-install-pytorch-gpu-or-cpu","title":"3. Install PyTorch (GPU or CPU)","text":"<p>With CUDA 11.8 (GPU): <pre><code>uv pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p> <p>CPU Only: <pre><code>uv pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n</code></pre></p>"},{"location":"getting-started/installation/#4-install-wilddetect-packages","title":"4. Install WildDetect Packages","text":"<p>Install all three packages in development mode: <pre><code># Install WilData\ncd wildata\nuv pip install -e .\ncd ..\n\n# Install WildTrain\ncd wildtrain\nuv pip install -e .\ncd ..\n\n# Install WildDetect (main package)\nuv pip install -e .\n</code></pre></p>"},{"location":"getting-started/installation/#5-install-mmdetection-optional","title":"5. Install MMDetection (Optional)","text":"<p>If you want to use MMDetection framework: <pre><code># Install OpenMMLab dependencies\nuv pip install -U openmim\nuv run mim install mmengine\n\n# Install MMCV (choose based on your setup)\n# For CPU:\nuv pip install mmcv==2.0.1 -f https://download.openmmlab.com/mmcv/dist/cpu/torch2.1/index.html\n\n# For CUDA 11.8:\nuv pip install mmcv==2.0.1 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.1/index.html\n\n# Install MMDetection\nuv run mim install mmdet\nuv pip install numpy==1.26.4\n</code></pre></p>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Verify your installation by checking package versions:</p> <pre><code># Check WilData\nwildata --version\n\n# Check WildTrain  \nwildtrain --version\n\n# Check WildDetect\nwildetect --version\n</code></pre> <p>You should also be able to import the packages in Python:</p> <pre><code>import wildata\nimport wildtrain\nimport wildetect\n\nprint(f\"WilData: {wildata.__version__}\")\nprint(f\"WildTrain: {wildtrain.__version__}\")\nprint(f\"WildDetect: {wildetect.__version__}\")\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#dvc-data-version-control","title":"DVC (Data Version Control)","text":"<p>For dataset versioning with WilData:</p> <pre><code># Basic DVC\nuv pip install \"wildata[dvc]\"\n\n# With cloud storage support\nuv pip install \"dvc[s3]\"      # AWS S3\nuv pip install \"dvc[gcs]\"     # Google Cloud Storage\nuv pip install \"dvc[azure]\"   # Azure Blob Storage\n</code></pre>"},{"location":"getting-started/installation/#label-studio-integration","title":"Label Studio Integration","text":"<p>For working with Label Studio annotations:</p> <pre><code>uv pip install label-studio-sdk\n</code></pre>"},{"location":"getting-started/installation/#fiftyone-visualization","title":"FiftyOne Visualization","text":"<p>For interactive dataset visualization:</p> <pre><code>uv pip install fiftyone\n</code></pre>"},{"location":"getting-started/installation/#gpu-setup","title":"GPU Setup","text":""},{"location":"getting-started/installation/#cuda-configuration","title":"CUDA Configuration","text":"<p>If you have an NVIDIA GPU, ensure CUDA is properly installed:</p> <ol> <li> <p>Check CUDA availability: <pre><code>python -c \"import torch; print(f'CUDA Available: {torch.cuda.is_available()}')\"\npython -c \"import torch; print(f'CUDA Version: {torch.version.cuda}')\"\n</code></pre></p> </li> <li> <p>Check GPU devices: <pre><code>python -c \"import torch; print(f'GPU Count: {torch.cuda.device_count()}')\"\npython -c \"import torch; print(f'GPU Name: {torch.cuda.get_device_name(0)}')\"\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#memory-requirements","title":"Memory Requirements","text":"Task Minimum RAM Recommended RAM GPU Memory Detection 8GB 16GB 4GB Training 16GB 32GB 8GB Large Rasters 32GB 64GB 8GB+"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"Import errors after installation <p>Make sure your virtual environment is activated: <pre><code># Windows\n.venv\\Scripts\\activate\n\n# Linux/macOS\nsource .venv/bin/activate\n</code></pre></p> CUDA out of memory <p>Reduce batch size or tile size in your configuration files: <pre><code>processing:\n  batch_size: 16  # Reduce from 32\n  tile_size: 640  # Reduce from 800\n</code></pre></p> MMDetection installation fails <p>Install dependencies in this order: 1. PyTorch 2. MMCV (matching your CUDA version) 3. MMEngine 4. MMDetection</p> uv command not found <p>Install uv package manager: <pre><code># Windows (PowerShell)\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Linux/macOS\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></p>"},{"location":"getting-started/installation/#windows-specific-issues","title":"Windows-Specific Issues","text":"<p>ProcessPool Not Supported</p> <p>On Windows, multiprocessing with <code>ProcessPoolExecutor</code> is not supported. The packages automatically use threading instead.</p>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the Troubleshooting Guide</li> <li>Search GitHub Issues</li> <li>Create a new issue with your error message and system info</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once installation is complete:</p> <ol> <li>\ud83d\udcda Set up your environment</li> <li>\ud83d\ude80 Follow the Quick Start guide</li> <li>\ud83d\udcd6 Explore tutorials</li> </ol> <p>Installation successful? Head to the Environment Setup to configure your workspace.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>Get up and running with WildDetect in minutes! This guide shows you the fastest path to running your first wildlife detection.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>\u2705 Installed all packages (Installation Guide)</li> <li>\u2705 Activated your Python environment</li> <li>\u2705 Some aerial images to process</li> <li>\u2705 A pre-trained model (or use our example)</li> </ul>"},{"location":"getting-started/quick-start/#quick-start-detection","title":"Quick Start: Detection","text":""},{"location":"getting-started/quick-start/#1-using-the-cli","title":"1. Using the CLI","text":"<p>The simplest way to run detection:</p> <pre><code>wildetect detect /path/to/images --model model.pt --output results/\n</code></pre>"},{"location":"getting-started/quick-start/#2-using-a-script-windows","title":"2. Using a Script (Windows)","text":"<p>Edit the configuration file, then run:</p> <pre><code>cd wildetect\nscripts\\run_detection.bat\n</code></pre>"},{"location":"getting-started/quick-start/#3-using-python","title":"3. Using Python","text":"<pre><code>from wildetect.core.detection import DetectionPipeline\n\n# Initialize pipeline\npipeline = DetectionPipeline(\n    model_path=\"model.pt\",\n    device=\"cuda\"  # or \"cpu\"\n)\n\n# Run detection\nresults = pipeline.detect_batch(\"/path/to/images\")\n\n# Save results\npipeline.save_results(results, \"results/detections.json\")\n</code></pre>"},{"location":"getting-started/quick-start/#quick-start-census-campaign","title":"Quick Start: Census Campaign","text":"<p>Run a complete census analysis:</p> <pre><code>wildetect census campaign_2024 /path/to/images \\\n    --model model.pt \\\n    --output campaign_results/ \\\n    --species \"elephant,giraffe,zebra\"\n</code></pre> <p>This will: - \u2705 Detect all animals in your images - \u2705 Generate population statistics - \u2705 Create geographic visualizations - \u2705 Export reports in JSON and CSV</p>"},{"location":"getting-started/quick-start/#quick-start-data-management","title":"Quick Start: Data Management","text":""},{"location":"getting-started/quick-start/#import-a-dataset","title":"Import a Dataset","text":"<pre><code># Import COCO format\nwildata import-dataset annotations.json \\\n    --format coco \\\n    --name my_dataset\n\n# Import YOLO format\nwildata import-dataset data.yaml \\\n    --format yolo \\\n    --name my_dataset\n</code></pre>"},{"location":"getting-started/quick-start/#visualize-data","title":"Visualize Data","text":"<pre><code># Launch FiftyOne viewer\nwildetect fiftyone --action launch --dataset my_dataset\n\n# Or use the script\nscripts\\launch_fiftyone.bat\n</code></pre>"},{"location":"getting-started/quick-start/#quick-start-model-training","title":"Quick Start: Model Training","text":""},{"location":"getting-started/quick-start/#train-a-classifier","title":"Train a Classifier","text":"<pre><code>cd wildtrain\nwildtrain train classifier -c configs/classification/classification_train.yaml\n</code></pre>"},{"location":"getting-started/quick-start/#train-a-detector-yolo","title":"Train a Detector (YOLO)","text":"<pre><code>wildtrain train detector -c configs/detection/yolo_configs/yolo.yaml\n</code></pre>"},{"location":"getting-started/quick-start/#using-the-web-ui","title":"Using the Web UI","text":"<p>Each package has a Streamlit-based web interface:</p>"},{"location":"getting-started/quick-start/#wilddetect-ui","title":"WildDetect UI","text":"<pre><code>wildetect ui\n# Or: scripts\\launch_ui.bat\n</code></pre> <p>Features: - Run detections interactively - Configure detection parameters - View results in real-time - Export to various formats</p>"},{"location":"getting-started/quick-start/#wildata-ui","title":"WilData UI","text":"<pre><code>cd wildata\nstreamlit run src/wildata/ui.py\n# Or: launch_ui.bat\n</code></pre> <p>Features: - Import and export datasets - Create ROI datasets - Update GPS metadata - Visualize data</p>"},{"location":"getting-started/quick-start/#wildtrain-ui","title":"WildTrain UI","text":"<pre><code>cd wildtrain\nstreamlit run src/wildtrain/ui.py\n# Or: launch_ui.bat\n</code></pre> <p>Features: - Configure training runs - Monitor training progress - Evaluate models - Register models to MLflow</p>"},{"location":"getting-started/quick-start/#configuration-files","title":"Configuration Files","text":"<p>All operations can be configured via YAML files:</p>"},{"location":"getting-started/quick-start/#detection-config-example","title":"Detection Config Example","text":"<p>Edit <code>config/detection.yaml</code>:</p> <pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"raster\"\n\noutput:\n  directory: \"results\"\n  dataset_name: \"my_detections\"\n</code></pre>"},{"location":"getting-started/quick-start/#dataset-import-config-example","title":"Dataset Import Config Example","text":"<p>Edit <code>wildata/configs/import-config-example.yaml</code>:</p> <pre><code>source_path: \"annotations.json\"\nsource_format: \"coco\"\ndataset_name: \"my_dataset\"\nroot: \"data\"\nsplit_name: \"train\"\n\ntransformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n    min_visibility: 0.7\n</code></pre>"},{"location":"getting-started/quick-start/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/quick-start/#workflow-1-detection-on-new-images","title":"Workflow 1: Detection on New Images","text":"<pre><code># 1. Run detection\nwildetect detect images/ --model model.pt --output results/\n\n# 2. View results\nwildetect fiftyone --action launch\n\n# 3. Export results\nwildetect analyze results/detections.json --output analysis/\n</code></pre>"},{"location":"getting-started/quick-start/#workflow-2-prepare-training-data","title":"Workflow 2: Prepare Training Data","text":"<pre><code># 1. Import annotations\nwildata import-dataset annotations.json --format coco --name train_data\n\n# 2. Apply transformations\nwildata import-dataset annotations.json \\\n    --format coco \\\n    --name augmented_data \\\n    --enable-tiling \\\n    --enable-augmentation\n\n# 3. Export for training\nwildata export-dataset augmented_data --format yolo\n</code></pre>"},{"location":"getting-started/quick-start/#workflow-3-train-and-deploy-model","title":"Workflow 3: Train and Deploy Model","text":"<pre><code># 1. Train model\ncd wildtrain\nwildtrain train detector -c configs/detection/yolo_configs/yolo.yaml\n\n# 2. Evaluate model\nscripts\\eval_detector.bat\n\n# 3. Register to MLflow\nscripts\\register_model.bat\n\n# 4. Use for detection\ncd ..\nwildetect detect images/ --model-name my_detector --output results/\n</code></pre>"},{"location":"getting-started/quick-start/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># MLflow Configuration\nMLFLOW_TRACKING_URI=http://localhost:5000\n\n# Label Studio (optional)\nLABEL_STUDIO_URL=http://localhost:8080\nLABEL_STUDIO_API_KEY=your_api_key\n\n# Model Storage\nMODEL_REGISTRY_PATH=models/\n\n# Data Storage\nDATA_ROOT=D:/data/\n\n# GPU Settings\nCUDA_VISIBLE_DEVICES=0\n</code></pre>"},{"location":"getting-started/quick-start/#launching-services","title":"Launching Services","text":""},{"location":"getting-started/quick-start/#mlflow-ui","title":"MLflow UI","text":"<p>Track experiments and manage models:</p> <pre><code>scripts\\launch_mlflow.bat\n# Access at http://localhost:5000\n</code></pre>"},{"location":"getting-started/quick-start/#label-studio","title":"Label Studio","text":"<p>Annotate images:</p> <pre><code>scripts\\launch_labelstudio.bat\n# Access at http://localhost:8080\n</code></pre>"},{"location":"getting-started/quick-start/#wildata-api","title":"WilData API","text":"<p>REST API for data operations:</p> <pre><code>cd wildata\nscripts\\launch_api.bat\n# Access at http://localhost:8441\n# Docs at http://localhost:8441/docs\n</code></pre>"},{"location":"getting-started/quick-start/#inference-server","title":"Inference Server","text":"<p>Deploy model as API:</p> <pre><code>scripts\\launch_inference_server.bat\n# Access at http://localhost:4141\n</code></pre>"},{"location":"getting-started/quick-start/#quick-reference","title":"Quick Reference","text":""},{"location":"getting-started/quick-start/#detection-commands","title":"Detection Commands","text":"<pre><code># Basic detection\nwildetect detect images/ --model model.pt\n\n# With tiling for large images\nwildetect detect large_image.tif --model model.pt --tile-size 800\n\n# Census with statistics\nwildetect census campaign images/ --model model.pt\n\n# Analyze results\nwildetect analyze results.json\n</code></pre>"},{"location":"getting-started/quick-start/#data-commands","title":"Data Commands","text":"<pre><code># Import\nwildata import-dataset source --format coco --name dataset\n\n# List datasets\nwildata dataset list\n\n# Export\nwildata dataset export dataset --format yolo\n\n# Create ROI dataset\nwildata create-roi annotations.json --format coco\n</code></pre>"},{"location":"getting-started/quick-start/#training-commands","title":"Training Commands","text":"<pre><code># Train classifier\nwildtrain train classifier -c config.yaml\n\n# Train detector\nwildtrain train detector -c config.yaml\n\n# Evaluate\nwildtrain eval classifier -c config.yaml\n\n# Register model\nwildtrain register model_path --name my_model\n</code></pre>"},{"location":"getting-started/quick-start/#getting-help","title":"Getting Help","text":""},{"location":"getting-started/quick-start/#command-help","title":"Command Help","text":"<p>Every command has a <code>--help</code> flag:</p> <pre><code>wildetect --help\nwildetect detect --help\nwildata import-dataset --help\nwildtrain train --help\n</code></pre>"},{"location":"getting-started/quick-start/#package-information","title":"Package Information","text":"<pre><code># System info\nwildetect info\n\n# Check installation\npython -c \"import wildetect; print(wildetect.__version__)\"\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you've run your first commands:</p> <ol> <li>\ud83d\udcd6 Deep Dive: Follow the End-to-End Detection Tutorial</li> <li>\ud83c\udfd7\ufe0f Understand Architecture: Read the Architecture Overview</li> <li>\ud83d\udd27 Configure: Explore Configuration Files</li> <li>\ud83d\udcda Learn More: Check out all Tutorials</li> </ol> <p>Questions? Check the Troubleshooting Guide or reach out via GitHub Issues.</p>"},{"location":"scripts/wildata/","title":"WilData Scripts Reference","text":"<p>This page documents all batch scripts available in the WilData package for data management operations.</p>"},{"location":"scripts/wildata/#overview","title":"Overview","text":"<p>All scripts are located in <code>wildata/scripts/</code> directory.</p>"},{"location":"scripts/wildata/#quick-reference","title":"Quick Reference","text":"Script Purpose Config File import-dataset-example.bat Import single dataset <code>configs/import-config-example.yaml</code> bulk-import-dataset.bat Bulk import datasets <code>configs/bulk-import-*.yaml</code> create-roi-dataset.bat Create ROI dataset <code>configs/roi-create-config.yaml</code> bulk-roi-create-config.bat Bulk create ROI datasets <code>configs/bulk-roi-create-config.yaml</code> update-gps-example.bat Update GPS from CSV <code>configs/gps-update-config-example.yaml</code> visualize_data.bat Visualize dataset None dvc-setup.bat Setup DVC None launch_api.bat Launch REST API <code>.env</code> running_tests.bat Run tests None"},{"location":"scripts/wildata/#data-import-scripts","title":"Data Import Scripts","text":""},{"location":"scripts/wildata/#import-dataset-examplebat","title":"import-dataset-example.bat","text":"<p>Purpose: Import a single dataset from COCO, YOLO, or Label Studio format.</p> <p>Location: <code>wildata/scripts/import-dataset-example.bat</code></p> <p>Command: <pre><code>uv run wildata import-dataset --config configs\\import-config-example.yaml\n</code></pre></p> <p>Configuration: <code>wildata/configs/import-config-example.yaml</code></p> <p>Key Parameters: <pre><code>source_path: \"path/to/annotations.json\"\nsource_format: \"coco\"  # coco, yolo, ls\ndataset_name: \"my_dataset\"\n\nroot: \"data\"\nsplit_name: \"train\"  # train, val, test\n\ntransformations:\n  enable_bbox_clipping: true\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n\nroi_config:\n  roi_box_size: 384\n  random_roi_count: 2\n</code></pre></p> <p>Example Usage: <pre><code>cd wildata\n\n# Edit config file first\nnotepad configs\\import-config-example.yaml\n\n# Run import\nscripts\\import-dataset-example.bat\n</code></pre></p> <p>Output: - Master format dataset in <code>data/datasets/</code> - Processed images (tiled if enabled) - ROI dataset (if configured)</p>"},{"location":"scripts/wildata/#bulk-import-datasetbat","title":"bulk-import-dataset.bat","text":"<p>Purpose: Import multiple datasets in batch mode.</p> <p>Location: <code>wildata/scripts/bulk-import-dataset.bat</code></p> <p>Command: <pre><code>uv run wildata bulk-import-datasets --config configs\\bulk-import-config-example.yaml -n 2\n</code></pre></p> <p>Configuration: <code>wildata/configs/bulk-import-train.yaml</code> or <code>bulk-import-val.yaml</code></p> <p>Parameters: - <code>-n 2</code>: Number of parallel workers (uses threading on Windows) - <code>--config</code>: Path to bulk import config</p> <p>Example Config: <pre><code># configs/bulk-import-train.yaml\nsource_paths:\n  - \"D:/annotations/dataset1.json\"\n  - \"D:/annotations/dataset2.json\"\n  - \"D:/annotations/dataset3.json\"\n\nsource_format: \"coco\"\nroot: \"D:/data\"\nsplit_name: \"train\"\n\n# Shared transformation settings\ntransformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n</code></pre></p> <p>Example Usage: <pre><code>cd wildata\nscripts\\bulk-import-dataset.bat\n</code></pre></p> <p>Features: - Parallel processing (thread-based) - Progress tracking - Error handling per dataset - Summary report</p>"},{"location":"scripts/wildata/#roi-dataset-scripts","title":"ROI Dataset Scripts","text":""},{"location":"scripts/wildata/#create-roi-datasetbat","title":"create-roi-dataset.bat","text":"<p>Purpose: Create Region of Interest (ROI) classification dataset from detection annotations.</p> <p>Location: <code>wildata/scripts/create-roi-dataset.bat</code></p> <p>Command: <pre><code>uv run wildata create-roi-dataset --config configs\\roi-create-config.yaml\n</code></pre></p> <p>Configuration: <code>wildata/configs/roi-create-config.yaml</code></p> <p>Key Parameters: <pre><code>source_path: \"annotations.json\"\nsource_format: \"coco\"\ndataset_name: \"roi_dataset\"\n\nroi_config:\n  roi_box_size: 128        # Size of extracted ROI\n  min_roi_size: 32         # Min object size to extract\n  random_roi_count: 10     # Background samples per image\n  background_class: \"background\"\n  save_format: \"jpg\"\n  quality: 95\n</code></pre></p> <p>Use Cases: - Hard sample mining - Error analysis - Training classification models - Creating balanced datasets</p> <p>Example Usage: <pre><code>cd wildata\nscripts\\create-roi-dataset.bat\n</code></pre></p> <p>Output: - ROI image crops - Classification labels - Class mapping JSON - Statistics file</p>"},{"location":"scripts/wildata/#bulk-roi-createbat","title":"bulk-roi-create.bat","text":"<p>Purpose: Create multiple ROI datasets in batch.</p> <p>Location: Script not shown, but referenced in configs</p> <p>Configuration: <code>wildata/configs/bulk-roi-create-config.yaml</code></p> <p>Example Config: <pre><code>source_paths:\n  - \"dataset1.json\"\n  - \"dataset2.json\"\n\nsource_format: \"coco\"\nsplit_name: \"val\"\n\nroi_config:\n  roi_box_size: 128\n  random_roi_count: 5\n</code></pre></p>"},{"location":"scripts/wildata/#gps-management-scripts","title":"GPS Management Scripts","text":""},{"location":"scripts/wildata/#update-gps-examplebat","title":"update-gps-example.bat","text":"<p>Purpose: Update image EXIF GPS data from CSV file.</p> <p>Location: <code>wildata/scripts/update-gps-example.bat</code></p> <p>Command: <pre><code>uv run wildata update-gps-from-csv --config configs\\gps-update-config-example.yaml\n</code></pre></p> <p>Configuration: <code>wildata/configs/gps-update-config-example.yaml</code></p> <p>Key Parameters: <pre><code>image_folder: \"path/to/images\"\ncsv_path: \"gps_coordinates.csv\"\noutput_dir: \"output/images\"\n\nskip_rows: 0\nfilename_col: \"filename\"\nlat_col: \"latitude\"\nlon_col: \"longitude\"\nalt_col: \"altitude\"\n</code></pre></p> <p>CSV Format: <pre><code>filename,latitude,longitude,altitude\nimage1.jpg,40.7128,-74.0060,10.5\nimage2.jpg,40.7589,-73.9851,15.2\n</code></pre></p> <p>Example Usage: <pre><code>cd wildata\n\n# Prepare CSV with GPS data\n# Edit config\nnotepad configs\\gps-update-config-example.yaml\n\n# Run update\nscripts\\update-gps-example.bat\n</code></pre></p> <p>Output: - Images with updated EXIF GPS - Summary report - Error log (if any)</p>"},{"location":"scripts/wildata/#visualization-scripts","title":"Visualization Scripts","text":""},{"location":"scripts/wildata/#visualize_databat","title":"visualize_data.bat","text":"<p>Purpose: Launch FiftyOne visualization for datasets.</p> <p>Location: <code>wildata/scripts/visualize_data.bat</code></p> <p>Command: <pre><code>uv run wildata visualize-dataset --dataset my_dataset --split train\n</code></pre></p> <p>Example Usage: <pre><code>cd wildata\n\n# Visualize training set\nuv run wildata visualize-dataset --dataset my_dataset --split train\n\n# Or use script\nscripts\\visualize_data.bat\n</code></pre></p> <p>Features: - Interactive dataset viewer - Annotation visualization - Filtering and search - Statistics display</p>"},{"location":"scripts/wildata/#dvc-scripts","title":"DVC Scripts","text":""},{"location":"scripts/wildata/#dvc-setupbat","title":"dvc-setup.bat","text":"<p>Purpose: Initialize and configure DVC for data versioning.</p> <p>Location: <code>wildata/scripts/dvc-setup.bat</code></p> <p>Command: <pre><code># Initialize DVC\ndvc init\n\n# Add remote storage\ndvc remote add -d myremote &lt;storage_path&gt;\n</code></pre></p> <p>Storage Options:</p> Local StorageAWS S3Google Cloud <pre><code>dvc remote add -d local D:\\dvc-storage\n</code></pre> <pre><code>dvc remote add -d s3remote s3://bucket/path\ndvc remote modify s3remote access_key_id YOUR_KEY\ndvc remote modify s3remote secret_access_key YOUR_SECRET\n</code></pre> <pre><code>dvc remote add -d gcs gs://bucket/path\nset GOOGLE_APPLICATION_CREDENTIALS=path\\to\\credentials.json\n</code></pre> <p>Example Usage: <pre><code>cd wildata\nscripts\\dvc-setup.bat\n\n# Track data\ndvc add data\\datasets\\my_dataset\n\n# Commit DVC file\ngit add data\\datasets\\my_dataset.dvc\ngit commit -m \"Add dataset\"\n\n# Push to remote\ndvc push\n</code></pre></p> <p>DVC Workflow: <pre><code># On another machine\ngit pull\ndvc pull  # Downloads data\n</code></pre></p>"},{"location":"scripts/wildata/#api-scripts","title":"API Scripts","text":""},{"location":"scripts/wildata/#launch_apibat","title":"launch_api.bat","text":"<p>Purpose: Launch WilData REST API server.</p> <p>Location: <code>wildata/scripts/launch_api.bat</code></p> <p>Command: <pre><code>uv run python -m wildata.api.main\n</code></pre></p> <p>Default Port: 8441</p> <p>Example Usage: <pre><code>cd wildata\nscripts\\launch_api.bat\n</code></pre></p> <p>Access: - API: <code>http://localhost:8441</code> - Docs: <code>http://localhost:8441/docs</code> - Redoc: <code>http://localhost:8441/redoc</code></p> <p>API Endpoints:</p>"},{"location":"scripts/wildata/#import-dataset","title":"Import Dataset","text":"<pre><code>POST /api/v1/datasets/import\nContent-Type: application/json\n\n{\n  \"source_path\": \"/path/to/data.json\",\n  \"source_format\": \"coco\",\n  \"dataset_name\": \"my_dataset\",\n  \"root\": \"data\"\n}\n</code></pre>"},{"location":"scripts/wildata/#list-datasets","title":"List Datasets","text":"<pre><code>GET /api/v1/datasets?root=data\n</code></pre>"},{"location":"scripts/wildata/#create-roi-dataset","title":"Create ROI Dataset","text":"<pre><code>POST /api/v1/roi/create\nContent-Type: application/json\n\n{\n  \"source_path\": \"/path/to/data.json\",\n  \"source_format\": \"coco\",\n  \"dataset_name\": \"roi_dataset\",\n  \"roi_config\": {\n    \"roi_box_size\": 128,\n    \"random_roi_count\": 10\n  }\n}\n</code></pre>"},{"location":"scripts/wildata/#job-status","title":"Job Status","text":"<pre><code>GET /api/v1/jobs/{job_id}\n</code></pre> <p>Environment Variables: <pre><code># In .env\nWILDATA_API_HOST=0.0.0.0\nWILDATA_API_PORT=8441\nWILDATA_API_DEBUG=false\n</code></pre></p> <p>Full API Documentation: See WilData API Reference</p>"},{"location":"scripts/wildata/#testing-scripts","title":"Testing Scripts","text":""},{"location":"scripts/wildata/#running_testsbat","title":"running_tests.bat","text":"<p>Purpose: Run WilData test suite.</p> <p>Location: <code>wildata/scripts/running_tests.bat</code></p> <p>Command: <pre><code>uv run pytest tests/ -v\n</code></pre></p> <p>Example Usage: <pre><code>cd wildata\nscripts\\running_tests.bat\n</code></pre></p> <p>Test Categories: - Format adapter tests - Transformation tests - Validation tests - API tests - Integration tests</p> <p>Run Specific Tests: <pre><code># Test imports\nuv run pytest tests/test_coco_import.py -v\n\n# Test transformations\nuv run pytest tests/test_transformations.py -v\n\n# Test API\nuv run pytest tests/api/ -v\n\n# With coverage\nuv run pytest --cov=wildata tests/\n</code></pre></p>"},{"location":"scripts/wildata/#common-workflows","title":"Common Workflows","text":""},{"location":"scripts/wildata/#dataset-preparation-workflow","title":"Dataset Preparation Workflow","text":"<pre><code># 1. Import dataset\ncd wildata\nscripts\\import-dataset-example.bat\n\n# 2. Visualize\nscripts\\visualize_data.bat\n\n# 3. Export for training\nuv run wildata dataset export my_dataset --format yolo\n</code></pre>"},{"location":"scripts/wildata/#roi-extraction-workflow","title":"ROI Extraction Workflow","text":"<pre><code># 1. Import detection dataset\nscripts\\import-dataset-example.bat\n\n# 2. Create ROI dataset\nscripts\\create-roi-dataset.bat\n\n# 3. Visualize ROI dataset\nuv run wildata visualize-dataset --dataset roi_dataset\n</code></pre>"},{"location":"scripts/wildata/#gps-management-workflow","title":"GPS Management Workflow","text":"<pre><code># 1. Extract GPS from images\n# (using WildDetect extract_gps.bat)\n\n# 2. Update GPS if needed\ncd wildata\nscripts\\update-gps-example.bat\n\n# 3. Verify GPS data\n# Check EXIF data in images\n</code></pre>"},{"location":"scripts/wildata/#dvc-workflow","title":"DVC Workflow","text":"<pre><code># Setup (once)\ncd wildata\nscripts\\dvc-setup.bat\n\n# After each dataset import\ndvc add data\\datasets\\new_dataset\ngit add data\\datasets\\new_dataset.dvc\ngit commit -m \"Add new dataset\"\ndvc push\n\n# On other machines\ngit pull\ndvc pull\n</code></pre>"},{"location":"scripts/wildata/#configuration-examples","title":"Configuration Examples","text":""},{"location":"scripts/wildata/#complete-import-config","title":"Complete Import Config","text":"<pre><code># configs/import-config-example.yaml\nsource_path: \"D:/annotations/dataset.json\"\nsource_format: \"coco\"\ndataset_name: \"wildlife_train\"\n\nroot: \"D:/data\"\nsplit_name: \"train\"\nprocessing_mode: \"batch\"\n\n# Label Studio integration\nls_xml_config: \"configs/label_studio_config.xml\"\nls_parse_config: false\n\n# ROI extraction\ndisable_roi: false\nroi_config:\n  random_roi_count: 2\n  roi_box_size: 384\n  min_roi_size: 32\n  background_class: \"background\"\n  sample_background: true\n\n# Transformations\ntransformations:\n  enable_bbox_clipping: true\n  bbox_clipping:\n    tolerance: 5\n    skip_invalid: false\n\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n    min_visibility: 0.7\n    max_negative_tiles_in_negative_image: 2\n    dark_threshold: 0.7\n\n  enable_augmentation: false\n  augmentation:\n    rotation_range: [-45, 45]\n    probability: 1.0\n    num_transforms: 2\n</code></pre>"},{"location":"scripts/wildata/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildata/#import-fails","title":"Import Fails","text":"<p>Issue: Dataset import fails with validation errors</p> <p>Solutions: 1. Check source file format is correct 2. Verify all image paths are valid 3. Check bbox coordinates are within image bounds 4. Use <code>--verbose</code> flag for detailed errors</p>"},{"location":"scripts/wildata/#dvc-push-fails","title":"DVC Push Fails","text":"<p>Issue: Can't push data to remote</p> <p>Solutions: 1. Verify remote credentials 2. Check network connection 3. Verify remote storage path exists 4. Use <code>dvc remote list</code> to check configuration</p>"},{"location":"scripts/wildata/#api-wont-start","title":"API Won't Start","text":"<p>Issue: API server fails to start</p> <p>Solutions: 1. Check port 8441 is not in use 2. Verify <code>.env</code> file configuration 3. Check all dependencies installed 4. Look at error logs</p>"},{"location":"scripts/wildata/#out-of-memory","title":"Out of Memory","text":"<p>Issue: Import fails with memory error</p> <p>Solutions: 1. Use <code>processing_mode: \"streaming\"</code> 2. Reduce number of parallel workers 3. Process datasets one at a time 4. Disable transformations temporarily</p>"},{"location":"scripts/wildata/#next-steps","title":"Next Steps","text":"<ul> <li>WilData Configuration Reference</li> <li>WilData CLI Reference</li> <li>Dataset Preparation Tutorial</li> <li>WilData API Documentation</li> </ul>"},{"location":"scripts/wildetect/","title":"WildDetect Scripts Reference","text":"<p>This page documents all batch scripts available in the WildDetect package. These scripts provide convenient ways to run common operations on Windows.</p>"},{"location":"scripts/wildetect/#overview","title":"Overview","text":"<p>All scripts are located in the <code>scripts/</code> directory and should be run from the project root.</p>"},{"location":"scripts/wildetect/#quick-reference","title":"Quick Reference","text":"Script Purpose Config File run_detection.bat Run wildlife detection <code>config/detection.yaml</code> run_census.bat Run census campaign <code>config/census.yaml</code> launch_ui.bat Launch Streamlit UI None launch_fiftyone.bat Launch FiftyOne viewer None launch_labelstudio.bat Launch Label Studio <code>.env</code> launch_mlflow.bat Launch MLflow UI None launch_inference_server.bat Launch inference API None register_model.bat Register model to MLflow <code>config/detector_registration.yaml</code> extract_gps.bat Extract GPS from images <code>config/extract-gps.yaml</code> profile_census.bat Profile census performance <code>config/census.yaml</code> run_integration_tests.bat Run integration tests None load_env.bat Load environment variables <code>.env</code>"},{"location":"scripts/wildetect/#detection-scripts","title":"Detection Scripts","text":""},{"location":"scripts/wildetect/#run_detectionbat","title":"run_detection.bat","text":"<p>Purpose: Run wildlife detection on images using a trained model.</p> <p>Location: <code>scripts/run_detection.bat</code></p> <p>Command: <pre><code>uv run --env-file .env wildetect detection detect -c config/detection.yaml\n</code></pre></p> <p>Configuration: <code>config/detection.yaml</code></p> <p>Key Parameters: - <code>model.mlflow_model_name</code>: Model name in MLflow registry - <code>model.mlflow_model_alias</code>: Model version/alias (e.g., \"production\") - <code>model.device</code>: Device to use (\"cuda\", \"cpu\", \"auto\") - <code>image_paths</code>: List of image paths to process - <code>image_dir</code>: Directory containing images - <code>processing.batch_size</code>: Batch size for inference - <code>processing.tile_size</code>: Tile size for large images - <code>processing.pipeline_type</code>: Pipeline strategy (\"raster\", \"multithreaded\", \"simple\") - <code>output.directory</code>: Output directory for results</p> <p>Example Usage: <pre><code># Edit config/detection.yaml first\ncd wildetect\nscripts\\run_detection.bat\n</code></pre></p> <p>Example Config: <pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nimage_dir: \"D:/images/survey_2024/\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  pipeline_type: \"raster\"\n\noutput:\n  directory: \"results/detections\"\n</code></pre></p> <p>Output: - Detection results: JSON and CSV files - Visualizations (if enabled) - FiftyOne dataset (if configured)</p>"},{"location":"scripts/wildetect/#run_censusbat","title":"run_census.bat","text":"<p>Purpose: Run a complete wildlife census campaign with statistics and reports.</p> <p>Location: <code>scripts/run_census.bat</code></p> <p>Command: <pre><code>uv run --env-file .env --no-sync wildetect detection census -c config/census.yaml\n</code></pre></p> <p>Configuration: <code>config/census.yaml</code></p> <p>Key Parameters: - <code>campaign.name</code>: Census campaign name - <code>campaign.target_species</code>: List of target species - <code>model</code>: Model configuration (same as detection) - <code>flight_specs.flight_height</code>: Flight altitude in meters - <code>flight_specs.gsd</code>: Ground Sample Distance (cm/px) - <code>analysis</code>: Analysis options (density, hotspots, maps) - <code>output.generate_pdf_report</code>: Generate PDF report</p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\run_census.bat\n</code></pre></p> <p>Example Config: <pre><code>campaign:\n  name: \"Summer_2024_Survey\"\n  target_species: [\"elephant\", \"giraffe\", \"zebra\"]\n\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n\nflight_specs:\n  flight_height: 120.0\n  gsd: 2.38\n\noutput:\n  directory: \"census_results\"\n  generate_pdf_report: true\n</code></pre></p> <p>Output: - Census statistics (counts, density) - Geographic analysis - Visualizations and maps - PDF report (if enabled)</p>"},{"location":"scripts/wildetect/#profile_censusbat","title":"profile_census.bat","text":"<p>Purpose: Profile census performance with detailed timing and memory analysis.</p> <p>Location: <code>scripts/profile_census.bat</code></p> <p>Command: <pre><code>uv run wildetect detection census -c config/census.yaml --profile --gpu-profile --line-profile\n</code></pre></p> <p>Flags: - <code>--profile</code>: Enable general profiling - <code>--gpu-profile</code>: Profile GPU memory usage - <code>--line-profile</code>: Line-by-line profiling</p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\profile_census.bat\n</code></pre></p> <p>Output: - Profiling reports - Performance metrics - Memory usage statistics - Bottleneck identification</p>"},{"location":"scripts/wildetect/#visualization-scripts","title":"Visualization Scripts","text":""},{"location":"scripts/wildetect/#extract_gpsbat","title":"extract_gps.bat","text":"<p>Purpose: Extract GPS coordinates from image EXIF data and create a summary.</p> <p>Location: <code>scripts/extract_gps.bat</code></p> <p>Command: <pre><code>uv run wildetect visualization extract-gps-coordinates -c config/extract-gps.yaml\n</code></pre></p> <p>Configuration: <code>config/extract-gps.yaml</code></p> <p>Key Parameters: - <code>image_directory</code>: Directory containing images with EXIF data - <code>output_file</code>: Output CSV file for GPS coordinates - <code>recursive</code>: Search subdirectories</p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\extract_gps.bat\n</code></pre></p> <p>Example Config: <pre><code>image_directory: \"D:/images/survey/\"\noutput_file: \"gps_coordinates.csv\"\nrecursive: true\n</code></pre></p> <p>Output: - CSV file with GPS coordinates - Summary statistics - Coverage map data</p>"},{"location":"scripts/wildetect/#ui-and-service-scripts","title":"UI and Service Scripts","text":""},{"location":"scripts/wildetect/#launch_uibat","title":"launch_ui.bat","text":"<p>Purpose: Launch the WildDetect Streamlit web interface.</p> <p>Location: <code>scripts/launch_ui.bat</code></p> <p>Command: <pre><code>uv run wildetect services ui\n</code></pre></p> <p>Features: - Interactive detection interface - Configuration editor - Results visualization - Real-time processing</p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\launch_ui.bat\n</code></pre></p> <p>Access: Opens browser at <code>http://localhost:8501</code></p>"},{"location":"scripts/wildetect/#launch_fiftyonebat","title":"launch_fiftyone.bat","text":"<p>Purpose: Launch FiftyOne app for interactive dataset visualization.</p> <p>Location: <code>scripts/launch_fiftyone.bat</code></p> <p>Command: <pre><code>uv run --no-sync --env-file .env fiftyone app launch\n</code></pre></p> <p>Prerequisites: - FiftyOne installed (<code>pip install fiftyone</code>) - Dataset loaded in FiftyOne</p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\launch_fiftyone.bat\n</code></pre></p> <p>Access: Opens browser at <code>http://localhost:5151</code></p> <p>Features: - Interactive dataset viewer - Detection visualization - Filtering and querying - Export capabilities</p>"},{"location":"scripts/wildetect/#launch_labelstudiobat","title":"launch_labelstudio.bat","text":"<p>Purpose: Launch Label Studio for data annotation.</p> <p>Location: <code>scripts/launch_labelstudio.bat</code></p> <p>Command: <pre><code># Activates separate venv and starts Label Studio\nlabel-studio start -p 8080\n</code></pre></p> <p>Prerequisites: - Label Studio venv configured - Label Studio installed in venv</p> <p>Example Usage: <pre><code>cd wildetect/scripts\nlaunch_labelstudio.bat\n</code></pre></p> <p>Access: Opens browser at <code>http://localhost:8080</code></p> <p>Configuration: - Set <code>LABEL_STUDIO_API_KEY</code> in <code>.env</code> - Configure project settings in Label Studio UI</p>"},{"location":"scripts/wildetect/#launch_mlflowbat","title":"launch_mlflow.bat","text":"<p>Purpose: Launch MLflow tracking server UI.</p> <p>Location: <code>scripts/launch_mlflow.bat</code></p> <p>Command: <pre><code>uv run mlflow server --backend-store-uri runs/mlflow --host 0.0.0.0 --port 5000\n</code></pre></p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\launch_mlflow.bat\n</code></pre></p> <p>Access: Opens browser at <code>http://localhost:5000</code></p> <p>Features: - View experiments and runs - Compare models - Model registry management - Metrics and artifacts</p> <p>Environment Variables: <pre><code># In .env\nMLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre></p>"},{"location":"scripts/wildetect/#launch_inference_serverbat","title":"launch_inference_server.bat","text":"<p>Purpose: Launch FastAPI inference server for remote detection.</p> <p>Location: <code>scripts/launch_inference_server.bat</code></p> <p>Command: <pre><code>uv run wildetect services inference-server --port 4141 --workers 2\n</code></pre></p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\launch_inference_server.bat\n</code></pre></p> <p>Access: - API: <code>http://localhost:4141</code> - Docs: <code>http://localhost:4141/docs</code></p> <p>API Endpoints: <pre><code># Health check\nGET /health\n\n# Run detection\nPOST /predict\nContent-Type: multipart/form-data\n{\n  \"file\": &lt;image_file&gt;,\n  \"confidence\": 0.5\n}\n\n# Batch detection\nPOST /predict/batch\n</code></pre></p> <p>Example Request: <pre><code>import requests\n\n# Single image\nwith open(\"image.jpg\", \"rb\") as f:\n    response = requests.post(\n        \"http://localhost:4141/predict\",\n        files={\"file\": f},\n        data={\"confidence\": 0.5}\n    )\n\ndetections = response.json()\n</code></pre></p>"},{"location":"scripts/wildetect/#model-management-scripts","title":"Model Management Scripts","text":""},{"location":"scripts/wildetect/#register_modelbat","title":"register_model.bat","text":"<p>Purpose: Register a trained model to MLflow model registry.</p> <p>Location: <code>scripts/register_model.bat</code></p> <p>Command: <pre><code>uv run wildtrain register detector config/detector_registration.yaml\n</code></pre></p> <p>Configuration: <code>config/detector_registration.yaml</code></p> <p>Key Parameters: - <code>model_path</code>: Path to model weights - <code>model_name</code>: Name for model registry - <code>model_type</code>: Model type (\"detector\" or \"classifier\") - <code>description</code>: Model description - <code>tags</code>: Metadata tags</p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\register_model.bat\n</code></pre></p> <p>Example Config: <pre><code>model_path: \"models/best.pt\"\nmodel_name: \"wildlife_detector\"\nmodel_type: \"detector\"\ndescription: \"YOLO model trained on aerial wildlife images\"\ntags:\n  framework: \"yolo\"\n  dataset: \"wildlife_v1\"\n  training_date: \"2024-01-15\"\n</code></pre></p> <p>Output: - Model registered in MLflow - Model version assigned - Artifacts logged</p>"},{"location":"scripts/wildetect/#testing-scripts","title":"Testing Scripts","text":""},{"location":"scripts/wildetect/#run_integration_testsbat","title":"run_integration_tests.bat","text":"<p>Purpose: Run integration tests for detection pipeline.</p> <p>Location: <code>scripts/run_integration_tests.bat</code></p> <p>Command: <pre><code># Detection pipeline tests\nuv run pytest tests/test_detection_pipeline.py::TestDetectionPipeline::test_detection_pipeline_with_real_images -v\n\n# Data loading tests\nuv run pytest tests/test_data_loading.py -v\n</code></pre></p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\run_integration_tests.bat\n</code></pre></p> <p>Tests Covered: - Detection pipeline with real images - Data loading and preprocessing - Model loading and inference - Result formatting and export</p> <p>Requirements: - Test data in <code>tests/data/</code> - Test model available - pytest installed</p>"},{"location":"scripts/wildetect/#utility-scripts","title":"Utility Scripts","text":""},{"location":"scripts/wildetect/#load_envbat","title":"load_env.bat","text":"<p>Purpose: Load environment variables from <code>.env</code> file.</p> <p>Location: <code>scripts/load_env.bat</code></p> <p>Usage: Called automatically by other scripts</p> <p>Environment Variables: <pre><code># Example .env file\nMLFLOW_TRACKING_URI=http://localhost:5000\nLABEL_STUDIO_URL=http://localhost:8080\nLABEL_STUDIO_API_KEY=your_api_key\nDATA_ROOT=D:/data/\nCUDA_VISIBLE_DEVICES=0\n</code></pre></p>"},{"location":"scripts/wildetect/#common-workflows","title":"Common Workflows","text":""},{"location":"scripts/wildetect/#detection-workflow","title":"Detection Workflow","text":"<pre><code># 1. Start MLflow\nscripts\\launch_mlflow.bat\n\n# 2. Run detection\nscripts\\run_detection.bat\n\n# 3. View results\nscripts\\launch_fiftyone.bat\n</code></pre>"},{"location":"scripts/wildetect/#census-workflow","title":"Census Workflow","text":"<pre><code># 1. Configure census\n# Edit config/census.yaml\n\n# 2. Run census\nscripts\\run_census.bat\n\n# 3. View results\n# Open census_results/report.pdf\n</code></pre>"},{"location":"scripts/wildetect/#model-training-and-registration","title":"Model Training and Registration","text":"<pre><code># 1. Train model (in wildtrain)\ncd wildtrain\nscripts\\train_classifier.bat\n\n# 2. Register model\ncd ..\nscripts\\register_model.bat\n\n# 3. View in MLflow\nscripts\\launch_mlflow.bat\n</code></pre>"},{"location":"scripts/wildetect/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/#script-wont-run","title":"Script Won't Run","text":"<p>Issue: Script exits immediately or shows error</p> <p>Solutions: 1. Check Python environment is activated 2. Run <code>uv sync</code> to install dependencies 3. Verify <code>.env</code> file exists and is configured 4. Check paths in configuration files</p>"},{"location":"scripts/wildetect/#model-loading-error","title":"Model Loading Error","text":"<p>Issue: Can't load model from MLflow</p> <p>Solutions: 1. Ensure MLflow server is running (<code>launch_mlflow.bat</code>) 2. Check model name and alias in config 3. Verify <code>MLFLOW_TRACKING_URI</code> in <code>.env</code> 4. Use <code>mlflow models list</code> to see available models</p>"},{"location":"scripts/wildetect/#gpu-not-detected","title":"GPU Not Detected","text":"<p>Issue: Detection runs on CPU despite having GPU</p> <p>Solutions: 1. Check CUDA installation: <code>python -c \"import torch; print(torch.cuda.is_available())\"</code> 2. Set <code>device: \"cuda\"</code> in config file 3. Check <code>CUDA_VISIBLE_DEVICES</code> environment variable 4. Reinstall PyTorch with CUDA support</p>"},{"location":"scripts/wildetect/#out-of-memory","title":"Out of Memory","text":"<p>Issue: CUDA out of memory or system RAM exhausted</p> <p>Solutions: 1. Reduce <code>batch_size</code> in config 2. Reduce <code>tile_size</code> for raster detection 3. Close other applications 4. Use <code>pipeline_type: \"simple\"</code> for lower memory usage</p>"},{"location":"scripts/wildetect/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Reference</li> <li>CLI Reference</li> <li>Tutorials</li> </ul>"},{"location":"scripts/wildtrain/","title":"WildTrain Scripts Reference","text":"<p>This page documents all batch scripts available in the WildTrain package for model training and evaluation.</p>"},{"location":"scripts/wildtrain/#overview","title":"Overview","text":"<p>All scripts are located in <code>wildtrain/scripts/</code> directory.</p>"},{"location":"scripts/wildtrain/#quick-reference","title":"Quick Reference","text":"Script Purpose Config File train_classifier.bat Train classification model <code>configs/classification/classification_train.yaml</code> eval_classifier.bat Evaluate classifier <code>configs/classification/classification_eval.yaml</code> eval_detector.bat Evaluate detector Various detector configs train_yolo.bat Train YOLO detector <code>configs/detection/yolo_configs/yolo.yaml</code> train_mmdet.bat Train MMDetection model <code>configs/detection/mmdet_configs/mmdet.yaml</code> register_model.bat Register model to MLflow <code>configs/registration/*.yaml</code> run_classification_pipeline.bat Full classification pipeline <code>configs/classification/classification_pipeline_config.yaml</code> run_detection_pipeline.bat Full detection pipeline <code>configs/detection/yolo_configs/yolo_pipeline_config.yaml</code> run_server.bat Run inference server <code>configs/inference.yaml</code> visualize_predictions.bat Visualize predictions Various configs create_dataset.bat Create/prepare dataset <code>configs/datapreparation/*.yaml</code>"},{"location":"scripts/wildtrain/#classification-scripts","title":"Classification Scripts","text":""},{"location":"scripts/wildtrain/#train_classifierbat","title":"train_classifier.bat","text":"<p>Purpose: Train an image classification model using PyTorch Lightning.</p> <p>Location: <code>wildtrain/scripts/train_classifier.bat</code></p> <p>Command: <pre><code>uv run wildtrain train classifier -c configs\\classification\\classification_train.yaml\n</code></pre></p> <p>Configuration: <code>wildtrain/configs/classification/classification_train.yaml</code></p> <p>Key Parameters: <pre><code>model:\n  architecture: \"resnet50\"  # resnet18, resnet50, efficientnet_b0, etc.\n  num_classes: 10\n  pretrained: true\n  learning_rate: 0.001\n\ndata:\n  root_data_directory: \"D:/data/roi_dataset\"\n  split: \"train\"\n  batch_size: 32\n  num_workers: 4\n  image_size: 224\n\ntraining:\n  max_epochs: 100\n  accelerator: \"gpu\"  # gpu, cpu\n  devices: 1\n  precision: 16  # 16, 32\n  gradient_clip_val: 1.0\n\ncallbacks:\n  early_stopping:\n    monitor: \"val_loss\"\n    patience: 10\n  model_checkpoint:\n    monitor: \"val_acc\"\n    mode: \"max\"\n    save_top_k: 3\n\nmlflow:\n  experiment_name: \"classification\"\n  tracking_uri: \"http://localhost:5000\"\n</code></pre></p> <p>Example Usage: <pre><code>cd wildtrain\n\n# Edit config\nnotepad configs\\classification\\classification_train.yaml\n\n# Train\nscripts\\train_classifier.bat\n</code></pre></p> <p>Output: - Trained model checkpoints - MLflow run with metrics - Training logs - Best model weights</p>"},{"location":"scripts/wildtrain/#eval_classifierbat","title":"eval_classifier.bat","text":"<p>Purpose: Evaluate a trained classification model.</p> <p>Location: <code>wildtrain/scripts/eval_classifier.bat</code></p> <p>Command: <pre><code>uv run wildtrain eval classifier -c configs\\classification\\classification_eval.yaml\n</code></pre></p> <p>Configuration: <code>wildtrain/configs/classification/classification_eval.yaml</code></p> <p>Key Parameters: <pre><code>model:\n  checkpoint_path: \"checkpoints/best.ckpt\"\n  # or load from MLflow\n  mlflow_model_name: \"wildlife_classifier\"\n  mlflow_model_version: \"latest\"\n\ndata:\n  root_data_directory: \"D:/data/roi_dataset\"\n  split: \"test\"\n  batch_size: 64\n\nevaluation:\n  save_predictions: true\n  generate_confusion_matrix: true\n  class_metrics: true\n</code></pre></p> <p>Example Usage: <pre><code>cd wildtrain\nscripts\\eval_classifier.bat\n</code></pre></p> <p>Output: - Evaluation metrics (accuracy, precision, recall, F1) - Confusion matrix - Per-class metrics - Predictions file (if enabled)</p>"},{"location":"scripts/wildtrain/#detection-scripts","title":"Detection Scripts","text":""},{"location":"scripts/wildtrain/#train_yolobat","title":"train_yolo.bat","text":"<p>Purpose: Train YOLO object detection model.</p> <p>Location: <code>wildtrain/scripts/train_yolo.bat</code> (or similar)</p> <p>Command: <pre><code>uv run wildtrain train detector -c configs\\detection\\yolo_configs\\yolo.yaml\n</code></pre></p> <p>Configuration: <code>wildtrain/configs/detection/yolo_configs/yolo.yaml</code></p> <p>Key Parameters: <pre><code>model:\n  framework: \"yolo\"\n  size: \"n\"  # n, s, m, l, x\n  pretrained: true\n\ndata:\n  data_yaml: \"D:/data/wildlife/data.yaml\"\n  # data.yaml contains:\n  # - train: path to train images\n  # - val: path to val images\n  # - nc: number of classes\n  # - names: class names\n\ntraining:\n  epochs: 100\n  imgsz: 640\n  batch: 16\n  optimizer: \"AdamW\"\n  lr0: 0.001\n  device: 0  # GPU device\n\naugmentation:\n  hsv_h: 0.015\n  hsv_s: 0.7\n  hsv_v: 0.4\n  degrees: 0.0\n  translate: 0.1\n  scale: 0.5\n  shear: 0.0\n  perspective: 0.0\n  flipud: 0.0\n  fliplr: 0.5\n  mosaic: 1.0\n\nmlflow:\n  experiment_name: \"yolo_detection\"\n</code></pre></p> <p>Example Usage: <pre><code>cd wildtrain\nscripts\\train_yolo.bat\n</code></pre></p> <p>YOLO Data Format: <pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 val/\n\u251c\u2500\u2500 labels/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 val/\n\u2514\u2500\u2500 data.yaml\n</code></pre></p> <p>data.yaml: <pre><code>train: ./images/train\nval: ./images/val\nnc: 3\nnames: ['elephant', 'giraffe', 'zebra']\n</code></pre></p>"},{"location":"scripts/wildtrain/#train_mmdetbat","title":"train_mmdet.bat","text":"<p>Purpose: Train model using MMDetection framework.</p> <p>Location: <code>wildtrain/scripts/train_mmdet.bat</code> (or similar)</p> <p>Command: <pre><code>uv run wildtrain train detector -c configs\\detection\\mmdet_configs\\mmdet.yaml\n</code></pre></p> <p>Configuration: <code>wildtrain/configs/detection/mmdet_configs/</code></p> <p>Supported Models: - Faster R-CNN - YOLO variants - ATSS - FCOS - RetinaNet</p> <p>Example Config: <pre><code>model:\n  framework: \"mmdet\"\n  config_file: \"configs/detection/mmdet_configs/faster_rcnn.py\"\n  checkpoint: null  # or pretrained weights\n\ndata:\n  data_root: \"D:/data/coco_format\"\n  ann_file_train: \"train.json\"\n  ann_file_val: \"val.json\"\n\ntraining:\n  work_dir: \"work_dirs/faster_rcnn\"\n  max_epochs: 12\n  batch_size: 2\n  num_workers: 2\n</code></pre></p>"},{"location":"scripts/wildtrain/#eval_detectorbat","title":"eval_detector.bat","text":"<p>Purpose: Evaluate object detection model.</p> <p>Location: <code>wildtrain/scripts/eval_detector.bat</code></p> <p>Command: <pre><code>uv run wildtrain eval detector -c configs\\detection\\yolo_configs\\yolo_eval.yaml\n</code></pre></p> <p>Example Usage: <pre><code>cd wildtrain\nscripts\\eval_detector.bat\n</code></pre></p> <p>Output: - mAP (mean Average Precision) - mAP@50, mAP@75 - Per-class AP - Precision-Recall curves - Detection visualizations</p>"},{"location":"scripts/wildtrain/#pipeline-scripts","title":"Pipeline Scripts","text":""},{"location":"scripts/wildtrain/#run_classification_pipelinebat","title":"run_classification_pipeline.bat","text":"<p>Purpose: Run complete classification training pipeline.</p> <p>Location: <code>wildtrain/scripts/run_classification_pipeline.bat</code></p> <p>Configuration: <code>wildtrain/configs/classification/classification_pipeline_config.yaml</code></p> <p>Pipeline Steps: 1. Data validation 2. Model initialization 3. Training 4. Evaluation 5. Model registration 6. Export for deployment</p> <p>Example Config: <pre><code>pipeline:\n  name: \"wildlife_classification_v1\"\n\ndata_preparation:\n  validate_data: true\n  augmentation: true\n\ntraining:\n  config_file: \"configs/classification/classification_train.yaml\"\n\nevaluation:\n  config_file: \"configs/classification/classification_eval.yaml\"\n\nregistration:\n  register_to_mlflow: true\n  model_name: \"wildlife_classifier\"\n  stage: \"Staging\"\n\nexport:\n  export_onnx: true\n  export_torchscript: true\n</code></pre></p>"},{"location":"scripts/wildtrain/#run_detection_pipelinebat","title":"run_detection_pipeline.bat","text":"<p>Purpose: Run complete detection training pipeline.</p> <p>Location: <code>wildtrain/scripts/run_detection_pipeline.bat</code></p> <p>Configuration: <code>wildtrain/configs/detection/yolo_configs/yolo_pipeline_config.yaml</code></p> <p>Pipeline Includes: - Data preparation - Training - Validation - Hyperparameter tuning (optional) - Model registration - Export</p>"},{"location":"scripts/wildtrain/#model-management-scripts","title":"Model Management Scripts","text":""},{"location":"scripts/wildtrain/#register_modelbat","title":"register_model.bat","text":"<p>Purpose: Register trained model to MLflow model registry.</p> <p>Location: <code>wildtrain/scripts/register_model.bat</code></p> <p>Command: <pre><code>uv run wildtrain register &lt;model_type&gt; &lt;config_file&gt;\n</code></pre></p> <p>Example Configs:</p>"},{"location":"scripts/wildtrain/#classifier-registration","title":"Classifier Registration","text":"<pre><code># configs/registration/classifier_registration_example.yaml\nmodel_path: \"checkpoints/best.ckpt\"\nmodel_name: \"wildlife_classifier\"\nmodel_type: \"classifier\"\n\ndescription: \"ResNet50 classifier for wildlife ROI\"\n\ntags:\n  architecture: \"resnet50\"\n  dataset: \"wildlife_roi_v1\"\n  num_classes: \"10\"\n  training_date: \"2024-01-15\"\n  accuracy: \"0.95\"\n\naliases:\n  - \"production\"\n  - \"latest\"\n</code></pre>"},{"location":"scripts/wildtrain/#detector-registration","title":"Detector Registration","text":"<pre><code># configs/registration/detector_registration_example.yaml\nmodel_path: \"runs/detect/train/weights/best.pt\"\nmodel_name: \"wildlife_detector\"\nmodel_type: \"detector\"\n\ndescription: \"YOLO11n detector for aerial wildlife\"\n\ntags:\n  framework: \"yolo\"\n  version: \"11n\"\n  dataset: \"wildlife_aerial_v2\"\n  map50: \"0.89\"\n  training_date: \"2024-01-15\"\n</code></pre> <p>Example Usage: <pre><code>cd wildtrain\nscripts\\register_model.bat\n</code></pre></p>"},{"location":"scripts/wildtrain/#inference-scripts","title":"Inference Scripts","text":""},{"location":"scripts/wildtrain/#run_serverbat","title":"run_server.bat","text":"<p>Purpose: Run inference server for model deployment.</p> <p>Location: <code>wildtrain/scripts/run_server.bat</code></p> <p>Command: <pre><code>uv run wildtrain serve -c configs\\inference.yaml\n</code></pre></p> <p>Configuration: <code>wildtrain/configs/inference.yaml</code></p> <p>Key Parameters: <pre><code>server:\n  host: \"0.0.0.0\"\n  port: 8000\n  workers: 2\n\nmodel:\n  mlflow_model_name: \"wildlife_detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\ninference:\n  batch_size: 8\n  confidence_threshold: 0.5\n  nms_threshold: 0.45\n</code></pre></p> <p>Example Usage: <pre><code>cd wildtrain\nscripts\\run_server.bat\n</code></pre></p> <p>API Access: - Endpoint: <code>http://localhost:8000</code> - Docs: <code>http://localhost:8000/docs</code></p> <p>Example Request: <pre><code>import requests\n\nwith open(\"image.jpg\", \"rb\") as f:\n    response = requests.post(\n        \"http://localhost:8000/predict\",\n        files={\"file\": f}\n    )\n\npredictions = response.json()\n</code></pre></p>"},{"location":"scripts/wildtrain/#utility-scripts","title":"Utility Scripts","text":""},{"location":"scripts/wildtrain/#visualize_predictionsbat","title":"visualize_predictions.bat","text":"<p>Purpose: Visualize model predictions on images.</p> <p>Location: <code>wildtrain/scripts/visualize_predictions.bat</code></p> <p>Configuration: Various visualization configs</p> <p>Features: - Draw bounding boxes - Show confidence scores - Save annotated images - Generate prediction gallery</p> <p>Example Usage: <pre><code>cd wildtrain\nscripts\\visualize_predictions.bat\n</code></pre></p>"},{"location":"scripts/wildtrain/#create_datasetbat","title":"create_dataset.bat","text":"<p>Purpose: Create or prepare dataset for training.</p> <p>Location: <code>wildtrain/scripts/create_dataset.bat</code></p> <p>Configuration: <code>wildtrain/configs/datapreparation/</code></p> <p>Capabilities: - Convert formats - Split train/val/test - Apply transformations - Validate dataset</p>"},{"location":"scripts/wildtrain/#common-workflows","title":"Common Workflows","text":""},{"location":"scripts/wildtrain/#training-workflow-classification","title":"Training Workflow (Classification)","text":"<pre><code>cd wildtrain\n\n# 1. Prepare data (if needed)\nscripts\\create_dataset.bat\n\n# 2. Train model\nscripts\\train_classifier.bat\n\n# 3. Evaluate\nscripts\\eval_classifier.bat\n\n# 4. Register to MLflow\nscripts\\register_model.bat\n</code></pre>"},{"location":"scripts/wildtrain/#training-workflow-detection","title":"Training Workflow (Detection)","text":"<pre><code>cd wildtrain\n\n# 1. Train YOLO\nscripts\\train_yolo.bat\n\n# 2. Evaluate\nscripts\\eval_detector.bat\n\n# 3. Visualize predictions\nscripts\\visualize_predictions.bat\n\n# 4. Register model\nscripts\\register_model.bat\n</code></pre>"},{"location":"scripts/wildtrain/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code># Use Optuna for HPO\ncd wildtrain\nuv run wildtrain tune classifier -c configs/classification/classification_sweep.yaml\n</code></pre>"},{"location":"scripts/wildtrain/#model-deployment","title":"Model Deployment","text":"<pre><code># 1. Register model\nscripts\\register_model.bat\n\n# 2. Start inference server\nscripts\\run_server.bat\n\n# 3. Test API\ncurl -X POST \"http://localhost:8000/predict\" -F \"file=@test.jpg\"\n</code></pre>"},{"location":"scripts/wildtrain/#configuration-examples","title":"Configuration Examples","text":""},{"location":"scripts/wildtrain/#complete-training-config","title":"Complete Training Config","text":"<pre><code># configs/classification/classification_train.yaml\nmodel:\n  architecture: \"resnet50\"\n  num_classes: 10\n  pretrained: true\n  learning_rate: 0.001\n  weight_decay: 0.0001\n  dropout: 0.5\n\ndata:\n  root_data_directory: \"D:/data/roi_dataset\"\n  split: \"train\"\n  batch_size: 32\n  num_workers: 4\n  image_size: 224\n  normalize: true\n  mean: [0.485, 0.456, 0.406]\n  std: [0.229, 0.224, 0.225]\n\naugmentation:\n  random_flip: 0.5\n  random_rotation: 15\n  color_jitter:\n    brightness: 0.2\n    contrast: 0.2\n    saturation: 0.2\n  random_crop: true\n\ntraining:\n  max_epochs: 100\n  accelerator: \"gpu\"\n  devices: 1\n  precision: 16\n  accumulate_grad_batches: 1\n  gradient_clip_val: 1.0\n  log_every_n_steps: 10\n\noptimizer:\n  type: \"Adam\"\n  lr: 0.001\n  betas: [0.9, 0.999]\n\nscheduler:\n  type: \"CosineAnnealingLR\"\n  T_max: 100\n  eta_min: 0.00001\n\ncallbacks:\n  early_stopping:\n    monitor: \"val_loss\"\n    patience: 10\n    mode: \"min\"\n  model_checkpoint:\n    monitor: \"val_acc\"\n    mode: \"max\"\n    save_top_k: 3\n    filename: \"epoch{epoch:02d}-acc{val_acc:.4f}\"\n\nmlflow:\n  experiment_name: \"wildlife_classification\"\n  tracking_uri: \"http://localhost:5000\"\n  log_models: true\n</code></pre>"},{"location":"scripts/wildtrain/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildtrain/#training-crashes","title":"Training Crashes","text":"<p>Issue: Training crashes with CUDA out of memory</p> <p>Solutions: 1. Reduce batch size 2. Use mixed precision (<code>precision: 16</code>) 3. Reduce image size 4. Enable gradient accumulation</p>"},{"location":"scripts/wildtrain/#mlflow-connection-error","title":"MLflow Connection Error","text":"<p>Issue: Can't connect to MLflow tracking server</p> <p>Solutions: 1. Start MLflow server: <code>mlflow server --port 5000</code> 2. Check <code>MLFLOW_TRACKING_URI</code> environment variable 3. Verify network connection 4. Check firewall settings</p>"},{"location":"scripts/wildtrain/#data-loading-slow","title":"Data Loading Slow","text":"<p>Issue: Data loading is bottleneck</p> <p>Solutions: 1. Increase <code>num_workers</code> (max 4 on Windows) 2. Enable <code>pin_memory: true</code> 3. Use SSD for data storage 4. Reduce data preprocessing complexity</p>"},{"location":"scripts/wildtrain/#model-wont-load","title":"Model Won't Load","text":"<p>Issue: Can't load trained model</p> <p>Solutions: 1. Check checkpoint path is correct 2. Verify model architecture matches 3. Check for version compatibility 4. Try loading with <code>strict=False</code></p>"},{"location":"scripts/wildtrain/#next-steps","title":"Next Steps","text":"<ul> <li>WildTrain Configuration Reference</li> <li>Model Training Tutorial</li> <li>WildTrain CLI Reference</li> <li>MLflow Integration Guide</li> </ul>"},{"location":"tutorials/census-campaign/","title":"Census Campaign Tutorial","text":"<p>Learn how to conduct a complete wildlife census campaign using WildDetect.</p>"},{"location":"tutorials/census-campaign/#overview","title":"Overview","text":"<p>A census campaign includes detection, population statistics, geographic analysis, and comprehensive reporting.</p>"},{"location":"tutorials/census-campaign/#prerequisites","title":"Prerequisites","text":"<ul> <li>WildDetect installed</li> <li>Aerial survey images with GPS data</li> <li>Trained detection model</li> <li>MLflow server (optional)</li> </ul>"},{"location":"tutorials/census-campaign/#step-1-organize-survey-data","title":"Step 1: Organize Survey Data","text":"<pre><code>census_2024/\n\u251c\u2500\u2500 images/              # Survey images with GPS EXIF\n\u2502   \u251c\u2500\u2500 flight1/\n\u2502   \u251c\u2500\u2500 flight2/\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 config/\n    \u2514\u2500\u2500 census.yaml\n</code></pre>"},{"location":"tutorials/census-campaign/#step-2-configure-census","title":"Step 2: Configure Census","text":"<p>Create <code>config/census.yaml</code>:</p> <pre><code>campaign:\n  name: \"Summer_2024_Survey\"\n  target_species: [\"elephant\", \"giraffe\", \"zebra\"]\n  area_name: \"Serengeti_North\"\n  start_date: \"2024-06-01\"\n  end_date: \"2024-06-15\"\n\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nimage_dir: \"D:/census_2024/images/\"\n\nflight_specs:\n  flight_height: 120.0\n  gsd: 2.38\n\nanalysis:\n  calculate_density: true\n  detect_hotspots: true\n  create_maps: true\n\noutput:\n  directory: \"census_results\"\n  generate_pdf_report: true\n</code></pre>"},{"location":"tutorials/census-campaign/#step-3-run-census","title":"Step 3: Run Census","text":"<pre><code>cd wildetect\n\n# Edit config\nnotepad config\\census.yaml\n\n# Run\nscripts\\run_census.bat\n</code></pre>"},{"location":"tutorials/census-campaign/#step-4-review-results","title":"Step 4: Review Results","text":"<pre><code>census_results/\n\u251c\u2500\u2500 detections.json              # All detections\n\u251c\u2500\u2500 statistics.json              # Population stats\n\u251c\u2500\u2500 census_report.pdf            # PDF report\n\u251c\u2500\u2500 maps/                        # Geographic maps\n\u2502   \u251c\u2500\u2500 distribution_map.html\n\u2502   \u251c\u2500\u2500 density_heatmap.html\n\u2502   \u2514\u2500\u2500 flight_path.html\n\u2514\u2500\u2500 visualizations/              # Annotated images\n</code></pre>"},{"location":"tutorials/census-campaign/#step-5-analyze-statistics","title":"Step 5: Analyze Statistics","text":"<p>The census generates:</p> <ul> <li>Total counts per species</li> <li>Population density (animals/km\u00b2)</li> <li>Species distribution analysis</li> <li>Hotspot locations</li> <li>Coverage area statistics</li> </ul>"},{"location":"tutorials/census-campaign/#geographic-analysis","title":"Geographic Analysis","text":"<p>View interactive maps:</p> <pre><code># Open in browser\nexplorer census_results\\maps\\distribution_map.html\n</code></pre> <p>Features: - Animal locations plotted on map - Density heatmaps - Flight path overlay - Filterable by species</p>"},{"location":"tutorials/census-campaign/#generate-custom-reports","title":"Generate Custom Reports","text":"<pre><code>from wildetect.analysis import ReportGenerator\n\ngenerator = ReportGenerator(\"census_results/detections.json\")\n\n# Custom report\nreport = generator.generate_report(\n    output_path=\"custom_report.pdf\",\n    include_maps=True,\n    include_statistics=True,\n    target_species=[\"elephant\"]\n)\n</code></pre>"},{"location":"tutorials/census-campaign/#example-census-output","title":"Example Census Output","text":"<pre><code>{\n  \"campaign\": \"Summer_2024_Survey\",\n  \"survey_area\": 25.5,  # km\u00b2\n  \"total_images\": 450,\n  \"total_detections\": 1234,\n\n  \"species_counts\": {\n    \"elephant\": 423,\n    \"giraffe\": 612,\n    \"zebra\": 199\n  },\n\n  \"density\": {\n    \"elephant\": 16.6,  # per km\u00b2\n    \"giraffe\": 24.0,\n    \"zebra\": 7.8\n  }\n}\n</code></pre> <p>Next Steps: - End-to-End Detection - Census Configuration</p>"},{"location":"tutorials/dataset-preparation/","title":"Dataset Preparation Tutorial","text":"<p>Learn how to prepare datasets for training using WilData.</p>"},{"location":"tutorials/dataset-preparation/#overview","title":"Overview","text":"<p>This tutorial covers importing, transforming, and exporting datasets for wildlife detection training.</p>"},{"location":"tutorials/dataset-preparation/#prerequisites","title":"Prerequisites","text":"<ul> <li>WilData installed</li> <li>Annotated images (COCO, YOLO, or Label Studio format)</li> </ul>"},{"location":"tutorials/dataset-preparation/#step-1-import-dataset","title":"Step 1: Import Dataset","text":""},{"location":"tutorials/dataset-preparation/#option-a-using-config-file","title":"Option A: Using Config File","text":"<p>Create <code>config.yaml</code>:</p> <pre><code>source_path: \"D:/annotations/dataset.json\"\nsource_format: \"coco\"\ndataset_name: \"wildlife_train\"\nroot: \"D:/data\"\nsplit_name: \"train\"\n\ntransformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n    min_visibility: 0.7\n</code></pre> <p>Run import:</p> <pre><code>cd wildata\nwildata import-dataset --config config.yaml\n</code></pre>"},{"location":"tutorials/dataset-preparation/#option-b-direct-cli","title":"Option B: Direct CLI","text":"<pre><code>wildata import-dataset annotations.json \\\n    --format coco \\\n    --name wildlife_train \\\n    --enable-tiling \\\n    --tile-size 800\n</code></pre>"},{"location":"tutorials/dataset-preparation/#step-2-apply-transformations","title":"Step 2: Apply Transformations","text":""},{"location":"tutorials/dataset-preparation/#tiling-for-large-images","title":"Tiling for Large Images","text":"<pre><code>transformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n    min_visibility: 0.7\n</code></pre>"},{"location":"tutorials/dataset-preparation/#bbox-clipping","title":"Bbox Clipping","text":"<pre><code>transformations:\n  enable_bbox_clipping: true\n  bbox_clipping:\n    tolerance: 5\n    skip_invalid: false\n</code></pre>"},{"location":"tutorials/dataset-preparation/#step-3-create-roi-dataset","title":"Step 3: Create ROI Dataset","text":"<p>For classification training:</p> <pre><code>cd wildata\nscripts\\create-roi-dataset.bat\n\n# Or with CLI\nwildata create-roi-dataset --config configs/roi-create-config.yaml\n</code></pre>"},{"location":"tutorials/dataset-preparation/#step-4-visualize","title":"Step 4: Visualize","text":"<pre><code># Launch FiftyOne\nwildata visualize-dataset --dataset wildlife_train --split train\n</code></pre>"},{"location":"tutorials/dataset-preparation/#step-5-export-for-training","title":"Step 5: Export for Training","text":"<pre><code># Export to YOLO format\nwildata dataset export wildlife_train --format yolo --output exports/yolo\n\n# Export to COCO\nwildata dataset export wildlife_train --format coco --output exports/coco\n</code></pre>"},{"location":"tutorials/dataset-preparation/#complete-example","title":"Complete Example","text":"<pre><code>from wildata import DataPipeline\n\n# Initialize\npipeline = DataPipeline(\"data\")\n\n# Import with transformations\nresult = pipeline.import_dataset(\n    source_path=\"annotations.json\",\n    source_format=\"coco\",\n    dataset_name=\"wildlife_train\",\n    transformations={\n        \"enable_tiling\": True,\n        \"tiling\": {\n            \"tile_size\": 800,\n            \"stride\": 640\n        }\n    }\n)\n\n# Export for training\npipeline.export_dataset(\"wildlife_train\", \"yolo\")\n</code></pre> <p>Next Steps: - Model Training Tutorial - WilData Scripts Reference</p>"},{"location":"tutorials/end-to-end-detection/","title":"End-to-End Detection Tutorial","text":"<p>This tutorial walks you through a complete wildlife detection workflow, from images to analysis results.</p>"},{"location":"tutorials/end-to-end-detection/#prerequisites","title":"Prerequisites","text":"<ul> <li>WildDetect installed (Installation Guide)</li> <li>Aerial images with wildlife</li> <li>Trained model or access to MLflow registry</li> <li>MLflow server running (optional but recommended)</li> </ul>"},{"location":"tutorials/end-to-end-detection/#workflow-overview","title":"Workflow Overview","text":"<pre><code>graph LR\n    A[Aerial Images] --&gt; B[Run Detection]\n    B --&gt; C[View Results]\n    C --&gt; D[Analyze]\n    D --&gt; E[Export]\n\n    style A fill:#e3f2fd\n    style C fill:#fff3e0\n    style E fill:#e8f5e9</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-1-prepare-your-environment","title":"Step 1: Prepare Your Environment","text":""},{"location":"tutorials/end-to-end-detection/#11-setup-directory-structure","title":"1.1 Setup Directory Structure","text":"<pre><code>mkdir D:\\wildlife_detection\ncd D:\\wildlife_detection\n\n# Create directories\nmkdir images\nmkdir results\nmkdir models\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#12-organize-your-images","title":"1.2 Organize Your Images","text":"<pre><code>D:\\wildlife_detection\\\n\u251c\u2500\u2500 images\\\n\u2502   \u251c\u2500\u2500 drone_001.jpg\n\u2502   \u251c\u2500\u2500 drone_002.jpg\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 results\\\n\u2514\u2500\u2500 models\\\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#13-start-mlflow-optional","title":"1.3 Start MLflow (Optional)","text":"<pre><code>cd wildetect\nscripts\\launch_mlflow.bat\n</code></pre> <p>Access at: <code>http://localhost:5000</code></p>"},{"location":"tutorials/end-to-end-detection/#step-2-configure-detection","title":"Step 2: Configure Detection","text":""},{"location":"tutorials/end-to-end-detection/#21-create-configuration-file","title":"2.1 Create Configuration File","text":"<p>Create <code>config/my_detection.yaml</code>:</p> <pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nimage_dir: \"D:/wildlife_detection/images/\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"simple\"  # or \"raster\" for large images\n  confidence_threshold: 0.5\n\nflight_specs:\n  flight_height: 120.0\n  gsd: 2.38  # Ground Sample Distance (cm/pixel)\n\noutput:\n  directory: \"D:/wildlife_detection/results\"\n  dataset_name: \"my_detections\"  # FiftyOne dataset name\n  save_visualizations: true\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#22-alternative-use-model-file","title":"2.2 Alternative: Use Model File","text":"<p>If not using MLflow:</p> <pre><code>model:\n  model_path: \"D:/wildlife_detection/models/detector.pt\"\n  device: \"cuda\"\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-3-run-detection","title":"Step 3: Run Detection","text":""},{"location":"tutorials/end-to-end-detection/#option-a-using-script","title":"Option A: Using Script","text":"<pre><code>cd wildetect\n\n# Edit config/detection.yaml\nnotepad config\\detection.yaml\n\n# Run\nscripts\\run_detection.bat\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#option-b-using-cli","title":"Option B: Using CLI","text":"<pre><code>wildetect detect D:/wildlife_detection/images/ \\\n    --model detector.pt \\\n    --output D:/wildlife_detection/results/ \\\n    --device cuda \\\n    --batch-size 32\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#option-c-using-python","title":"Option C: Using Python","text":"<pre><code>from wildetect.core.pipeline import DetectionPipeline\nfrom pathlib import Path\n\n# Initialize pipeline\npipeline = DetectionPipeline(\n    model_path=\"detector.pt\",\n    device=\"cuda\"\n)\n\n# Run detection\nimage_dir = Path(\"D:/wildlife_detection/images\")\nresults = pipeline.detect_batch(image_dir)\n\n# Save results\npipeline.save_results(results, \"D:/wildlife_detection/results/detections.json\")\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#expected-output","title":"Expected Output","text":"<pre><code>Processing images: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:45&lt;00:00,  1.11it/s]\nDetection complete!\nResults saved to: D:/wildlife_detection/results/detections.json\nTotal detections: 1,234\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-4-review-results","title":"Step 4: Review Results","text":""},{"location":"tutorials/end-to-end-detection/#41-results-structure","title":"4.1 Results Structure","text":"<pre><code>results/\n\u251c\u2500\u2500 detections.json          # All detections\n\u251c\u2500\u2500 detections.csv          # CSV format\n\u251c\u2500\u2500 summary.txt             # Summary statistics\n\u251c\u2500\u2500 visualizations/         # Annotated images\n\u2502   \u251c\u2500\u2500 drone_001.jpg\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 fiftyone/              # FiftyOne dataset (if enabled)\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#42-detection-json-format","title":"4.2 Detection JSON Format","text":"<pre><code>{\n  \"image_path\": \"D:/wildlife_detection/images/drone_001.jpg\",\n  \"image_size\": [1920, 1080],\n  \"processing_time\": 0.5,\n  \"detections\": [\n    {\n      \"class_name\": \"elephant\",\n      \"confidence\": 0.95,\n      \"bbox\": [100, 200, 150, 180],\n      \"bbox_normalized\": [0.052, 0.185, 0.078, 0.167]\n    }\n  ]\n}\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-5-visualize-results","title":"Step 5: Visualize Results","text":""},{"location":"tutorials/end-to-end-detection/#option-a-using-fiftyone","title":"Option A: Using FiftyOne","text":"<pre><code># Launch FiftyOne\ncd wildetect\nscripts\\launch_fiftyone.bat\n\n# Or with CLI\nwildetect fiftyone --action launch --dataset my_detections\n</code></pre> <p>Features: - Interactive viewing - Filtering by confidence - Filtering by species - Export capabilities</p>"},{"location":"tutorials/end-to-end-detection/#option-b-view-saved-visualizations","title":"Option B: View Saved Visualizations","text":"<pre><code># Open visualizations folder\nexplorer D:\\wildlife_detection\\results\\visualizations\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#option-c-using-web-ui","title":"Option C: Using Web UI","text":"<pre><code>cd wildetect\nscripts\\launch_ui.bat\n</code></pre> <p>Navigate to results viewer.</p>"},{"location":"tutorials/end-to-end-detection/#step-6-analyze-results","title":"Step 6: Analyze Results","text":""},{"location":"tutorials/end-to-end-detection/#61-basic-statistics","title":"6.1 Basic Statistics","text":"<pre><code>wildetect analyze D:/wildlife_detection/results/detections.json \\\n    --output D:/wildlife_detection/results/analysis/\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#62-python-analysis","title":"6.2 Python Analysis","text":"<pre><code>import json\nimport pandas as pd\nfrom collections import Counter\n\n# Load results\nwith open(\"results/detections.json\") as f:\n    results = json.load(f)\n\n# Count by species\nspecies_counts = Counter()\nfor result in results:\n    for det in result[\"detections\"]:\n        species_counts[det[\"class_name\"]] += 1\n\nprint(\"Species Counts:\")\nfor species, count in species_counts.items():\n    print(f\"  {species}: {count}\")\n\n# Calculate average confidence\nconfidences = []\nfor result in results:\n    for det in result[\"detections\"]:\n        confidences.append(det[\"confidence\"])\n\nprint(f\"\\nAverage Confidence: {sum(confidences)/len(confidences):.2f}\")\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#63-generate-report","title":"6.3 Generate Report","text":"<pre><code>from wildetect.analysis import ReportGenerator\n\ngenerator = ReportGenerator(results_path=\"results/detections.json\")\nreport = generator.generate_report(\n    output_path=\"results/report.pdf\",\n    include_maps=True,\n    include_statistics=True\n)\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-7-filter-and-refine","title":"Step 7: Filter and Refine","text":""},{"location":"tutorials/end-to-end-detection/#71-filter-by-confidence","title":"7.1 Filter by Confidence","text":"<pre><code># Filter detections by confidence threshold\nfiltered_results = []\nconfidence_threshold = 0.7\n\nfor result in results:\n    filtered_dets = [\n        det for det in result[\"detections\"]\n        if det[\"confidence\"] &gt;= confidence_threshold\n    ]\n    if filtered_dets:\n        filtered_results.append({\n            **result,\n            \"detections\": filtered_dets\n        })\n\n# Save filtered results\nwith open(\"results/filtered_detections.json\", \"w\") as f:\n    json.dump(filtered_results, f, indent=2)\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#72-filter-by-species","title":"7.2 Filter by Species","text":"<pre><code># Keep only specific species\ntarget_species = [\"elephant\", \"giraffe\"]\n\nspecies_results = []\nfor result in results:\n    species_dets = [\n        det for det in result[\"detections\"]\n        if det[\"class_name\"] in target_species\n    ]\n    if species_dets:\n        species_results.append({\n            **result,\n            \"detections\": species_dets\n        })\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-8-export-results","title":"Step 8: Export Results","text":""},{"location":"tutorials/end-to-end-detection/#81-export-to-csv","title":"8.1 Export to CSV","text":"<pre><code>import csv\n\n# Convert to CSV\ncsv_data = []\nfor result in results:\n    for det in result[\"detections\"]:\n        csv_data.append({\n            \"image\": result[\"image_path\"],\n            \"species\": det[\"class_name\"],\n            \"confidence\": det[\"confidence\"],\n            \"x\": det[\"bbox\"][0],\n            \"y\": det[\"bbox\"][1],\n            \"width\": det[\"bbox\"][2],\n            \"height\": det[\"bbox\"][3]\n        })\n\n# Save CSV\nwith open(\"results/detections.csv\", \"w\", newline=\"\") as f:\n    writer = csv.DictWriter(f, fieldnames=csv_data[0].keys())\n    writer.writeheader()\n    writer.writerows(csv_data)\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#82-export-to-excel","title":"8.2 Export to Excel","text":"<pre><code>import pandas as pd\n\ndf = pd.DataFrame(csv_data)\n\n# Create Excel with multiple sheets\nwith pd.ExcelWriter(\"results/detections.xlsx\") as writer:\n    df.to_excel(writer, sheet_name=\"All Detections\", index=False)\n\n    # Summary by species\n    summary = df.groupby(\"species\").agg({\n        \"confidence\": [\"count\", \"mean\"]\n    })\n    summary.to_excel(writer, sheet_name=\"Summary\")\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#83-export-to-coco-format","title":"8.3 Export to COCO Format","text":"<pre><code>from wildetect.export import COCOExporter\n\nexporter = COCOExporter(results)\nexporter.export(\"results/detections_coco.json\")\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-9-advanced-processing","title":"Step 9: Advanced Processing","text":""},{"location":"tutorials/end-to-end-detection/#91-large-raster-detection","title":"9.1 Large Raster Detection","text":"<p>For large GeoTIFF files:</p> <pre><code># config/raster_detection.yaml\nmodel:\n  mlflow_model_name: \"detector\"\n  device: \"cuda\"\n\nimage_paths:\n  - \"D:/orthomosaics/large_ortho.tif\"\n\nprocessing:\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"raster\"\n  nms_threshold: 0.5\n\nflight_specs:\n  gsd: 2.38  # Required for raster detection\n\noutput:\n  directory: \"results/raster\"\n</code></pre> <pre><code>wildetect detect --config config/raster_detection.yaml\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#92-batch-processing-multiple-folders","title":"9.2 Batch Processing Multiple Folders","text":"<pre><code>from pathlib import Path\n\n# Process multiple folders\nfolders = [\n    \"D:/surveys/site_a/\",\n    \"D:/surveys/site_b/\",\n    \"D:/surveys/site_c/\"\n]\n\nfor folder in folders:\n    folder_name = Path(folder).name\n    results = pipeline.detect_batch(folder)\n    pipeline.save_results(results, f\"results/{folder_name}_detections.json\")\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/end-to-end-detection/#detection-is-slow","title":"Detection is Slow","text":"<p>Solutions: 1. Increase batch size (if GPU memory allows) 2. Use smaller tile size 3. Enable GPU acceleration 4. Use multithreaded pipeline</p>"},{"location":"tutorials/end-to-end-detection/#out-of-memory","title":"Out of Memory","text":"<p>Solutions: 1. Reduce batch size 2. Reduce tile size 3. Use CPU instead of GPU 4. Close other applications</p>"},{"location":"tutorials/end-to-end-detection/#low-detection-accuracy","title":"Low Detection Accuracy","text":"<p>Solutions: 1. Check model is appropriate for your data 2. Adjust confidence threshold 3. Verify image quality 4. Check GSD matches training data</p>"},{"location":"tutorials/end-to-end-detection/#model-wont-load","title":"Model Won't Load","text":"<p>Solutions: 1. Verify MLflow server is running 2. Check model name and alias 3. Verify model path if using file 4. Check CUDA availability</p>"},{"location":"tutorials/end-to-end-detection/#next-steps","title":"Next Steps","text":"<ul> <li>Census Campaign Tutorial - Run a full census</li> <li>Dataset Preparation - Prepare your own training data</li> <li>Model Training - Train custom models</li> <li>Geographic Visualization - Create maps</li> </ul>"},{"location":"tutorials/end-to-end-detection/#complete-example-script","title":"Complete Example Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nComplete detection workflow example\n\"\"\"\nfrom pathlib import Path\nfrom wildetect.core.pipeline import DetectionPipeline\nimport json\n\ndef main():\n    # Configuration\n    image_dir = Path(\"D:/wildlife_detection/images\")\n    output_dir = Path(\"D:/wildlife_detection/results\")\n    model_path = \"detector.pt\"\n\n    # Initialize pipeline\n    print(\"Initializing detection pipeline...\")\n    pipeline = DetectionPipeline(\n        model_path=model_path,\n        device=\"cuda\"\n    )\n\n    # Run detection\n    print(f\"Processing images in {image_dir}...\")\n    results = pipeline.detect_batch(image_dir)\n\n    # Save results\n    output_dir.mkdir(exist_ok=True)\n    results_file = output_dir / \"detections.json\"\n    pipeline.save_results(results, results_file)\n    print(f\"Results saved to {results_file}\")\n\n    # Print summary\n    total_detections = sum(len(r[\"detections\"]) for r in results)\n    print(f\"\\nSummary:\")\n    print(f\"  Images processed: {len(results)}\")\n    print(f\"  Total detections: {total_detections}\")\n\n    # Species breakdown\n    from collections import Counter\n    species = Counter()\n    for result in results:\n        for det in result[\"detections\"]:\n            species[det[\"class_name\"]] += 1\n\n    print(f\"\\nSpecies breakdown:\")\n    for name, count in species.items():\n        print(f\"  {name}: {count}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Save as <code>run_detection.py</code> and run:</p> <pre><code>python run_detection.py\n</code></pre> <p>Congratulations! You've completed the end-to-end detection tutorial. You now know how to run detection, visualize results, and export data for further analysis.</p>"},{"location":"tutorials/model-training/","title":"Model Training Tutorial","text":"<p>Learn how to train detection and classification models using WildTrain.</p>"},{"location":"tutorials/model-training/#prerequisites","title":"Prerequisites","text":"<ul> <li>WildTrain installed</li> <li>Prepared dataset (see Dataset Preparation)</li> <li>MLflow server running</li> </ul>"},{"location":"tutorials/model-training/#training-a-yolo-detector","title":"Training a YOLO Detector","text":""},{"location":"tutorials/model-training/#step-1-prepare-data","title":"Step 1: Prepare Data","text":"<p>Ensure YOLO format:</p> <pre><code>data/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 val/\n\u251c\u2500\u2500 labels/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 val/\n\u2514\u2500\u2500 data.yaml\n</code></pre> <p><code>data.yaml</code>: <pre><code>train: ./images/train\nval: ./images/val\nnc: 3\nnames: ['elephant', 'giraffe', 'zebra']\n</code></pre></p>"},{"location":"tutorials/model-training/#step-2-configure-training","title":"Step 2: Configure Training","text":"<p>Create <code>configs/yolo_train.yaml</code>:</p> <pre><code>model:\n  framework: \"yolo\"\n  size: \"n\"  # n, s, m, l, x\n  pretrained: true\n\ndata:\n  data_yaml: \"D:/data/wildlife/data.yaml\"\n\ntraining:\n  epochs: 100\n  imgsz: 640\n  batch: 16\n  device: 0\n\nmlflow:\n  experiment_name: \"yolo_wildlife\"\n</code></pre>"},{"location":"tutorials/model-training/#step-3-train","title":"Step 3: Train","text":"<pre><code>cd wildtrain\nscripts\\train_yolo.bat\n\n# Or with CLI\nwildtrain train detector -c configs/yolo_train.yaml\n</code></pre>"},{"location":"tutorials/model-training/#step-4-evaluate","title":"Step 4: Evaluate","text":"<pre><code>wildtrain eval detector -c configs/yolo_eval.yaml\n</code></pre>"},{"location":"tutorials/model-training/#step-5-register-model","title":"Step 5: Register Model","text":"<pre><code>wildtrain register detector configs/registration/detector_registration.yaml\n</code></pre>"},{"location":"tutorials/model-training/#training-a-classifier","title":"Training a Classifier","text":""},{"location":"tutorials/model-training/#step-1-prepare-roi-dataset","title":"Step 1: Prepare ROI Dataset","text":"<p>Use WilData to create ROI dataset from detections.</p>"},{"location":"tutorials/model-training/#step-2-configure","title":"Step 2: Configure","text":"<p><code>configs/classification_train.yaml</code>:</p> <pre><code>model:\n  architecture: \"resnet50\"\n  num_classes: 10\n  pretrained: true\n  learning_rate: 0.001\n\ndata:\n  root_data_directory: \"D:/data/roi_dataset\"\n  batch_size: 32\n\ntraining:\n  max_epochs: 100\n  accelerator: \"gpu\"\n</code></pre>"},{"location":"tutorials/model-training/#step-3-train_1","title":"Step 3: Train","text":"<pre><code>cd wildtrain\nscripts\\train_classifier.bat\n</code></pre>"},{"location":"tutorials/model-training/#monitor-with-mlflow","title":"Monitor with MLflow","text":"<pre><code># Start MLflow\nscripts\\launch_mlflow.bat\n\n# Access at http://localhost:5000\n</code></pre> <p>View: - Training metrics - Model performance - Hyperparameters - Artifacts</p>"},{"location":"tutorials/model-training/#complete-python-example","title":"Complete Python Example","text":"<pre><code>from wildtrain import Trainer\n\n# Configure\nconfig = Trainer.load_config(\"configs/yolo_train.yaml\")\n\n# Train\ntrainer = Trainer(config)\nmodel = trainer.train()\n\n# Evaluate\nmetrics = trainer.evaluate()\n\n# Register\ntrainer.register_model(\n    model_name=\"wildlife_detector\",\n    tags={\"dataset\": \"wildlife_v1\"}\n)\n</code></pre> <p>Next Steps: - End-to-End Detection - WildTrain Scripts</p>"}]}