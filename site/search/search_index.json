{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"WildDetect Monorepo Documentation","text":"<p>Welcome to the comprehensive documentation for the WildDetect monorepo - a complete wildlife monitoring and conservation toolkit for aerial imagery analysis.</p>"},{"location":"#what-is-wilddetect","title":"What is WildDetect?","text":"<p>WildDetect is an integrated ecosystem of three specialized packages designed to streamline the entire wildlife detection workflow, from data management to model training and deployment:</p> <ul> <li>WilData - Data pipeline and management</li> <li>WildTrain - Model training and evaluation  </li> <li>WildDetect - Detection deployment and census analysis</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#complete-wildlife-detection-pipeline","title":"\ud83c\udfaf Complete Wildlife Detection Pipeline","text":"<ul> <li>Multi-species detection using state-of-the-art YOLO models</li> <li>Batch processing of large-scale aerial imagery</li> <li>Automated census campaigns with population statistics</li> <li>Geographic visualization and analysis</li> </ul>"},{"location":"#comprehensive-data-management","title":"\ud83d\udce6 Comprehensive Data Management","text":"<ul> <li>Import from COCO, YOLO, and Label Studio formats</li> <li>Data transformations (tiling, augmentation, filtering)</li> <li>DVC integration for dataset versioning</li> <li>ROI extraction for hard sample mining</li> </ul>"},{"location":"#flexible-training-framework","title":"\ud83e\udde0 Flexible Training Framework","text":"<ul> <li>Support for YOLO and MMDetection frameworks</li> <li>Classification and object detection training</li> <li>MLflow experiment tracking</li> <li>Hyperparameter optimization with Optuna</li> </ul>"},{"location":"#geographic-analysis","title":"\ud83c\udf0d Geographic Analysis","text":"<ul> <li>GPS metadata extraction and management</li> <li>Flight path analysis and coverage maps</li> <li>Interactive visualizations with FiftyOne</li> <li>Population density and distribution analysis</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#for-new-users","title":"For New Users","text":"<p>Start here to get up and running quickly:</p> <ul> <li>Installation Guide - Install all packages</li> <li>Quick Start - Your first detection</li> <li>Environment Setup - Configure your environment</li> </ul>"},{"location":"#for-researchers-conservationists","title":"For Researchers &amp; Conservationists","text":"<p>Learn how to use the tools for your wildlife monitoring needs:</p> <ul> <li>End-to-End Detection Tutorial - Complete workflow</li> <li>Census Campaign Guide - Run a census campaign</li> <li>Scripts Reference - Available scripts and tools</li> </ul>"},{"location":"#for-developers","title":"For Developers","text":"<p>Understand the architecture and extend the toolkit:</p> <ul> <li>Architecture Overview - System design and components</li> <li>Python API Reference - Programmatic usage</li> <li>Data Flow - How data moves through the system</li> </ul>"},{"location":"#for-data-scientists-ml-engineers","title":"For Data Scientists &amp; ML Engineers","text":"<p>Prepare datasets and train models:</p> <ul> <li>Dataset Preparation - Data pipeline tutorial</li> <li>Model Training - Train custom models</li> <li>Configuration Reference - Configuration files</li> </ul>"},{"location":"#package-overview","title":"Package Overview","text":""},{"location":"#wildata-data-pipeline","title":"\ud83d\uddc2\ufe0f WilData - Data Pipeline","text":"<p>The foundation for dataset management and preparation.</p> <p>Key Capabilities: - Import datasets from multiple formats (COCO, YOLO, Label Studio) - Apply transformations (tiling, augmentation, bbox clipping) - Create ROI datasets for classification - Update GPS metadata from CSV files - DVC integration for version control - REST API for programmatic access</p> <p>Learn more about WilData \u2192</p>"},{"location":"#wildtrain-training-framework","title":"\ud83c\udf93 WildTrain - Training Framework","text":"<p>Modular training system for detection and classification models.</p> <p>Key Capabilities: - YOLO and MMDetection framework support - PyTorch Lightning for classification - Hydra configuration management - MLflow experiment tracking - Model registration and versioning - Hyperparameter tuning</p> <p>Learn more about WildTrain \u2192</p>"},{"location":"#wilddetect-detection-analysis","title":"\ud83d\udd0d WildDetect - Detection &amp; Analysis","text":"<p>Production-ready detection and census system.</p> <p>Key Capabilities: - Multi-threaded detection pipelines - Raster (large image) detection support - Census campaign orchestration - Geographic analysis and visualization - FiftyOne integration - Comprehensive reporting (JSON, CSV)</p> <p>Learn more about WildDetect \u2192</p>"},{"location":"#common-workflows","title":"Common Workflows","text":""},{"location":"#detection-workflow","title":"Detection Workflow","text":"<pre><code>graph LR\n    A[Aerial Images] --&gt; B[WildDetect]\n    B --&gt; C[Detections]\n    C --&gt; D[Analysis]\n    D --&gt; E[Reports &amp; Maps]</code></pre>"},{"location":"#training-workflow","title":"Training Workflow","text":"<pre><code>graph LR\n    A[Annotations] --&gt; B[WilData]\n    B --&gt; C[Processed Dataset]\n    C --&gt; D[WildTrain]\n    D --&gt; E[Trained Model]\n    E --&gt; F[WildDetect]</code></pre>"},{"location":"#census-workflow","title":"Census Workflow","text":"<pre><code>graph LR\n    A[Flight Planning] --&gt; B[Image Capture]\n    B --&gt; C[WildDetect Census]\n    C --&gt; D[Statistics]\n    D --&gt; E[Geographic Viz]\n    E --&gt; F[Conservation Reports]</code></pre>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Tutorials: Step-by-step guides for common tasks</li> <li>API Reference: Complete command and function documentation</li> <li>Troubleshooting: Solutions to common issues</li> <li>GitHub Issues: Report bugs or request features</li> </ul>"},{"location":"#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.9 or higher</li> <li>GPU: CUDA-capable GPU recommended (optional)</li> <li>OS: Windows, Linux, macOS</li> <li>Memory: 16GB RAM minimum, 32GB recommended</li> <li>Storage: SSD recommended for large datasets</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! This is an open-source project designed for the conservation community.</p> <ul> <li>Submit bug reports and feature requests via GitHub Issues</li> <li>Contribute code via pull requests</li> <li>Share your use cases and results</li> <li>Help improve documentation</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE files in each package for details.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use WildDetect in your research, please cite:</p> <pre><code>@software{wildetect2025,\n  author = {Seydou, Fadel M.},\n  title = {WildDetect: Wildlife Detection and Census System for Aerial Imagery},\n  year = {2025},\n  url = {https://github.com/fadelmamar/wildetect}\n}\n</code></pre>"},{"location":"#support","title":"Support","text":"<p>For questions and support: - \ud83d\udce7 Email: [your-email@example.com] - \ud83d\udcac GitHub Discussions - \ud83d\udc1b GitHub Issues</p> <p>Ready to get started? Head to the Installation Guide to set up your environment.</p>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>Common issues and solutions for the WildDetect monorepo.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#uv-command-not-found","title":"uv command not found","text":"<p>Solution: <pre><code># Windows (PowerShell)\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Linux/macOS\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></p>"},{"location":"troubleshooting/#importerror-after-installation","title":"ImportError after installation","text":"<p>Solution: <pre><code># Ensure virtual environment is activated\n.venv\\Scripts\\activate  # Windows\nsource .venv/bin/activate  # Linux/macOS\n\n# Reinstall in development mode\ncd wildetect\nuv pip install -e .\n</code></pre></p>"},{"location":"troubleshooting/#gpu-and-cuda-issues","title":"GPU and CUDA Issues","text":""},{"location":"troubleshooting/#cuda-out-of-memory","title":"CUDA out of memory","text":"<p>Solutions: 1. Reduce batch size:    <pre><code>processing:\n  batch_size: 16  # Reduce from 32\n</code></pre></p> <ol> <li> <p>Reduce tile size:    <pre><code>processing:\n  tile_size: 640  # Reduce from 800\n</code></pre></p> </li> <li> <p>Clear GPU cache:    <pre><code>import torch\ntorch.cuda.empty_cache()\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#gpu-not-detected","title":"GPU not detected","text":"<p>Check CUDA: <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\n</code></pre></p> <p>Solutions: 1. Reinstall PyTorch with CUDA:    <pre><code>uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p> <ol> <li>Set device explicitly:    <pre><code>model:\n  device: \"cuda:0\"\n</code></pre></li> </ol>"},{"location":"troubleshooting/#windows-specific-issues","title":"Windows-Specific Issues","text":""},{"location":"troubleshooting/#processpool-not-supported","title":"ProcessPool not supported","text":"<p>Issue: <code>ProcessPoolExecutor</code> doesn't work on Windows</p> <p>Solution: Package automatically uses <code>ThreadPoolExecutor</code> on Windows</p>"},{"location":"troubleshooting/#path-issues","title":"Path issues","text":"<p>Use forward slashes or raw strings: <pre><code># Good\npath = \"D:/data/images\"\npath = r\"D:\\data\\images\"\n\n# Bad\npath = \"D:\\data\\images\"  # Backslashes can cause issues\n</code></pre></p>"},{"location":"troubleshooting/#mlflow-issues","title":"MLflow Issues","text":""},{"location":"troubleshooting/#cant-connect-to-mlflow-server","title":"Can't connect to MLflow server","text":"<p>Solutions: 1. Start MLflow server:    <pre><code>scripts\\launch_mlflow.bat\n</code></pre></p> <ol> <li> <p>Check environment variable:    <pre><code>echo %MLFLOW_TRACKING_URI%\n# Should be: http://localhost:5000\n</code></pre></p> </li> <li> <p>Set in <code>.env</code>:    <pre><code>MLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#model-not-found-in-registry","title":"Model not found in registry","text":"<p>Solutions: 1. List available models:    <pre><code>mlflow models list\n</code></pre></p> <ol> <li>Check model name and alias:    <pre><code>model:\n  mlflow_model_name: \"detector\"  # Check this is correct\n  mlflow_model_alias: \"production\"  # or version number\n</code></pre></li> </ol>"},{"location":"troubleshooting/#data-loading-issues","title":"Data Loading Issues","text":""},{"location":"troubleshooting/#images-not-found","title":"Images not found","text":"<p>Solutions: 1. Use absolute paths 2. Check file extensions match 3. Verify directory structure</p>"},{"location":"troubleshooting/#annotation-format-errors","title":"Annotation format errors","text":"<p>Solutions: 1. Validate COCO format:    <pre><code>from wildata.validation import validate_coco\nerrors = validate_coco(\"annotations.json\")\n</code></pre></p> <ol> <li>Check bbox coordinates</li> <li>Verify image IDs match</li> </ol>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#detection-is-slow","title":"Detection is slow","text":"<p>Solutions: 1. Use GPU if available 2. Increase batch size 3. Use multithreaded pipeline:    <pre><code>processing:\n  pipeline_type: \"multithreaded\"\n  num_data_workers: 4\n</code></pre></p>"},{"location":"troubleshooting/#high-memory-usage","title":"High memory usage","text":"<p>Solutions: 1. Enable streaming mode (WilData):    <pre><code>processing_mode: \"streaming\"\n</code></pre></p> <ol> <li>Process in smaller batches</li> <li>Clear cache between batches</li> </ol>"},{"location":"troubleshooting/#dvc-issues","title":"DVC Issues","text":""},{"location":"troubleshooting/#dvc-push-fails","title":"DVC push fails","text":"<p>Solutions: 1. Check remote configuration:    <pre><code>dvc remote list\n</code></pre></p> <ol> <li> <p>Verify credentials:    <pre><code>dvc remote modify myremote access_key_id YOUR_KEY\n</code></pre></p> </li> <li> <p>Test connection:    <pre><code>dvc remote list\ndvc status\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#label-studio-integration","title":"Label Studio Integration","text":""},{"location":"troubleshooting/#cant-connect-to-label-studio","title":"Can't connect to Label Studio","text":"<p>Solutions: 1. Start Label Studio:    <pre><code>scripts\\launch_labelstudio.bat\n</code></pre></p> <ol> <li>Check API key in <code>.env</code>:    <pre><code>LABEL_STUDIO_API_KEY=your_key\nLABEL_STUDIO_URL=http://localhost:8080\n</code></pre></li> </ol>"},{"location":"troubleshooting/#fiftyone-issues","title":"FiftyOne Issues","text":""},{"location":"troubleshooting/#fiftyone-app-wont-launch","title":"FiftyOne app won't launch","text":"<p>Solutions: 1. Check FiftyOne installation:    <pre><code>uv pip install fiftyone\n</code></pre></p> <ol> <li> <p>Clear FiftyOne database:    <pre><code>fiftyone app config database_dir\n</code></pre></p> </li> <li> <p>Use different port:    <pre><code>fiftyone app launch --port 5152\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#common-error-messages","title":"Common Error Messages","text":""},{"location":"troubleshooting/#no-module-named-wildetect","title":"\"No module named 'wildetect'\"","text":"<p>Solution: Install in development mode: <pre><code>cd wildetect\nuv pip install -e .\n</code></pre></p>"},{"location":"troubleshooting/#permission-denied","title":"\"Permission denied\"","text":"<p>Solution: Run as administrator or fix permissions: <pre><code>icacls \"D:\\data\" /grant %USERNAME%:F /t\n</code></pre></p>"},{"location":"troubleshooting/#port-already-in-use","title":"\"Port already in use\"","text":"<p>Solution: Kill process using port: <pre><code># Find process\nnetstat -ano | findstr :5000\n\n# Kill process (replace PID)\ntaskkill /PID &lt;PID&gt; /F\n</code></pre></p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<ol> <li>Check logs: Look in <code>logs/</code> directory</li> <li>Enable verbose mode: Add <code>--verbose</code> flag</li> <li>GitHub Issues: Report bugs</li> <li>Discussions: Ask questions in GitHub Discussions</li> </ol>"},{"location":"troubleshooting/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging:</p> <pre><code>logging:\n  log_level: \"DEBUG\"\n  verbose: true\n</code></pre> <p>Or in Python:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre> <p>Still having issues? Open an issue on GitHub with: - Error message - System info (<code>wildetect info</code>) - Steps to reproduce - Relevant configuration files</p>"},{"location":"api-reference/python-api/","title":"Python API Reference","text":"<p>Using WildDetect packages programmatically.</p>"},{"location":"api-reference/python-api/#wilddetect","title":"WildDetect","text":""},{"location":"api-reference/python-api/#detection-pipeline","title":"Detection Pipeline","text":"<pre><code>from wildetect.core.pipeline import DetectionPipeline\n\n# Initialize\npipeline = DetectionPipeline(\n    model_path=\"detector.pt\",\n    device=\"cuda\"\n)\n\n# Detect single image\nresult = pipeline.detect(\"image.jpg\")\n\n# Detect batch\nresults = pipeline.detect_batch(\"images/\")\n\n# Save results\npipeline.save_results(results, \"results.json\")\n</code></pre>"},{"location":"api-reference/python-api/#census-engine","title":"Census Engine","text":"<pre><code>from wildetect.core.census import CensusEngine, CensusConfig\n\n# Configure\nconfig = CensusConfig.from_yaml(\"config/census.yaml\")\n\n# Run census\nengine = CensusEngine(config)\ncensus_result = engine.run_census(\"survey_images/\")\n\n# Generate report\ncensus_result.save_report(\"report.pdf\")\n</code></pre>"},{"location":"api-reference/python-api/#wildata","title":"WilData","text":""},{"location":"api-reference/python-api/#data-pipeline","title":"Data Pipeline","text":"<pre><code>from wildata.pipeline import DataPipeline\n\n# Initialize\npipeline = DataPipeline(\"data\")\n\n# Import dataset\nresult = pipeline.import_dataset(\n    source_path=\"annotations.json\",\n    source_format=\"coco\",\n    dataset_name=\"my_dataset\"\n)\n\n# List datasets\ndatasets = pipeline.list_datasets()\n\n# Export\npipeline.export_dataset(\"my_dataset\", \"yolo\")\n</code></pre>"},{"location":"api-reference/python-api/#roi-adapter","title":"ROI Adapter","text":"<pre><code>from wildata.adapters import ROIAdapter\nimport json\n\n# Load COCO data\nwith open(\"annotations.json\") as f:\n    coco_data = json.load(f)\n\n# Create ROI adapter\nadapter = ROIAdapter(\n    coco_data,\n    roi_box_size=128,\n    random_roi_count=10\n)\n\n# Convert\nroi_data = adapter.convert()\n\n# Save\nadapter.save(roi_data, \"roi_dataset/\")\n</code></pre>"},{"location":"api-reference/python-api/#wildtrain","title":"WildTrain","text":""},{"location":"api-reference/python-api/#training-classification","title":"Training (Classification)","text":"<pre><code>from wildtrain.models import ImageClassifier\nfrom wildtrain.data import ClassificationDataModule\nimport pytorch_lightning as pl\n\n# Create model\nmodel = ImageClassifier(\n    architecture=\"resnet50\",\n    num_classes=10,\n    learning_rate=0.001\n)\n\n# Create data module\ndatamodule = ClassificationDataModule(\n    data_root=\"data/roi_dataset\",\n    batch_size=32\n)\n\n# Train\ntrainer = pl.Trainer(max_epochs=100, accelerator=\"gpu\")\ntrainer.fit(model, datamodule)\n</code></pre>"},{"location":"api-reference/python-api/#model-registry","title":"Model Registry","text":"<pre><code>from wildtrain.registry import ModelRegistry\n\n# Initialize\nregistry = ModelRegistry(\"http://localhost:5000\")\n\n# Register model\nversion = registry.register_model(\n    model_path=\"checkpoints/best.ckpt\",\n    model_name=\"wildlife_classifier\",\n    description=\"ResNet50 classifier\",\n    tags={\"accuracy\": \"0.95\"}\n)\n\n# Load model\nmodel = registry.load_model(\"wildlife_classifier\", version=\"latest\")\n\n# Promote to production\nregistry.promote_model(\"wildlife_classifier\", version, \"Production\")\n</code></pre> <p>For complete architecture details, see: - WildDetect Architecture - WilData Architecture - WildTrain Architecture</p>"},{"location":"api-reference/wildata-api/","title":"WilData REST API Reference","text":"<p>FastAPI-based REST API for WilData operations.</p>"},{"location":"api-reference/wildata-api/#getting-started","title":"Getting Started","text":""},{"location":"api-reference/wildata-api/#start-api-server","title":"Start API Server","text":"<pre><code>cd wildata\nscripts\\launch_api.bat\n</code></pre> <p>Access: - API: <code>http://localhost:8441</code> - Docs: <code>http://localhost:8441/docs</code> - Redoc: <code>http://localhost:8441/redoc</code></p>"},{"location":"api-reference/wildata-api/#endpoints","title":"Endpoints","text":""},{"location":"api-reference/wildata-api/#health-check","title":"Health Check","text":"<pre><code>GET /api/v1/health\n</code></pre>"},{"location":"api-reference/wildata-api/#import-dataset","title":"Import Dataset","text":"<pre><code>POST /api/v1/datasets/import\nContent-Type: application/json\n\n{\n  \"source_path\": \"/path/to/annotations.json\",\n  \"source_format\": \"coco\",\n  \"dataset_name\": \"my_dataset\",\n  \"root\": \"data\"\n}\n</code></pre>"},{"location":"api-reference/wildata-api/#list-datasets","title":"List Datasets","text":"<pre><code>GET /api/v1/datasets?root=data\n</code></pre>"},{"location":"api-reference/wildata-api/#create-roi-dataset","title":"Create ROI Dataset","text":"<pre><code>POST /api/v1/roi/create\nContent-Type: application/json\n\n{\n  \"source_path\": \"/path/to/data.json\",\n  \"source_format\": \"coco\",\n  \"dataset_name\": \"roi_dataset\",\n  \"roi_config\": {\n    \"roi_box_size\": 128,\n    \"random_roi_count\": 10\n  }\n}\n</code></pre>"},{"location":"api-reference/wildata-api/#job-status","title":"Job Status","text":"<pre><code>GET /api/v1/jobs/{job_id}\n</code></pre>"},{"location":"api-reference/wildata-api/#python-client-example","title":"Python Client Example","text":"<pre><code>import requests\n\n# Import dataset\nresponse = requests.post(\n    \"http://localhost:8441/api/v1/datasets/import\",\n    json={\n        \"source_path\": \"annotations.json\",\n        \"source_format\": \"coco\",\n        \"dataset_name\": \"my_dataset\"\n    }\n)\n\njob_id = response.json()[\"job_id\"]\n\n# Check job status\nstatus_response = requests.get(\n    f\"http://localhost:8441/api/v1/jobs/{job_id}\"\n)\n\nprint(status_response.json())\n</code></pre> <p>For complete API documentation, see: - Interactive docs at <code>/docs</code> endpoint when the API server is running - API endpoint reference in this document</p>"},{"location":"api-reference/wildata-cli/","title":"WilData CLI Reference","text":"<p>Complete command-line interface reference for WilData.</p>"},{"location":"api-reference/wildata-cli/#main-commands","title":"Main Commands","text":""},{"location":"api-reference/wildata-cli/#import-dataset","title":"import-dataset","text":"<p>Import dataset from various formats.</p> <pre><code>wildata import-dataset [SOURCE] [OPTIONS]\n</code></pre> <p>Options: - <code>-f, --format TEXT</code>: Source format (coco/yolo/ls) - <code>-n, --name TEXT</code>: Dataset name - <code>-c, --config PATH</code>: Config file path - <code>--root PATH</code>: Data root directory - <code>--split TEXT</code>: Split name (train/val/test) - <code>--enable-tiling</code>: Enable image tiling - <code>--tile-size INTEGER</code>: Tile size - <code>-v, --verbose</code>: Verbose output</p> <p>Examples: <pre><code># From config\nwildata import-dataset --config configs/import-config.yaml\n\n# Direct arguments\nwildata import-dataset data.json --format coco --name dataset1\n</code></pre></p>"},{"location":"api-reference/wildata-cli/#bulk-import-datasets","title":"bulk-import-datasets","text":"<p>Bulk import multiple datasets.</p> <pre><code>wildata bulk-import-datasets [OPTIONS]\n</code></pre> <p>Options: - <code>-c, --config PATH</code>: Bulk import config - <code>-n, --num-workers INTEGER</code>: Number of workers</p>"},{"location":"api-reference/wildata-cli/#create-roi-dataset","title":"create-roi-dataset","text":"<p>Create ROI classification dataset from detection annotations.</p> <pre><code>wildata create-roi-dataset [OPTIONS]\n</code></pre> <p>Options: - <code>-c, --config PATH</code>: ROI config file - <code>--roi-size INTEGER</code>: ROI box size - <code>--random-count INTEGER</code>: Background samples per image</p>"},{"location":"api-reference/wildata-cli/#dataset-list","title":"dataset list","text":"<p>List all datasets.</p> <pre><code>wildata dataset list [--root PATH]\n</code></pre>"},{"location":"api-reference/wildata-cli/#dataset-export","title":"dataset export","text":"<p>Export dataset to format.</p> <pre><code>wildata dataset export &lt;name&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--format TEXT</code>: Target format (coco/yolo) - <code>--output PATH</code>: Output directory</p>"},{"location":"api-reference/wildata-cli/#visualize-dataset","title":"visualize-dataset","text":"<p>Launch dataset visualization.</p> <pre><code>wildata visualize-dataset --dataset &lt;name&gt; --split &lt;split&gt;\n</code></pre>"},{"location":"api-reference/wildata-cli/#update-gps-from-csv","title":"update-gps-from-csv","text":"<p>Update image GPS metadata from CSV.</p> <pre><code>wildata update-gps-from-csv [OPTIONS]\n</code></pre> <p>Options: - <code>-c, --config PATH</code>: GPS update config - <code>--image-folder PATH</code>: Image folder - <code>--csv PATH</code>: CSV file path - <code>--output PATH</code>: Output directory</p> <p>For detailed script documentation, see WilData Scripts.</p>"},{"location":"api-reference/wildetect-cli/","title":"WildDetect CLI Reference","text":"<p>Complete command-line interface reference for WildDetect.</p>"},{"location":"api-reference/wildetect-cli/#main-commands","title":"Main Commands","text":"<pre><code>wildetect [COMMAND] [OPTIONS]\n</code></pre>"},{"location":"api-reference/wildetect-cli/#detect","title":"detect","text":"<p>Run wildlife detection on images.</p> <pre><code>wildetect detect &lt;images&gt; [OPTIONS]\n</code></pre> <p>Arguments: - <code>images</code>: Path to image file or directory</p> <p>Options: - <code>-m, --model TEXT</code>: Model path or MLflow model name - <code>-c, --config PATH</code>: Configuration file path - <code>-o, --output PATH</code>: Output directory - <code>--device TEXT</code>: Device (cuda/cpu/auto) - <code>--batch-size INTEGER</code>: Batch size - <code>--confidence FLOAT</code>: Confidence threshold - <code>--tile-size INTEGER</code>: Tile size for large images - <code>-v, --verbose</code>: Verbose output</p> <p>Examples: <pre><code># Basic detection\nwildetect detect images/ --model detector.pt\n\n# With config file\nwildetect detect images/ -c config/detection.yaml\n\n# Custom settings\nwildetect detect images/ --model detector.pt --batch-size 32 --confidence 0.7\n</code></pre></p>"},{"location":"api-reference/wildetect-cli/#census","title":"census","text":"<p>Run census campaign with analysis and reporting.</p> <pre><code>wildetect census &lt;campaign_name&gt; &lt;images&gt; [OPTIONS]\n</code></pre> <p>Arguments: - <code>campaign_name</code>: Census campaign name - <code>images</code>: Image directory</p> <p>Options: - <code>-c, --config PATH</code>: Configuration file (required) - <code>-o, --output PATH</code>: Output directory - <code>--species TEXT</code>: Target species (comma-separated) - <code>--generate-report</code>: Generate PDF report</p> <p>Examples: <pre><code>wildetect census summer_2024 images/ -c config/census.yaml\n</code></pre></p>"},{"location":"api-reference/wildetect-cli/#analyze","title":"analyze","text":"<p>Analyze detection results.</p> <pre><code>wildetect analyze &lt;results&gt; [OPTIONS]\n</code></pre> <p>Arguments: - <code>results</code>: Path to detection results JSON</p> <p>Options: - <code>-o, --output PATH</code>: Output directory - <code>--format TEXT</code>: Output format (json/csv/excel)</p>"},{"location":"api-reference/wildetect-cli/#fiftyone","title":"fiftyone","text":"<p>Manage FiftyOne datasets.</p> <pre><code>wildetect fiftyone [OPTIONS]\n</code></pre> <p>Options: - <code>--action TEXT</code>: Action (launch/info/export) - <code>--dataset TEXT</code>: Dataset name - <code>--port INTEGER</code>: Port number</p> <p>Examples: <pre><code># Launch viewer\nwildetect fiftyone --action launch --dataset my_dataset\n\n# Get dataset info\nwildetect fiftyone --action info --dataset my_dataset\n\n# Export\nwildetect fiftyone --action export --format coco --output export/\n</code></pre></p>"},{"location":"api-reference/wildetect-cli/#ui","title":"ui","text":"<p>Launch Streamlit web interface.</p> <pre><code>wildetect ui\n</code></pre>"},{"location":"api-reference/wildetect-cli/#info","title":"info","text":"<p>Show system and environment information.</p> <pre><code>wildetect info\n</code></pre> <p>For detailed script documentation, see WildDetect Scripts.</p>"},{"location":"api-reference/wildtrain-cli/","title":"WildTrain CLI Reference","text":"<p>Complete command-line interface reference for WildTrain.</p>"},{"location":"api-reference/wildtrain-cli/#main-commands","title":"Main Commands","text":""},{"location":"api-reference/wildtrain-cli/#train","title":"train","text":"<p>Train a model.</p> <pre><code>wildtrain train &lt;task&gt; [OPTIONS]\n</code></pre> <p>Arguments: - <code>task</code>: Task type (classifier/detector)</p> <p>Options: - <code>-c, --config PATH</code>: Config file (required) - <code>--resume PATH</code>: Resume from checkpoint - <code>--dry-run</code>: Dry run without training</p> <p>Examples: <pre><code># Train classifier\nwildtrain train classifier -c configs/classification/train.yaml\n\n# Train detector\nwildtrain train detector -c configs/detection/yolo.yaml\n</code></pre></p>"},{"location":"api-reference/wildtrain-cli/#eval","title":"eval","text":"<p>Evaluate a trained model.</p> <pre><code>wildtrain eval &lt;task&gt; [OPTIONS]\n</code></pre> <p>Arguments: - <code>task</code>: Task type (classifier/detector)</p> <p>Options: - <code>-c, --config PATH</code>: Config file (required) - <code>--checkpoint PATH</code>: Model checkpoint - <code>--split TEXT</code>: Dataset split (test/val)</p> <p>Examples: <pre><code>wildtrain eval classifier -c configs/classification/eval.yaml\nwildtrain eval detector -c configs/detection/yolo_eval.yaml\n</code></pre></p>"},{"location":"api-reference/wildtrain-cli/#register","title":"register","text":"<p>Register model to MLflow registry.</p> <pre><code>wildtrain register &lt;model_type&gt; &lt;config&gt;\n</code></pre> <p>Arguments: - <code>model_type</code>: Model type (classifier/detector) - <code>config</code>: Registration config file</p> <p>Example: <pre><code>wildtrain register detector configs/registration/detector_registration.yaml\n</code></pre></p>"},{"location":"api-reference/wildtrain-cli/#tune","title":"tune","text":"<p>Run hyperparameter tuning.</p> <pre><code>wildtrain tune &lt;task&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>-c, --config PATH</code>: Config file - <code>--n-trials INTEGER</code>: Number of trials</p> <p>Example: <pre><code>wildtrain tune classifier -c configs/classification/sweep.yaml --n-trials 50\n</code></pre></p>"},{"location":"api-reference/wildtrain-cli/#serve","title":"serve","text":"<p>Start inference server.</p> <pre><code>wildtrain serve [OPTIONS]\n</code></pre> <p>Options: - <code>-c, --config PATH</code>: Inference config - <code>--port INTEGER</code>: Server port - <code>--workers INTEGER</code>: Number of workers</p> <p>For detailed script documentation, see WildTrain Scripts.</p>"},{"location":"architecture/data-flow/","title":"Data Flow","text":"<p>This document describes how data flows through the WildDetect ecosystem, from raw annotations to final detection results and analysis.</p>"},{"location":"architecture/data-flow/#complete-pipeline-overview","title":"Complete Pipeline Overview","text":"<pre><code>flowchart TB\n    subgraph \"Stage 1: Data Collection\"\n        A[Raw Images]\n        B[Annotation Tools&lt;br/&gt;Label Studio/CVAT]\n        C[Annotations&lt;br/&gt;COCO/YOLO/LS]\n    end\n\n    subgraph \"Stage 2: Data Preparation (WilData)\"\n        D[Import &amp; Validate]\n        E[Transformations&lt;br/&gt;Tile/Augment/Clip]\n        F[Master Format&lt;br/&gt;Storage]\n        G[Export&lt;br/&gt;Train/Val/Test]\n    end\n\n    subgraph \"Stage 3: Model Training (WildTrain)\"\n        H[DataLoader]\n        I[Training Loop]\n        J[Validation]\n        K[MLflow Tracking]\n        L[Model Registry]\n    end\n\n    subgraph \"Stage 4: Deployment (WildDetect)\"\n        M[Load Model]\n        N[Detection Pipeline]\n        O[Detections]\n    end\n\n    subgraph \"Stage 5: Analysis\"\n        P[Census Statistics]\n        Q[Geographic Analysis]\n        R[Visualizations]\n        S[Reports]\n    end\n\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n    J --&gt; K\n    I --&gt; L\n    L --&gt; M\n    M --&gt; N\n    A --&gt; N\n    N --&gt; O\n    O --&gt; P\n    O --&gt; Q\n    Q --&gt; R\n    P --&gt; S\n\n    style F fill:#e3f2fd\n    style L fill:#fff3e0\n    style O fill:#e8f5e9\n    style S fill:#f3e5f5</code></pre>"},{"location":"architecture/data-flow/#stage-1-data-collection","title":"Stage 1: Data Collection","text":""},{"location":"architecture/data-flow/#raw-image-acquisition","title":"Raw Image Acquisition","text":"<p>Aerial images captured from drones or aircraft:</p> <pre><code>Input: Raw aerial images\nFormat: JPG, PNG, TIFF, GeoTIFF\nMetadata: GPS coordinates (EXIF), flight parameters\nSize: Varies (100MB - 10GB per image for rasters)\n</code></pre>"},{"location":"architecture/data-flow/#annotation-process","title":"Annotation Process","text":"<p>Images are annotated using labeling tools:</p> <p>Supported Tools: - Label Studio (recommended for collaboration) - CVAT (Computer Vision Annotation Tool) - Manual COCO/YOLO annotation</p> <p>Output Formats: - COCO JSON: <code>annotations.json</code> - YOLO: <code>labels/*.txt</code> + <code>data.yaml</code> - Label Studio: Export JSON</p>"},{"location":"architecture/data-flow/#example-label-studio-workflow","title":"Example: Label Studio Workflow","text":"<pre><code># 1. Setup Label Studio project\n# 2. Upload images\n# 3. Annotate with bounding boxes\n# 4. Export annotations\n\n# Example export structure\n{\n  \"annotations\": [\n    {\n      \"id\": 1,\n      \"image\": \"drone_001.jpg\",\n      \"annotations\": [\n        {\n          \"result\": [{\n            \"value\": {\n              \"x\": 10, \"y\": 20, \"width\": 50, \"height\": 60,\n              \"rectanglelabels\": [\"elephant\"]\n            }\n          }]\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"architecture/data-flow/#stage-2-data-preparation-wildata","title":"Stage 2: Data Preparation (WilData)","text":""},{"location":"architecture/data-flow/#import-process","title":"Import Process","text":"<pre><code>flowchart LR\n    A[Source&lt;br/&gt;Annotations] --&gt; B[Format&lt;br/&gt;Adapter]\n    B --&gt; C[Master&lt;br/&gt;Format]\n    C --&gt; D[Validation]\n    D --&gt; E{Valid?}\n    E --&gt;|Yes| F[Save]\n    E --&gt;|No| G[Error Report]\n    F --&gt; H[Master&lt;br/&gt;Storage]</code></pre>"},{"location":"architecture/data-flow/#format-conversion","title":"Format Conversion","text":"<p>All formats converted to unified master format:</p> <pre><code># COCO Input\n{\n  \"images\": [...],\n  \"annotations\": [...],\n  \"categories\": [...]\n}\n\n# \u2193 Converted to \u2193\n\n# Master Format\n{\n  \"info\": {\n    \"dataset_name\": \"my_dataset\",\n    \"source_format\": \"coco\",\n    \"created_at\": \"2024-01-01T00:00:00\"\n  },\n  \"images\": [\n    {\n      \"id\": 1,\n      \"file_name\": \"image.jpg\",\n      \"width\": 1920,\n      \"height\": 1080,\n      \"path\": \"data/images/train/image.jpg\"\n    }\n  ],\n  \"annotations\": [\n    {\n      \"id\": 1,\n      \"image_id\": 1,\n      \"category_id\": 1,\n      \"bbox\": [x, y, width, height],\n      \"area\": 12000,\n      \"confidence\": 1.0\n    }\n  ],\n  \"categories\": [\n    {\"id\": 1, \"name\": \"elephant\"}\n  ]\n}\n</code></pre>"},{"location":"architecture/data-flow/#transformation-pipeline","title":"Transformation Pipeline","text":"<p>Data transformations applied sequentially:</p>"},{"location":"architecture/data-flow/#1-bbox-clipping","title":"1. Bbox Clipping","text":"<pre><code># Before\nbbox = [x=-10, y=20, width=100, height=80]  # Outside image bounds\n\n# After clipping\nbbox = [x=0, y=20, width=90, height=80]  # Clipped to image\n</code></pre>"},{"location":"architecture/data-flow/#2-tiling","title":"2. Tiling","text":"<p>For large images:</p> <pre><code># Original: 8000x6000 image with 5 animals\n# \u2193\n# Tiles: 12 tiles of 800x800\n# - Tile (0,0): 1 animal\n# - Tile (1,0): 2 animals\n# - Tile (0,1): 1 animal\n# - etc.\n</code></pre>"},{"location":"architecture/data-flow/#3-augmentation","title":"3. Augmentation","text":"<p>Create variations for training:</p> <pre><code># Original image\n# \u2193\n# Augmented versions:\n# - Rotated +15\u00b0\n# - Rotated -15\u00b0\n# - Brightness adjusted\n# - etc.\n</code></pre>"},{"location":"architecture/data-flow/#export-for-training","title":"Export for Training","text":"<p>Convert to framework-specific format:</p> <p>YOLO Format: <pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 val/\n\u251c\u2500\u2500 labels/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 val/\n\u2514\u2500\u2500 data.yaml\n</code></pre></p> <p>COCO Format: <pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u251c\u2500\u2500 train.json\n\u2514\u2500\u2500 val.json\n</code></pre></p>"},{"location":"architecture/data-flow/#stage-3-model-training-wildtrain","title":"Stage 3: Model Training (WildTrain)","text":""},{"location":"architecture/data-flow/#data-loading","title":"Data Loading","text":"<pre><code># DataLoader creates batches\nfor batch in dataloader:\n    images, targets = batch\n    # images: tensor [B, 3, H, W]\n    # targets: list of dicts with 'boxes', 'labels'\n</code></pre>"},{"location":"architecture/data-flow/#training-loop","title":"Training Loop","text":"<pre><code>flowchart LR\n    A[Load Batch] --&gt; B[Forward Pass]\n    B --&gt; C[Calculate Loss]\n    C --&gt; D[Backpropagation]\n    D --&gt; E[Update Weights]\n    E --&gt; F{Epoch End?}\n    F --&gt;|No| A\n    F --&gt;|Yes| G[Validation]\n    G --&gt; H[Log Metrics]\n    H --&gt; I{Training Done?}\n    I --&gt;|No| A\n    I --&gt;|Yes| J[Save Model]</code></pre>"},{"location":"architecture/data-flow/#model-versioning","title":"Model Versioning","text":"<pre><code># Training produces:\n1. Model weights: model.pt\n2. Training metrics: logged to MLflow\n3. Model artifacts: configs, preprocessing params\n4. Model metadata: framework, version, dataset\n\n# Registered to MLflow:\nmodels:/detector_name/version\n</code></pre>"},{"location":"architecture/data-flow/#data-flow-in-training","title":"Data Flow in Training","text":"<pre><code># Epoch 1:\ntrain_images \u2192 model \u2192 predictions \u2192 loss \u2192 optimizer \u2192 updated_model\nval_images \u2192 updated_model \u2192 predictions \u2192 metrics \u2192 log\n\n# Epoch 2:\ntrain_images \u2192 updated_model \u2192 predictions \u2192 loss \u2192 optimizer \u2192 updated_model\nval_images \u2192 updated_model \u2192 predictions \u2192 metrics \u2192 log\n\n# ...\n\n# Epoch N:\ntrain_images \u2192 final_model \u2192 predictions \u2192 loss \u2192 optimizer \u2192 best_model\nval_images \u2192 best_model \u2192 predictions \u2192 metrics \u2192 save\n</code></pre>"},{"location":"architecture/data-flow/#stage-4-deployment-wilddetect","title":"Stage 4: Deployment (WildDetect)","text":""},{"location":"architecture/data-flow/#model-loading","title":"Model Loading","text":"<pre><code># Load from MLflow\nmodel = mlflow.pytorch.load_model(\"models:/detector/production\")\n\n# Or from file\nmodel = torch.load(\"detector.pt\")\n\n# Model ready for inference\n</code></pre>"},{"location":"architecture/data-flow/#detection-pipeline","title":"Detection Pipeline","text":"<pre><code>flowchart TB\n    A[Input Image] --&gt; B{Large Raster?}\n    B --&gt;|Yes| C[Tile Image]\n    B --&gt;|No| D[Preprocess]\n    C --&gt; E[Process Tiles]\n    E --&gt; F[Detect on Each Tile]\n    F --&gt; G[Stitch Results]\n    G --&gt; H[Apply NMS]\n    D --&gt; I[Detect]\n    I --&gt; H\n    H --&gt; J[Format Results]\n    J --&gt; K[Output Detections]</code></pre>"},{"location":"architecture/data-flow/#detection-output-format","title":"Detection Output Format","text":"<pre><code>{\n  \"image_path\": \"drone_001.jpg\",\n  \"image_size\": [1920, 1080],\n  \"processing_time\": 0.5,\n  \"detections\": [\n    {\n      \"class_name\": \"elephant\",\n      \"confidence\": 0.95,\n      \"bbox\": [100, 200, 150, 180],\n      \"bbox_normalized\": [0.052, 0.185, 0.078, 0.167]\n    },\n    {\n      \"class_name\": \"giraffe\",\n      \"confidence\": 0.89,\n      \"bbox\": [500, 300, 120, 200]\n    }\n  ],\n  \"metadata\": {\n    \"model_name\": \"detector_v1\",\n    \"model_version\": \"3\",\n    \"timestamp\": \"2024-01-01T12:00:00\"\n  }\n}\n</code></pre>"},{"location":"architecture/data-flow/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple images\nimages = [\"img1.jpg\", \"img2.jpg\", \"img3.jpg\"]\n\n# Parallel detection\nresults = []\nfor image in images:\n    result = pipeline.detect(image)\n    results.append(result)\n\n# Save all results\nsave_results(results, \"batch_results.json\")\n</code></pre>"},{"location":"architecture/data-flow/#stage-5-analysis","title":"Stage 5: Analysis","text":""},{"location":"architecture/data-flow/#census-statistics","title":"Census Statistics","text":"<p>Aggregate detections across campaign:</p> <pre><code># Input: All detection results\ntotal_detections = 1523 animals\n\n# Statistics by species:\n{\n  \"elephant\": {\n    \"count\": 423,\n    \"percentage\": 27.8,\n    \"avg_confidence\": 0.93\n  },\n  \"giraffe\": {\n    \"count\": 612,\n    \"percentage\": 40.2,\n    \"avg_confidence\": 0.89\n  },\n  \"zebra\": {\n    \"count\": 488,\n    \"percentage\": 32.0,\n    \"avg_confidence\": 0.91\n  }\n}\n\n# Density analysis:\nsurvey_area = 25 km\u00b2\ndensity = {\n  \"elephant\": 16.9 per km\u00b2,\n  \"giraffe\": 24.5 per km\u00b2,\n  \"zebra\": 19.5 per km\u00b2\n}\n</code></pre>"},{"location":"architecture/data-flow/#geographic-analysis","title":"Geographic Analysis","text":"<pre><code># Extract GPS coordinates from images\nimage_locations = [\n  (lat1, lon1),  # Image 1 location\n  (lat2, lon2),  # Image 2 location\n  ...\n]\n\n# Map detections to geographic space\ndetection_map = {\n  (lat1, lon1): [\"elephant\", \"giraffe\"],\n  (lat2, lon2): [\"zebra\", \"elephant\", \"elephant\"],\n  ...\n}\n\n# Analyze distribution\nhotspots = identify_hotspots(detection_map)\ncoverage = calculate_coverage(image_locations)\n</code></pre>"},{"location":"architecture/data-flow/#visualization-pipeline","title":"Visualization Pipeline","text":"<pre><code>flowchart LR\n    A[Detections] --&gt; B[Statistics]\n    A --&gt; C[Geographic Data]\n    B --&gt; D[Charts &amp; Graphs]\n    C --&gt; E[Maps]\n    D --&gt; F[Report]\n    E --&gt; F\n    A --&gt; G[FiftyOne]\n    G --&gt; H[Interactive Viewer]</code></pre>"},{"location":"architecture/data-flow/#data-storage-and-persistence","title":"Data Storage and Persistence","text":""},{"location":"architecture/data-flow/#directory-structure","title":"Directory Structure","text":"<pre><code>project/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/                    # Original images\n\u2502   \u251c\u2500\u2500 annotations/            # Original annotations\n\u2502   \u2514\u2500\u2500 datasets/               # Processed datasets\n\u2502       \u2514\u2500\u2500 my_dataset/\n\u2502           \u251c\u2500\u2500 images/\n\u2502           \u2502   \u251c\u2500\u2500 train/\n\u2502           \u2502   \u2514\u2500\u2500 val/\n\u2502           \u2514\u2500\u2500 annotations/\n\u2502               \u251c\u2500\u2500 train.json  # Master format\n\u2502               \u2514\u2500\u2500 val.json\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 checkpoints/            # Training checkpoints\n\u2502   \u2514\u2500\u2500 trained/                # Final models\n\u251c\u2500\u2500 results/\n\u2502   \u251c\u2500\u2500 detections/             # Detection outputs\n\u2502   \u251c\u2500\u2500 census/                 # Census reports\n\u2502   \u2514\u2500\u2500 visualizations/         # Maps, charts\n\u2514\u2500\u2500 mlruns/                     # MLflow tracking data\n</code></pre>"},{"location":"architecture/data-flow/#data-versioning-with-dvc","title":"Data Versioning with DVC","text":"<pre><code># Track data with DVC\ndvc add data/datasets/my_dataset\n\n# Creates .dvc file\ndata/datasets/my_dataset.dvc\n\n# Push to remote storage\ndvc push\n\n# On another machine\ndvc pull\n</code></pre>"},{"location":"architecture/data-flow/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/data-flow/#data-loading-optimization","title":"Data Loading Optimization","text":"<pre><code># Efficient data loading\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    num_workers=4,      # Parallel loading (threads on Windows)\n    pin_memory=True,    # Faster GPU transfer\n    prefetch_factor=2   # Prefetch batches\n)\n</code></pre>"},{"location":"architecture/data-flow/#memory-management","title":"Memory Management","text":"<pre><code># For large images\nwith rasterio.open(large_image) as src:\n    # Process in windows\n    for window in tile_windows:\n        tile = src.read(window=window)\n        process(tile)\n        del tile  # Free memory\n</code></pre>"},{"location":"architecture/data-flow/#caching","title":"Caching","text":"<pre><code># Cache loaded models\n@lru_cache(maxsize=1)\ndef load_model(model_path):\n    return torch.load(model_path)\n\n# Cache detection results\nresults_cache = {}\nif image_hash in results_cache:\n    return results_cache[image_hash]\n</code></pre>"},{"location":"architecture/data-flow/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"architecture/data-flow/#validation-checkpoints","title":"Validation Checkpoints","text":"<pre><code>flowchart TB\n    A[Input Data] --&gt; B{Valid Format?}\n    B --&gt;|No| C[Error Report]\n    B --&gt;|Yes| D{Images Exist?}\n    D --&gt;|No| C\n    D --&gt;|Yes| E{Bboxes Valid?}\n    E --&gt;|No| C\n    E --&gt;|Yes| F[Process Data]</code></pre>"},{"location":"architecture/data-flow/#recovery-mechanisms","title":"Recovery Mechanisms","text":"<pre><code># Checkpoint-based recovery\nfor i, image in enumerate(images):\n    try:\n        result = detect(image)\n        save_checkpoint(i, result)\n    except Exception as e:\n        logger.error(f\"Failed on image {i}: {e}\")\n        if should_continue:\n            continue\n        else:\n            # Resume from last checkpoint\n            resume_from_checkpoint(i)\n</code></pre>"},{"location":"architecture/data-flow/#integration-points","title":"Integration Points","text":""},{"location":"architecture/data-flow/#wildata-wildtrain","title":"WilData \u2194 WildTrain","text":"<pre><code># WilData exports dataset\nwildata.export_dataset(\"my_dataset\", format=\"yolo\", output=\"data/yolo\")\n\n# WildTrain loads dataset\ndatamodule = DataModule(data_root=\"data/yolo\")\ntrainer.fit(model, datamodule)\n</code></pre>"},{"location":"architecture/data-flow/#wildtrain-wilddetect","title":"WildTrain \u2194 WildDetect","text":"<pre><code># WildTrain registers model\nmlflow.pytorch.log_model(model, \"model\")\nmlflow.register_model(\"runs:/.../model\", \"detector\")\n\n# WildDetect loads model\npipeline = DetectionPipeline(mlflow_model_name=\"detector\")\n</code></pre>"},{"location":"architecture/data-flow/#wilddetect-fiftyone","title":"WildDetect \u2194 FiftyOne","text":"<pre><code># WildDetect creates FiftyOne dataset\nfo_dataset = create_fiftyone_dataset(detections)\n\n# Launch viewer\nsession = fo.launch_app(fo_dataset)\n</code></pre>"},{"location":"architecture/data-flow/#example-complete-workflow","title":"Example: Complete Workflow","text":"<pre><code># 1. Import annotations (WilData)\nfrom wildata import DataPipeline\n\npipeline = DataPipeline(\"data\")\npipeline.import_dataset(\n    source_path=\"annotations.json\",\n    source_format=\"coco\",\n    dataset_name=\"training_data\",\n    transformations={\"enable_tiling\": True}\n)\n\n# 2. Train model (WildTrain)\nfrom wildtrain import Trainer\n\ntrainer = Trainer.from_config(\"configs/yolo.yaml\")\nmodel = trainer.train()\nmodel_uri = trainer.register_model(\"wildlife_detector\")\n\n# 3. Run detection (WildDetect)\nfrom wildetect import DetectionPipeline\n\ndetector = DetectionPipeline(mlflow_model_uri=model_uri)\nresults = detector.detect_batch(\"survey_images/\")\n\n# 4. Analyze results (WildDetect)\nfrom wildetect import CensusEngine\n\ncensus = CensusEngine.from_detections(results)\ncensus.generate_report(\"census_report.pdf\")\n</code></pre>"},{"location":"architecture/data-flow/#next-steps","title":"Next Steps","text":"<ul> <li>WilData Architecture \u2192</li> <li>WildTrain Architecture \u2192</li> <li>WildDetect Architecture \u2192</li> <li>End-to-End Tutorial \u2192</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>The WildDetect monorepo is designed as a modular ecosystem of three interconnected packages, each serving a specific purpose in the wildlife detection and analysis pipeline.</p>"},{"location":"architecture/overview/#monorepo-structure","title":"Monorepo Structure","text":"<pre><code>wildetect/                 # Monorepo root\n\u251c\u2500\u2500 wildata/              # \ud83d\udce6 Data management package\n\u251c\u2500\u2500 wildtrain/            # \ud83c\udf93 Model training package  \n\u251c\u2500\u2500 src/wildetect/        # \ud83d\udd0d Detection and analysis package\n\u251c\u2500\u2500 config/               # Shared configurations\n\u251c\u2500\u2500 scripts/              # Batch scripts\n\u251c\u2500\u2500 docs/                 # Documentation\n\u2514\u2500\u2500 mkdocs.yml           # Documentation config\n</code></pre>"},{"location":"architecture/overview/#package-relationships","title":"Package Relationships","text":"<p>The three packages have a clear dependency hierarchy:</p> <pre><code>graph TD\n    A[WilData&lt;br/&gt;Data Pipeline] --&gt; B[WildTrain&lt;br/&gt;Model Training]\n    B --&gt; C[WildDetect&lt;br/&gt;Detection &amp; Analysis]\n    A -.optional.-&gt; C\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#e8f5e9</code></pre>"},{"location":"architecture/overview/#dependency-flow","title":"Dependency Flow","text":"<ol> <li>WilData (Foundation)</li> <li>Standalone package</li> <li>No dependencies on other packages</li> <li> <p>Provides data management primitives</p> </li> <li> <p>WildTrain (Training)</p> </li> <li>Depends on WilData for dataset loading</li> <li>Can be used independently for model training</li> <li> <p>Outputs models for WildDetect</p> </li> <li> <p>WildDetect (Application)</p> </li> <li>Depends on WildTrain for model structures</li> <li>Optionally uses WilData for data handling</li> <li>Top-level application package</li> </ol>"},{"location":"architecture/overview/#core-principles","title":"Core Principles","text":""},{"location":"architecture/overview/#1-separation-of-concerns","title":"1. Separation of Concerns","text":"<p>Each package has a single, well-defined responsibility:</p> <ul> <li>WilData: \"How do I manage and transform data?\"</li> <li>WildTrain: \"How do I train and evaluate models?\"</li> <li>WildDetect: \"How do I detect wildlife and analyze results?\"</li> </ul>"},{"location":"architecture/overview/#2-modularity","title":"2. Modularity","text":"<p>Packages can be used independently:</p> <pre><code># Use WilData alone for data management\nfrom wildata import DataPipeline\n\n# Use WildTrain alone for training\nfrom wildtrain import Trainer\n\n# Use WildDetect for detection\nfrom wildetect import DetectionPipeline\n</code></pre>"},{"location":"architecture/overview/#3-configuration-driven","title":"3. Configuration-Driven","text":"<p>All behavior is configurable via YAML files:</p> <pre><code># Each package has its own configs\nwildetect/config/         # Detection configs\nwildata/configs/          # Data configs\nwildtrain/configs/        # Training configs\n</code></pre>"},{"location":"architecture/overview/#4-clean-architecture","title":"4. Clean Architecture","text":"<p>Each package follows clean architecture principles:</p> <pre><code>src/package/\n\u251c\u2500\u2500 core/          # Business logic (domain)\n\u251c\u2500\u2500 adapters/      # External interfaces\n\u251c\u2500\u2500 cli/           # Command-line interface\n\u251c\u2500\u2500 api/           # REST API (if applicable)\n\u2514\u2500\u2500 ui/            # User interface (if applicable)\n</code></pre>"},{"location":"architecture/overview/#package-overview","title":"Package Overview","text":""},{"location":"architecture/overview/#wildata-data-management","title":"\ud83d\udce6 WilData - Data Management","text":"<p>Purpose: Unified data pipeline for object detection datasets</p> <p>Key Features: - Multi-format import/export (COCO, YOLO, Label Studio) - Data transformations (tiling, augmentation, clipping) - ROI dataset creation - DVC integration for versioning - REST API for remote operations</p> <p>Use Cases: - Import annotations from labeling tools - Prepare datasets for training - Create ROI datasets for hard sample mining - Version control large datasets</p> <p>Learn more \u2192</p>"},{"location":"architecture/overview/#wildtrain-model-training","title":"\ud83c\udf93 WildTrain - Model Training","text":"<p>Purpose: Modular training framework for detection and classification</p> <p>Key Features: - Multiple frameworks (YOLO, MMDetection, PyTorch Lightning) - Hydra configuration management - MLflow experiment tracking - Hyperparameter optimization (Optuna) - Model registration and versioning</p> <p>Use Cases: - Train custom detection models - Train classification models - Hyperparameter tuning - Model evaluation and comparison - Export models for deployment</p> <p>Learn more \u2192</p>"},{"location":"architecture/overview/#wilddetect-detection-analysis","title":"\ud83d\udd0d WildDetect - Detection &amp; Analysis","text":"<p>Purpose: Production detection system with census capabilities</p> <p>Key Features: - Multi-threaded detection pipelines - Large raster image support - Census campaign orchestration - Geographic analysis and visualization - FiftyOne integration - Comprehensive reporting</p> <p>Use Cases: - Run detection on aerial imagery - Conduct wildlife census campaigns - Generate population statistics - Create geographic visualizations - Export results for analysis</p> <p>Learn more \u2192</p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#complete-workflow","title":"Complete Workflow","text":"<pre><code>graph TB\n    subgraph \"1. Data Preparation (WilData)\"\n        A[Raw Annotations&lt;br/&gt;COCO/YOLO/LS] --&gt; B[WilData Import]\n        B --&gt; C[Transformations&lt;br/&gt;Tile/Augment]\n        C --&gt; D[Processed Dataset]\n    end\n\n    subgraph \"2. Model Training (WildTrain)\"\n        D --&gt; E[WildTrain]\n        E --&gt; F[Training Loop]\n        F --&gt; G[Trained Model]\n        G --&gt; H[MLflow Registry]\n    end\n\n    subgraph \"3. Deployment (WildDetect)\"\n        H --&gt; I[WildDetect]\n        J[Aerial Images] --&gt; I\n        I --&gt; K[Detections]\n        K --&gt; L[Analysis &amp; Reports]\n    end\n\n    style A fill:#e3f2fd\n    style D fill:#e3f2fd\n    style G fill:#fff3e0\n    style L fill:#e8f5e9</code></pre>"},{"location":"architecture/overview/#example-complete-pipeline","title":"Example: Complete Pipeline","text":"<pre><code># 1. WilData: Prepare dataset\nfrom wildata import DataPipeline\n\npipeline = DataPipeline(\"data\")\npipeline.import_dataset(\n    source_path=\"annotations.json\",\n    source_format=\"coco\",\n    dataset_name=\"training_data\"\n)\n\n# 2. WildTrain: Train model\nfrom wildtrain import Trainer\n\ntrainer = Trainer(config=\"configs/yolo.yaml\")\nmodel = trainer.train()\nmodel_uri = trainer.register_model(\"detector_v1\")\n\n# 3. WildDetect: Run detection\nfrom wildetect import DetectionPipeline\n\ndetector = DetectionPipeline(model_uri=model_uri)\nresults = detector.detect_batch(\"aerial_images/\")\ndetector.generate_report(results, \"census_report.json\")\n</code></pre> <p>See detailed data flow \u2192</p>"},{"location":"architecture/overview/#technology-stack","title":"Technology Stack","text":""},{"location":"architecture/overview/#core-technologies","title":"Core Technologies","text":"Component Technology Purpose Language Python 3.9+ Primary language CLI Typer Command-line interfaces Config Hydra/OmegaConf Configuration management Detection YOLO, MMDetection Object detection Training PyTorch Lightning Model training API FastAPI REST API (WilData) UI Streamlit Web interfaces Visualization FiftyOne Dataset visualization Tracking MLflow Experiment tracking Versioning DVC Data versioning"},{"location":"architecture/overview/#key-libraries","title":"Key Libraries","text":""},{"location":"architecture/overview/#data-processing","title":"Data Processing","text":"<ul> <li>Pillow: Image processing</li> <li>OpenCV: Computer vision operations</li> <li>Rasterio: Geospatial raster data</li> <li>Pandas: Tabular data manipulation</li> <li>Albumentations: Data augmentation</li> </ul>"},{"location":"architecture/overview/#machine-learning","title":"Machine Learning","text":"<ul> <li>PyTorch: Deep learning framework</li> <li>Ultralytics: YOLO implementation</li> <li>MMDetection: Detection framework</li> <li>Torchvision: Vision utilities</li> </ul>"},{"location":"architecture/overview/#utilities","title":"Utilities","text":"<ul> <li>Pydantic: Data validation</li> <li>Rich: Terminal formatting</li> <li>TQDM: Progress bars</li> <li>PyYAML: YAML parsing</li> </ul>"},{"location":"architecture/overview/#design-patterns","title":"Design Patterns","text":""},{"location":"architecture/overview/#1-factory-pattern","title":"1. Factory Pattern","text":"<p>Used for creating detectors, trainers, and datasets:</p> <pre><code># Detector factory\ndetector = DetectorFactory.create(\n    framework=\"yolo\",\n    model_path=\"model.pt\"\n)\n\n# Dataset factory\ndataset = DatasetFactory.create(\n    format=\"coco\",\n    path=\"annotations.json\"\n)\n</code></pre>"},{"location":"architecture/overview/#2-strategy-pattern","title":"2. Strategy Pattern","text":"<p>Used for different processing strategies:</p> <pre><code># Different detection strategies\npipeline = DetectionPipeline(\n    strategy=\"raster\"  # or \"simple\", \"multithreaded\", etc.\n)\n</code></pre>"},{"location":"architecture/overview/#3-adapter-pattern","title":"3. Adapter Pattern","text":"<p>Used for format conversions:</p> <pre><code># COCO to YOLO adapter\ncoco_data = COCODataset(path)\nyolo_adapter = YOLOAdapter(coco_data)\nyolo_data = yolo_adapter.convert()\n</code></pre>"},{"location":"architecture/overview/#4-pipeline-pattern","title":"4. Pipeline Pattern","text":"<p>Used for data transformations:</p> <pre><code># Transformation pipeline\npipeline = TransformationPipeline([\n    BBoxClippingTransform(),\n    TilingTransform(tile_size=800),\n    AugmentationTransform()\n])\ntransformed = pipeline.apply(dataset)\n</code></pre>"},{"location":"architecture/overview/#configuration-management","title":"Configuration Management","text":""},{"location":"architecture/overview/#hierarchical-configuration","title":"Hierarchical Configuration","text":"<p>Each package uses a hierarchical configuration system:</p> <pre><code># configs/main.yaml\ndefaults:\n  - model: yolo\n  - data: coco\n  - training: default\n\n# Override with CLI\npython main.py model=custom data.batch_size=64\n</code></pre>"},{"location":"architecture/overview/#configuration-sources","title":"Configuration Sources","text":"<ol> <li>Default configs: Sensible defaults in code</li> <li>YAML files: User configurations</li> <li>Environment variables: <code>.env</code> files</li> <li>CLI arguments: Command-line overrides</li> </ol> <p>Priority: CLI &gt; Env Vars &gt; YAML &gt; Defaults</p>"},{"location":"architecture/overview/#error-handling","title":"Error Handling","text":""},{"location":"architecture/overview/#centralized-error-management","title":"Centralized Error Management","text":"<pre><code>from wildetect.core.exceptions import (\n    DetectionError,\n    ModelLoadError,\n    ConfigurationError\n)\n\ntry:\n    detector.detect(image)\nexcept ModelLoadError as e:\n    logger.error(f\"Failed to load model: {e}\")\nexcept DetectionError as e:\n    logger.error(f\"Detection failed: {e}\")\n</code></pre>"},{"location":"architecture/overview/#validation","title":"Validation","text":"<p>All inputs are validated using Pydantic:</p> <pre><code>from pydantic import BaseModel, validator\n\nclass DetectionConfig(BaseModel):\n    batch_size: int\n    tile_size: int\n\n    @validator('batch_size')\n    def validate_batch_size(cls, v):\n        if v &lt;= 0:\n            raise ValueError(\"batch_size must be positive\")\n        return v\n</code></pre>"},{"location":"architecture/overview/#logging-and-monitoring","title":"Logging and Monitoring","text":""},{"location":"architecture/overview/#structured-logging","title":"Structured Logging","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\nlogger.info(\"Processing image\", extra={\n    \"image_path\": path,\n    \"tile_size\": 800,\n    \"batch_size\": 32\n})\n</code></pre>"},{"location":"architecture/overview/#experiment-tracking","title":"Experiment Tracking","text":"<pre><code>import mlflow\n\nwith mlflow.start_run():\n    mlflow.log_params(config)\n    mlflow.log_metrics({\"accuracy\": 0.95})\n    mlflow.log_artifact(\"model.pt\")\n</code></pre>"},{"location":"architecture/overview/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/overview/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/              # Unit tests\n\u2502   \u251c\u2500\u2500 test_core/\n\u2502   \u2514\u2500\u2500 test_adapters/\n\u251c\u2500\u2500 integration/       # Integration tests\n\u2502   \u2514\u2500\u2500 test_pipelines/\n\u2514\u2500\u2500 e2e/              # End-to-end tests\n    \u2514\u2500\u2500 test_workflows/\n</code></pre>"},{"location":"architecture/overview/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest tests/ -v\n\n# Run specific package tests\nuv run pytest tests/test_detection_pipeline.py -v\n\n# With coverage\nuv run pytest --cov=wildetect tests/\n</code></pre>"},{"location":"architecture/overview/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/overview/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li>Multi-threading: For I/O-bound operations (Windows-compatible)</li> <li>Batch Processing: Process multiple images together</li> <li>Caching: Cache loaded models and configurations</li> <li>Memory Management: Efficient image loading and cleanup</li> <li>GPU Utilization: Maximize GPU usage with appropriate batch sizes</li> </ol>"},{"location":"architecture/overview/#scalability","title":"Scalability","text":"<ul> <li>Horizontal: Process multiple images in parallel</li> <li>Vertical: Use larger models and batch sizes</li> <li>Distributed: Deploy inference servers for remote processing</li> </ul>"},{"location":"architecture/overview/#security-considerations","title":"Security Considerations","text":"<ul> <li>API Keys: Stored in <code>.env</code>, never in code</li> <li>File Paths: Validated before processing</li> <li>Input Validation: All inputs validated with Pydantic</li> <li>Dependency Management: Regular security updates</li> </ul>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<p>Explore individual package architectures:</p> <ul> <li>WilData Architecture \u2192</li> <li>WildTrain Architecture \u2192</li> <li>WildDetect Architecture \u2192</li> <li>Data Flow Details \u2192</li> </ul> <p>Or dive into specific topics:</p> <ul> <li>Scripts Reference</li> <li>Configuration Reference</li> <li>API Documentation</li> </ul>"},{"location":"architecture/wildata/","title":"WilData Architecture","text":"<p>WilData is the data management foundation of the WildDetect ecosystem, providing a unified pipeline for importing, transforming, and exporting object detection datasets.</p>"},{"location":"architecture/wildata/#overview","title":"Overview","text":"<p>Purpose: Unified data pipeline and management system for computer vision datasets</p> <p>Key Responsibilities: - Multi-format dataset import/export - Data transformations and augmentation - ROI dataset creation - DVC integration for versioning - REST API for programmatic access</p>"},{"location":"architecture/wildata/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TB\n    subgraph \"Input Layer\"\n        A[COCO Format]\n        B[YOLO Format]\n        C[Label Studio]\n    end\n\n    subgraph \"Core Pipeline\"\n        D[Format Adapters]\n        E[Master Format]\n        F[Transformation Pipeline]\n        G[Validation Layer]\n    end\n\n    subgraph \"Storage Layer\"\n        H[File System]\n        I[DVC Storage]\n    end\n\n    subgraph \"Output Layer\"\n        J[COCO Export]\n        K[YOLO Export]\n        L[ROI Dataset]\n    end\n\n    subgraph \"API Layer\"\n        M[REST API]\n        N[CLI]\n        O[Python API]\n    end\n\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    G --&gt; I\n    E --&gt; J\n    E --&gt; K\n    E --&gt; L\n\n    M --&gt; D\n    N --&gt; D\n    O --&gt; D\n\n    style E fill:#e1f5ff\n    style F fill:#fff4e1\n    style H fill:#e8f5e9</code></pre>"},{"location":"architecture/wildata/#core-components","title":"Core Components","text":""},{"location":"architecture/wildata/#1-format-adapters","title":"1. Format Adapters","text":"<p>Convert between different annotation formats.</p>"},{"location":"architecture/wildata/#coco-adapter","title":"COCO Adapter","text":"<pre><code># src/wildata/adapters/coco_adapter.py\nclass COCOAdapter:\n    \"\"\"Adapter for COCO format datasets.\"\"\"\n\n    def load(self, path: Path) -&gt; MasterDataset:\n        \"\"\"Load COCO annotations into master format.\"\"\"\n\n    def save(self, dataset: MasterDataset, path: Path):\n        \"\"\"Export master format to COCO.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#yolo-adapter","title":"YOLO Adapter","text":"<pre><code># src/wildata/adapters/yolo_adapter.py\nclass YOLOAdapter:\n    \"\"\"Adapter for YOLO format datasets.\"\"\"\n\n    def load(self, path: Path) -&gt; MasterDataset:\n        \"\"\"Load YOLO annotations into master format.\"\"\"\n\n    def save(self, dataset: MasterDataset, path: Path):\n        \"\"\"Export master format to YOLO.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#label-studio-adapter","title":"Label Studio Adapter","text":"<pre><code># src/wildata/adapters/ls_adapter.py\nclass LabelStudioAdapter:\n    \"\"\"Adapter for Label Studio export format.\"\"\"\n\n    def load(self, path: Path) -&gt; MasterDataset:\n        \"\"\"Load Label Studio annotations.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#2-master-format","title":"2. Master Format","text":"<p>Internal unified representation for all datasets.</p> <pre><code># src/wildata/core/master_format.py\n@dataclass\nclass MasterDataset:\n    \"\"\"Unified dataset representation.\"\"\"\n    images: List[ImageInfo]\n    annotations: List[Annotation]\n    categories: List[Category]\n    metadata: Dict[str, Any]\n\n@dataclass\nclass ImageInfo:\n    id: int\n    file_name: str\n    width: int\n    height: int\n    path: Path\n    metadata: Optional[Dict] = None\n\n@dataclass\nclass Annotation:\n    id: int\n    image_id: int\n    category_id: int\n    bbox: List[float]  # [x, y, width, height]\n    area: float\n    segmentation: Optional[List] = None\n</code></pre>"},{"location":"architecture/wildata/#3-transformation-pipeline","title":"3. Transformation Pipeline","text":"<p>Apply transformations to datasets.</p>"},{"location":"architecture/wildata/#bbox-clipping","title":"Bbox Clipping","text":"<pre><code># src/wildata/transforms/bbox_clipping.py\nclass BBoxClippingTransform:\n    \"\"\"Clip bounding boxes to image boundaries.\"\"\"\n\n    def __init__(self, tolerance: int = 5, skip_invalid: bool = False):\n        self.tolerance = tolerance\n        self.skip_invalid = skip_invalid\n\n    def apply(self, dataset: MasterDataset) -&gt; MasterDataset:\n        \"\"\"Apply clipping to all bboxes.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#tiling-transform","title":"Tiling Transform","text":"<pre><code># src/wildata/transforms/tiling.py\nclass TilingTransform:\n    \"\"\"Tile large images into smaller patches.\"\"\"\n\n    def __init__(\n        self,\n        tile_size: int = 512,\n        stride: int = 416,\n        min_visibility: float = 0.1\n    ):\n        self.tile_size = tile_size\n        self.stride = stride\n        self.min_visibility = min_visibility\n\n    def apply(self, dataset: MasterDataset) -&gt; MasterDataset:\n        \"\"\"Tile all images in dataset.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#augmentation-transform","title":"Augmentation Transform","text":"<pre><code># src/wildata/transforms/augmentation.py\nclass AugmentationTransform:\n    \"\"\"Apply data augmentation.\"\"\"\n\n    def __init__(\n        self,\n        rotation_range: Tuple[float, float] = (-45, 45),\n        probability: float = 1.0,\n        num_transforms: int = 2\n    ):\n        self.rotation_range = rotation_range\n        self.probability = probability\n        self.num_transforms = num_transforms\n\n    def apply(self, dataset: MasterDataset) -&gt; MasterDataset:\n        \"\"\"Augment dataset.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#4-roi-adapter","title":"4. ROI Adapter","text":"<p>Extract regions of interest for classification datasets.</p> <pre><code># src/wildata/adapters/roi_adapter.py\nclass ROIAdapter:\n    \"\"\"Convert detection datasets to ROI classification datasets.\"\"\"\n\n    def __init__(\n        self,\n        roi_box_size: int = 128,\n        min_roi_size: int = 32,\n        random_roi_count: int = 10,\n        background_class: str = \"background\"\n    ):\n        self.roi_box_size = roi_box_size\n        self.min_roi_size = min_roi_size\n        self.random_roi_count = random_roi_count\n        self.background_class = background_class\n\n    def convert(self, coco_data: dict) -&gt; ROIDataset:\n        \"\"\"Convert COCO dataset to ROI dataset.\"\"\"\n        # Extract ROIs from bboxes\n        # Generate background samples\n        # Create classification dataset\n</code></pre> <p>Use Cases: - Hard sample mining - Error analysis - Training ROI-based classifiers - Creating balanced classification datasets</p>"},{"location":"architecture/wildata/#5-data-pipeline","title":"5. Data Pipeline","text":"<p>Main orchestrator for data operations.</p> <pre><code># src/wildata/pipeline/data_pipeline.py\nclass DataPipeline:\n    \"\"\"Main data pipeline orchestrator.\"\"\"\n\n    def __init__(self, root: str = \"data\", enable_dvc: bool = False):\n        self.root = Path(root)\n        self.enable_dvc = enable_dvc\n        self.dvc_manager = DVCManager() if enable_dvc else None\n\n    def import_dataset(\n        self,\n        source_path: str,\n        source_format: str,\n        dataset_name: str,\n        transformations: Optional[TransformConfig] = None,\n        track_with_dvc: bool = False\n    ) -&gt; ImportResult:\n        \"\"\"Import dataset with optional transformations.\"\"\"\n        # 1. Load using appropriate adapter\n        # 2. Validate data\n        # 3. Apply transformations\n        # 4. Save to master format\n        # 5. Track with DVC if enabled\n\n    def export_dataset(\n        self,\n        dataset_name: str,\n        target_format: str,\n        output_path: Optional[str] = None\n    ) -&gt; ExportResult:\n        \"\"\"Export dataset to target format.\"\"\"\n\n    def list_datasets(self) -&gt; List[DatasetInfo]:\n        \"\"\"List all available datasets.\"\"\"\n\n    def get_dataset_info(self, dataset_name: str) -&gt; DatasetInfo:\n        \"\"\"Get detailed dataset information.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#6-dvc-manager","title":"6. DVC Manager","text":"<p>Handle data versioning with DVC.</p> <pre><code># src/wildata/pipeline/dvc_manager.py\nclass DVCManager:\n    \"\"\"DVC integration for data versioning.\"\"\"\n\n    def setup(self, storage_type: DVCStorageType, storage_path: str):\n        \"\"\"Initialize DVC remote.\"\"\"\n\n    def track(self, path: Path) -&gt; bool:\n        \"\"\"Add path to DVC tracking.\"\"\"\n\n    def push(self) -&gt; bool:\n        \"\"\"Push data to remote.\"\"\"\n\n    def pull(self, dataset_name: Optional[str] = None) -&gt; bool:\n        \"\"\"Pull data from remote.\"\"\"\n\n    def status(self) -&gt; DVCStatus:\n        \"\"\"Get DVC status.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#rest-api","title":"REST API","text":"<p>FastAPI-based API for remote operations.</p>"},{"location":"architecture/wildata/#api-structure","title":"API Structure","text":"<pre><code># src/wildata/api/main.py\napp = FastAPI(title=\"WilData API\")\n\n# Routers\napp.include_router(datasets_router, prefix=\"/api/v1/datasets\")\napp.include_router(roi_router, prefix=\"/api/v1/roi\")\napp.include_router(gps_router, prefix=\"/api/v1/gps\")\napp.include_router(jobs_router, prefix=\"/api/v1/jobs\")\napp.include_router(health_router, prefix=\"/api/v1/health\")\n</code></pre>"},{"location":"architecture/wildata/#background-jobs","title":"Background Jobs","text":"<p>Long-running operations handled asynchronously:</p> <pre><code># src/wildata/api/services/job_queue.py\nclass JobQueue:\n    \"\"\"Background job queue for async operations.\"\"\"\n\n    def submit(self, job_type: str, **kwargs) -&gt; str:\n        \"\"\"Submit job and return job_id.\"\"\"\n\n    def get_status(self, job_id: str) -&gt; JobStatus:\n        \"\"\"Get job status.\"\"\"\n\n    def cancel(self, job_id: str) -&gt; bool:\n        \"\"\"Cancel running job.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#cli-interface","title":"CLI Interface","text":"<p>Command-line interface built with Typer.</p> <pre><code># src/wildata/cli/main.py\napp = typer.Typer()\n\n@app.command()\ndef import_dataset(\n    source_path: str,\n    format: str = typer.Option(..., \"--format\", \"-f\"),\n    name: str = typer.Option(..., \"--name\", \"-n\"),\n    config: Optional[str] = typer.Option(None, \"--config\", \"-c\")\n):\n    \"\"\"Import dataset from source format.\"\"\"\n\n@app.command()\ndef export_dataset(\n    dataset_name: str,\n    format: str,\n    output: Optional[str] = None\n):\n    \"\"\"Export dataset to target format.\"\"\"\n\n@app.command()\ndef create_roi(\n    source_path: str,\n    config: str = typer.Option(..., \"--config\", \"-c\")\n):\n    \"\"\"Create ROI dataset.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#configuration-system","title":"Configuration System","text":""},{"location":"architecture/wildata/#import-configuration","title":"Import Configuration","text":"<pre><code># configs/import-config-example.yaml\nsource_path: \"annotations.json\"\nsource_format: \"coco\"  # coco, yolo, ls\ndataset_name: \"my_dataset\"\n\nroot: \"data\"\nsplit_name: \"train\"  # train, val, test\nprocessing_mode: \"batch\"  # streaming, batch\n\n# Transformations\ntransformations:\n  enable_bbox_clipping: true\n  bbox_clipping:\n    tolerance: 5\n    skip_invalid: false\n\n  enable_augmentation: false\n  augmentation:\n    rotation_range: [-45, 45]\n    probability: 1.0\n    num_transforms: 2\n\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n    min_visibility: 0.7\n    max_negative_tiles_in_negative_image: 2\n\n# ROI Configuration\nroi_config:\n  random_roi_count: 10\n  roi_box_size: 128\n  min_roi_size: 32\n  background_class: \"background\"\n  save_format: \"jpg\"\n  quality: 95\n</code></pre>"},{"location":"architecture/wildata/#data-storage","title":"Data Storage","text":""},{"location":"architecture/wildata/#directory-structure","title":"Directory Structure","text":"<pre><code>data/\n\u251c\u2500\u2500 datasets/\n\u2502   \u251c\u2500\u2500 dataset_name/\n\u2502   \u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 val/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 test/\n\u2502   \u2502   \u2514\u2500\u2500 annotations/\n\u2502   \u2502       \u251c\u2500\u2500 train.json        # Master format\n\u2502   \u2502       \u251c\u2500\u2500 val.json\n\u2502   \u2502       \u2514\u2500\u2500 test.json\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 exports/\n\u2502   \u251c\u2500\u2500 coco/\n\u2502   \u2514\u2500\u2500 yolo/\n\u2514\u2500\u2500 .dvc/                         # DVC metadata\n</code></pre>"},{"location":"architecture/wildata/#master-format-storage","title":"Master Format Storage","text":"<p>Datasets are stored in an extended COCO-like format:</p> <pre><code>{\n  \"info\": {\n    \"dataset_name\": \"my_dataset\",\n    \"created_at\": \"2024-01-01T00:00:00\",\n    \"source_format\": \"coco\",\n    \"transformations_applied\": [\"tiling\", \"clipping\"]\n  },\n  \"images\": [...],\n  \"annotations\": [...],\n  \"categories\": [...]\n}\n</code></pre>"},{"location":"architecture/wildata/#validation","title":"Validation","text":"<p>All data is validated at import:</p> <pre><code># src/wildata/core/validation.py\nclass DatasetValidator:\n    \"\"\"Validate dataset integrity.\"\"\"\n\n    def validate_coco(self, data: dict) -&gt; ValidationResult:\n        \"\"\"Validate COCO format.\"\"\"\n        # Check required fields\n        # Validate bbox coordinates\n        # Check image references\n        # Validate category IDs\n\n    def validate_yolo(self, data_yaml: Path) -&gt; ValidationResult:\n        \"\"\"Validate YOLO format.\"\"\"\n\n    def validate_master(self, dataset: MasterDataset) -&gt; ValidationResult:\n        \"\"\"Validate master format.\"\"\"\n</code></pre>"},{"location":"architecture/wildata/#performance-optimization","title":"Performance Optimization","text":""},{"location":"architecture/wildata/#streaming-mode","title":"Streaming Mode","text":"<p>For large datasets:</p> <pre><code># Process datasets in streaming mode\npipeline.import_dataset(\n    source_path=\"large_dataset.json\",\n    source_format=\"coco\",\n    dataset_name=\"large\",\n    processing_mode=\"streaming\"  # Process in chunks\n)\n</code></pre>"},{"location":"architecture/wildata/#parallel-processing","title":"Parallel Processing","text":"<p>Use threading for I/O-bound operations:</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    futures = [\n        executor.submit(process_image, img)\n        for img in images\n    ]\n</code></pre>"},{"location":"architecture/wildata/#use-cases","title":"Use Cases","text":""},{"location":"architecture/wildata/#1-import-and-transform","title":"1. Import and Transform","text":"<pre><code>from wildata import DataPipeline\n\npipeline = DataPipeline(\"data\")\n\n# Import with transformations\nresult = pipeline.import_dataset(\n    source_path=\"annotations.json\",\n    source_format=\"coco\",\n    dataset_name=\"processed\",\n    transformations={\n        \"enable_tiling\": True,\n        \"tiling\": {\"tile_size\": 800, \"stride\": 640}\n    }\n)\n</code></pre>"},{"location":"architecture/wildata/#2-create-roi-dataset","title":"2. Create ROI Dataset","text":"<pre><code>from wildata.adapters import ROIAdapter\n\n# Load COCO data\nwith open(\"annotations.json\") as f:\n    coco_data = json.load(f)\n\n# Convert to ROI dataset\nroi_adapter = ROIAdapter(\n    roi_box_size=128,\n    random_roi_count=10\n)\nroi_dataset = roi_adapter.convert(coco_data)\nroi_adapter.save(roi_dataset, output_dir=\"roi_dataset\")\n</code></pre>"},{"location":"architecture/wildata/#3-dvc-workflow","title":"3. DVC Workflow","text":"<pre><code># Setup DVC\nwildata dvc setup --storage-type s3 --storage-path s3://bucket/data\n\n# Import with tracking\nwildata import-dataset data.json --format coco --name ds --track-dvc\n\n# Push to remote\nwildata dvc push\n\n# On another machine\nwildata dvc pull ds\n</code></pre>"},{"location":"architecture/wildata/#next-steps","title":"Next Steps","text":"<ul> <li>WildTrain Architecture \u2192</li> <li>WildDetect Architecture \u2192</li> <li>Data Flow Details \u2192</li> <li>WilData CLI Reference \u2192</li> <li>WilData Scripts \u2192</li> </ul>"},{"location":"architecture/wildetect/","title":"WildDetect Architecture","text":"<p>WildDetect is the top-level application package that provides production-ready wildlife detection, census analysis, and geographic visualization capabilities.</p>"},{"location":"architecture/wildetect/#overview","title":"Overview","text":"<p>Purpose: Production detection system with census and analysis capabilities</p> <p>Key Responsibilities: - Wildlife detection on aerial imagery - Census campaign orchestration - Geographic analysis and visualization - Population statistics and reporting - Integration with FiftyOne and Label Studio</p>"},{"location":"architecture/wildetect/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TB\n    subgraph \"Input Layer\"\n        A[Aerial Images]\n        B[MLflow Models]\n        C[Configuration]\n    end\n\n    subgraph \"Detection Core\"\n        D[Model Loader]\n        E[Detection Pipeline]\n        F[Tiling Engine]\n        G[NMS &amp; Stitching]\n    end\n\n    subgraph \"Processing Strategies\"\n        H[Simple Pipeline]\n        I[Multi-threaded]\n        J[Raster Pipeline]\n    end\n\n    subgraph \"Analysis Layer\"\n        K[Census Engine]\n        L[Statistics]\n        M[Geographic Analysis]\n    end\n\n    subgraph \"Visualization Layer\"\n        N[FiftyOne Integration]\n        O[Geographic Maps]\n        P[Reports]\n    end\n\n    subgraph \"Output Layer\"\n        Q[Detections JSON/CSV]\n        R[Census Reports]\n        S[Visualizations]\n    end\n\n    A --&gt; E\n    B --&gt; D\n    C --&gt; E\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    E --&gt; H\n    E --&gt; I\n    E --&gt; J\n    G --&gt; K\n    K --&gt; L\n    K --&gt; M\n    L --&gt; P\n    M --&gt; O\n    K --&gt; N\n    G --&gt; Q\n    K --&gt; R\n    O --&gt; S\n\n    style E fill:#e1f5ff\n    style K fill:#fff4e1\n    style Q fill:#e8f5e9</code></pre>"},{"location":"architecture/wildetect/#core-components","title":"Core Components","text":""},{"location":"architecture/wildetect/#1-detection-pipelines","title":"1. Detection Pipelines","text":"<p>Multiple pipeline strategies for different use cases.</p>"},{"location":"architecture/wildetect/#base-pipeline-interface","title":"Base Pipeline Interface","text":"<pre><code># src/wildetect/core/pipeline/base.py\nfrom abc import ABC, abstractmethod\n\nclass DetectionPipeline(ABC):\n    \"\"\"Base detection pipeline interface.\"\"\"\n\n    def __init__(\n        self,\n        model_path: Optional[str] = None,\n        mlflow_model_name: Optional[str] = None,\n        device: str = \"cuda\"\n    ):\n        self.device = device\n        self.model = self._load_model(model_path, mlflow_model_name)\n\n    @abstractmethod\n    def detect(self, image_path: str) -&gt; DetectionResult:\n        \"\"\"Detect objects in single image.\"\"\"\n        pass\n\n    @abstractmethod\n    def detect_batch(self, image_paths: List[str]) -&gt; List[DetectionResult]:\n        \"\"\"Detect objects in batch of images.\"\"\"\n        pass\n\n    def _load_model(self, model_path, mlflow_model_name):\n        \"\"\"Load model from file or MLflow.\"\"\"\n        if mlflow_model_name:\n            return self._load_from_mlflow(mlflow_model_name)\n        return self._load_from_file(model_path)\n</code></pre>"},{"location":"architecture/wildetect/#simple-pipeline","title":"Simple Pipeline","text":"<pre><code># src/wildetect/core/pipeline/simple.py\nclass SimplePipeline(DetectionPipeline):\n    \"\"\"Simple sequential pipeline.\"\"\"\n\n    def detect_batch(self, image_paths: List[str]) -&gt; List[DetectionResult]:\n        \"\"\"Process images sequentially.\"\"\"\n        results = []\n        for image_path in tqdm(image_paths):\n            result = self.detect(image_path)\n            results.append(result)\n        return results\n</code></pre>"},{"location":"architecture/wildetect/#multi-threaded-pipeline","title":"Multi-threaded Pipeline","text":"<pre><code># src/wildetect/core/pipeline/multithreaded.py\nfrom concurrent.futures import ThreadPoolExecutor\nfrom queue import Queue\n\nclass MultiThreadedPipeline(DetectionPipeline):\n    \"\"\"Multi-threaded pipeline for parallel processing.\"\"\"\n\n    def __init__(self, num_data_workers: int = 2, queue_size: int = 64, **kwargs):\n        super().__init__(**kwargs)\n        self.num_data_workers = num_data_workers\n        self.queue_size = queue_size\n\n    def detect_batch(self, image_paths: List[str]) -&gt; List[DetectionResult]:\n        \"\"\"Process images using thread pool.\"\"\"\n        with ThreadPoolExecutor(max_workers=self.num_data_workers) as executor:\n            results = list(executor.map(self.detect, image_paths))\n        return results\n</code></pre>"},{"location":"architecture/wildetect/#raster-pipeline","title":"Raster Pipeline","text":"<pre><code># src/wildetect/core/pipeline/raster.py\nclass RasterPipeline(DetectionPipeline):\n    \"\"\"Pipeline for large raster images (GeoTIFF, etc.).\"\"\"\n\n    def __init__(\n        self,\n        tile_size: int = 800,\n        overlap_ratio: float = 0.2,\n        gsd: Optional[float] = None,  # Ground Sample Distance\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.tile_size = tile_size\n        self.overlap_ratio = overlap_ratio\n        self.gsd = gsd\n        self.tiler = RasterTiler(tile_size, overlap_ratio)\n        self.stitcher = DetectionStitcher(nms_threshold=0.5)\n\n    def detect(self, raster_path: str) -&gt; DetectionResult:\n        \"\"\"Detect objects in large raster image.\"\"\"\n        # 1. Tile the raster\n        tiles = self.tiler.tile_raster(raster_path)\n\n        # 2. Detect on each tile\n        tile_detections = []\n        for tile in tqdm(tiles, desc=\"Processing tiles\"):\n            detections = self._detect_tile(tile)\n            tile_detections.append(detections)\n\n        # 3. Stitch detections together\n        stitched = self.stitcher.stitch(tile_detections, tiles)\n\n        # 4. Convert to geographic coordinates if GSD provided\n        if self.gsd:\n            stitched = self._convert_to_geographic(stitched)\n\n        return stitched\n</code></pre>"},{"location":"architecture/wildetect/#2-tiling-engine","title":"2. Tiling Engine","text":"<p>Handle large image tiling and stitching.</p> <pre><code># src/wildetect/core/tiling/tiler.py\nimport rasterio\nfrom rasterio.windows import Window\n\nclass RasterTiler:\n    \"\"\"Tile large raster images.\"\"\"\n\n    def __init__(self, tile_size: int = 800, overlap_ratio: float = 0.2):\n        self.tile_size = tile_size\n        self.overlap = int(tile_size * overlap_ratio)\n        self.stride = tile_size - self.overlap\n\n    def tile_raster(self, raster_path: str) -&gt; List[TileInfo]:\n        \"\"\"Generate tiles from raster.\"\"\"\n        tiles = []\n\n        with rasterio.open(raster_path) as src:\n            width, height = src.width, src.height\n\n            for y in range(0, height, self.stride):\n                for x in range(0, width, self.stride):\n                    # Calculate window\n                    w = min(self.tile_size, width - x)\n                    h = min(self.tile_size, height - y)\n\n                    window = Window(x, y, w, h)\n\n                    # Read tile\n                    tile_data = src.read(window=window)\n\n                    tiles.append(TileInfo(\n                        data=tile_data,\n                        window=window,\n                        x_offset=x,\n                        y_offset=y,\n                        transform=src.window_transform(window)\n                    ))\n\n        return tiles\n</code></pre> <pre><code># src/wildetect/core/tiling/stitcher.py\nclass DetectionStitcher:\n    \"\"\"Stitch tile detections together.\"\"\"\n\n    def __init__(self, nms_threshold: float = 0.5):\n        self.nms_threshold = nms_threshold\n\n    def stitch(\n        self,\n        tile_detections: List[List[Detection]],\n        tiles: List[TileInfo]\n    ) -&gt; List[Detection]:\n        \"\"\"Stitch detections from tiles.\"\"\"\n        # 1. Convert tile coordinates to global coordinates\n        global_detections = []\n        for detections, tile in zip(tile_detections, tiles):\n            for det in detections:\n                # Adjust bbox to global coordinates\n                det.bbox[0] += tile.x_offset\n                det.bbox[1] += tile.y_offset\n                global_detections.append(det)\n\n        # 2. Apply NMS to remove duplicates at boundaries\n        final_detections = self._apply_nms(global_detections)\n\n        return final_detections\n\n    def _apply_nms(self, detections: List[Detection]) -&gt; List[Detection]:\n        \"\"\"Apply Non-Maximum Suppression.\"\"\"\n        # Group by class\n        by_class = {}\n        for det in detections:\n            if det.class_name not in by_class:\n                by_class[det.class_name] = []\n            by_class[det.class_name].append(det)\n\n        # Apply NMS per class\n        final = []\n        for class_name, dets in by_class.items():\n            nms_dets = self._nms_class(dets)\n            final.extend(nms_dets)\n\n        return final\n</code></pre>"},{"location":"architecture/wildetect/#3-census-system","title":"3. Census System","text":"<p>Orchestrate wildlife census campaigns.</p> <pre><code># src/wildetect/core/census/census_engine.py\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n@dataclass\nclass CensusConfig:\n    campaign_name: str\n    target_species: List[str]\n    flight_specs: FlightSpecs\n    coverage_area: Optional[Polygon] = None\n\nclass CensusEngine:\n    \"\"\"Census campaign orchestrator.\"\"\"\n\n    def __init__(self, config: CensusConfig, detection_pipeline: DetectionPipeline):\n        self.config = config\n        self.pipeline = detection_pipeline\n        self.statistics = StatisticsCalculator()\n        self.geographic = GeographicAnalyzer()\n\n    def run_census(self, image_dir: str) -&gt; CensusResult:\n        \"\"\"Run complete census campaign.\"\"\"\n        # 1. Discover images\n        images = self._discover_images(image_dir)\n\n        # 2. Run detection\n        detections = self.pipeline.detect_batch(images)\n\n        # 3. Calculate statistics\n        stats = self.statistics.calculate(detections, self.config.target_species)\n\n        # 4. Geographic analysis\n        geographic = self.geographic.analyze(detections, self.config.flight_specs)\n\n        # 5. Generate report\n        report = self._generate_report(stats, geographic)\n\n        return CensusResult(\n            campaign_name=self.config.campaign_name,\n            statistics=stats,\n            geographic=geographic,\n            detections=detections,\n            report=report\n        )\n</code></pre>"},{"location":"architecture/wildetect/#4-statistics-calculator","title":"4. Statistics Calculator","text":"<p>Compute population statistics.</p> <pre><code># src/wildetect/core/census/statistics.py\nclass StatisticsCalculator:\n    \"\"\"Calculate census statistics.\"\"\"\n\n    def calculate(\n        self,\n        detections: List[DetectionResult],\n        target_species: List[str]\n    ) -&gt; CensusStatistics:\n        \"\"\"Calculate comprehensive statistics.\"\"\"\n        stats = CensusStatistics()\n\n        # Total counts per species\n        stats.total_counts = self._count_by_species(detections)\n\n        # Counts per image\n        stats.counts_per_image = self._counts_per_image(detections)\n\n        # Density analysis\n        stats.density = self._calculate_density(detections)\n\n        # Confidence distribution\n        stats.confidence_dist = self._confidence_distribution(detections)\n\n        # Species co-occurrence\n        stats.co_occurrence = self._species_co_occurrence(detections)\n\n        return stats\n\n    def _count_by_species(self, detections: List[DetectionResult]) -&gt; Dict[str, int]:\n        \"\"\"Count detections per species.\"\"\"\n        counts = {}\n        for result in detections:\n            for det in result.detections:\n                counts[det.class_name] = counts.get(det.class_name, 0) + 1\n        return counts\n</code></pre>"},{"location":"architecture/wildetect/#5-geographic-analyzer","title":"5. Geographic Analyzer","text":"<p>Analyze geographic distribution.</p> <pre><code># src/wildetect/core/geographic/analyzer.py\nfrom shapely.geometry import Point, Polygon\nfrom shapely.ops import unary_union\n\nclass GeographicAnalyzer:\n    \"\"\"Analyze geographic distribution of detections.\"\"\"\n\n    def analyze(\n        self,\n        detections: List[DetectionResult],\n        flight_specs: FlightSpecs\n    ) -&gt; GeographicAnalysis:\n        \"\"\"Perform geographic analysis.\"\"\"\n        # 1. Extract GPS coordinates\n        gps_data = self._extract_gps(detections)\n\n        # 2. Calculate coverage area\n        coverage = self._calculate_coverage(gps_data)\n\n        # 3. Create distribution map\n        distribution = self._create_distribution_map(detections, gps_data)\n\n        # 4. Flight path analysis\n        flight_path = self._analyze_flight_path(gps_data)\n\n        # 5. Hotspot detection\n        hotspots = self._detect_hotspots(detections, gps_data)\n\n        return GeographicAnalysis(\n            coverage_area=coverage,\n            distribution_map=distribution,\n            flight_path=flight_path,\n            hotspots=hotspots,\n            total_area_surveyed=coverage.area\n        )\n</code></pre>"},{"location":"architecture/wildetect/#6-fiftyone-integration","title":"6. FiftyOne Integration","text":"<p>Integrate with FiftyOne for visualization.</p> <pre><code># src/wildetect/core/fiftyone/manager.py\nimport fiftyone as fo\n\nclass FiftyOneManager:\n    \"\"\"Manage FiftyOne datasets.\"\"\"\n\n    def create_dataset(\n        self,\n        name: str,\n        detections: List[DetectionResult],\n        image_dir: str\n    ) -&gt; fo.Dataset:\n        \"\"\"Create FiftyOne dataset from detections.\"\"\"\n        # Create dataset\n        dataset = fo.Dataset(name)\n\n        # Add samples\n        samples = []\n        for result in detections:\n            sample = fo.Sample(filepath=result.image_path)\n\n            # Add detections\n            detections_fo = []\n            for det in result.detections:\n                detections_fo.append(\n                    fo.Detection(\n                        label=det.class_name,\n                        bounding_box=self._convert_bbox(det.bbox),\n                        confidence=det.confidence\n                    )\n                )\n\n            sample[\"detections\"] = fo.Detections(detections=detections_fo)\n            samples.append(sample)\n\n        dataset.add_samples(samples)\n        return dataset\n\n    def launch_app(self, dataset_name: str, port: int = 5151):\n        \"\"\"Launch FiftyOne app.\"\"\"\n        dataset = fo.load_dataset(dataset_name)\n        session = fo.launch_app(dataset, port=port)\n        return session\n</code></pre>"},{"location":"architecture/wildetect/#cli-interface","title":"CLI Interface","text":"<pre><code># src/wildetect/cli/detect.py\nimport typer\n\napp = typer.Typer()\n\n@app.command()\ndef detect(\n    images: str = typer.Argument(..., help=\"Image path or directory\"),\n    model: Optional[str] = typer.Option(None, \"--model\", \"-m\"),\n    config: Optional[str] = typer.Option(None, \"--config\", \"-c\"),\n    output: str = typer.Option(\"results/\", \"--output\", \"-o\")\n):\n    \"\"\"Run wildlife detection.\"\"\"\n\n@app.command()\ndef census(\n    campaign_name: str = typer.Argument(...),\n    images: str = typer.Argument(...),\n    config: str = typer.Option(..., \"--config\", \"-c\"),\n    output: str = typer.Option(\"results/\", \"--output\", \"-o\")\n):\n    \"\"\"Run census campaign.\"\"\"\n\n@app.command()\ndef analyze(\n    results: str = typer.Argument(..., help=\"Detection results JSON\"),\n    output: str = typer.Option(\"analysis/\", \"--output\", \"-o\")\n):\n    \"\"\"Analyze detection results.\"\"\"\n\n@app.command()\ndef visualize(\n    results: str = typer.Argument(...),\n    output: str = typer.Option(\"visualizations/\", \"--output\", \"-o\")\n):\n    \"\"\"Create visualizations.\"\"\"\n</code></pre>"},{"location":"architecture/wildetect/#configuration-files","title":"Configuration Files","text":""},{"location":"architecture/wildetect/#detection-configuration","title":"Detection Configuration","text":"<pre><code># config/detection.yaml\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"raster\"  # simple, multithreaded, raster\n  queue_size: 64\n  num_data_workers: 2\n  nms_threshold: 0.5\n\nflight_specs:\n  sensor_height: 24  # mm\n  focal_length: 35.0  # mm\n  flight_height: 180.0  # meters\n  gsd: 2.38  # cm/px (Ground Sample Distance)\n\noutput:\n  directory: \"results\"\n  dataset_name: \"detections_fiftyone\"  # null to disable FiftyOne\n  save_visualizations: true\n</code></pre>"},{"location":"architecture/wildetect/#census-configuration","title":"Census Configuration","text":"<pre><code># config/census.yaml\ncampaign:\n  name: \"Summer_2024_Survey\"\n  target_species: [\"elephant\", \"giraffe\", \"zebra\"]\n  area_name: \"Serengeti_North\"\n\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n\nflight_specs:\n  flight_height: 120.0\n  gsd: 2.38\n\nanalysis:\n  calculate_density: true\n  detect_hotspots: true\n  create_maps: true\n\noutput:\n  directory: \"census_results\"\n  generate_pdf_report: true\n</code></pre>"},{"location":"architecture/wildetect/#use-cases","title":"Use Cases","text":""},{"location":"architecture/wildetect/#1-simple-detection","title":"1. Simple Detection","text":"<pre><code>from wildetect import DetectionPipeline\n\npipeline = DetectionPipeline(model_path=\"detector.pt\", device=\"cuda\")\nresults = pipeline.detect_batch(\"images/\")\n\n# Save results\npipeline.save_results(results, \"results.json\")\n</code></pre>"},{"location":"architecture/wildetect/#2-census-campaign","title":"2. Census Campaign","text":"<pre><code>from wildetect import CensusEngine, CensusConfig\n\nconfig = CensusConfig.from_yaml(\"config/census.yaml\")\nengine = CensusEngine(config)\n\ncensus_result = engine.run_census(\"survey_images/\")\n\n# Generate report\ncensus_result.save_report(\"census_report.pdf\")\n</code></pre>"},{"location":"architecture/wildetect/#3-large-raster-detection","title":"3. Large Raster Detection","text":"<pre><code>from wildetect import RasterPipeline\n\npipeline = RasterPipeline(\n    model_path=\"detector.pt\",\n    tile_size=800,\n    overlap_ratio=0.2,\n    gsd=2.38  # cm/px\n)\n\n# Detect on large GeoTIFF\nresults = pipeline.detect(\"large_ortho.tif\")\n</code></pre>"},{"location":"architecture/wildetect/#next-steps","title":"Next Steps","text":"<ul> <li>Data Flow Details \u2192</li> <li>Detection Tutorial \u2192</li> <li>Census Tutorial \u2192</li> <li>WildDetect Scripts \u2192</li> </ul>"},{"location":"architecture/wildtrain/","title":"WildTrain Architecture","text":"<p>WildTrain is a modular training framework that supports both object detection (YOLO, MMDetection) and classification (PyTorch Lightning) with integrated experiment tracking and model management.</p>"},{"location":"architecture/wildtrain/#overview","title":"Overview","text":"<p>Purpose: Flexible model training and evaluation framework</p> <p>Key Responsibilities: - Model training (detection and classification) - Experiment tracking with MLflow - Hyperparameter optimization - Model evaluation and metrics - Model registration and versioning</p>"},{"location":"architecture/wildtrain/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TB\n    subgraph \"Configuration Layer\"\n        A[Hydra Config]\n        B[YAML Files]\n    end\n\n    subgraph \"Data Layer\"\n        C[WilData Integration]\n        D[DataModule]\n        E[DataLoaders]\n    end\n\n    subgraph \"Model Layer\"\n        F[YOLO Models]\n        G[MMDet Models]\n        H[Classification Models]\n    end\n\n    subgraph \"Training Layer\"\n        I[Trainer]\n        J[Training Loop]\n        K[Validation]\n    end\n\n    subgraph \"Tracking Layer\"\n        L[MLflow]\n        M[Metrics]\n        N[Artifacts]\n    end\n\n    subgraph \"Output Layer\"\n        O[Trained Models]\n        P[Model Registry]\n        Q[Checkpoints]\n    end\n\n    A --&gt; I\n    B --&gt; A\n    C --&gt; D\n    D --&gt; E\n    E --&gt; J\n    F --&gt; J\n    G --&gt; J\n    H --&gt; J\n    I --&gt; J\n    J --&gt; K\n    K --&gt; M\n    M --&gt; L\n    J --&gt; O\n    O --&gt; P\n    O --&gt; Q\n    L --&gt; N\n\n    style I fill:#e1f5ff\n    style L fill:#fff4e1\n    style P fill:#e8f5e9</code></pre>"},{"location":"architecture/wildtrain/#core-components","title":"Core Components","text":""},{"location":"architecture/wildtrain/#1-model-architectures","title":"1. Model Architectures","text":""},{"location":"architecture/wildtrain/#yolo-detection","title":"YOLO Detection","text":"<pre><code># src/wildtrain/models/detection/yolo_model.py\nfrom ultralytics import YOLO\n\nclass YOLODetector:\n    \"\"\"YOLO-based object detector.\"\"\"\n\n    def __init__(self, model_size: str = \"n\"):\n        self.model = YOLO(f\"yolo11{model_size}.pt\")\n\n    def train(self, data_yaml: str, **kwargs):\n        \"\"\"Train YOLO model.\"\"\"\n        results = self.model.train(\n            data=data_yaml,\n            epochs=kwargs.get('epochs', 100),\n            imgsz=kwargs.get('imgsz', 640),\n            batch=kwargs.get('batch', 16)\n        )\n        return results\n\n    def validate(self, data_yaml: str):\n        \"\"\"Validate model.\"\"\"\n        return self.model.val(data=data_yaml)\n</code></pre>"},{"location":"architecture/wildtrain/#mmdetection","title":"MMDetection","text":"<pre><code># src/wildtrain/models/detection/mmdet_model.py\nfrom mmdet.apis import init_detector, train_detector\nfrom mmdet.apis import inference_detector\n\nclass MMDetDetector:\n    \"\"\"MMDetection-based detector.\"\"\"\n\n    def __init__(self, config: str, checkpoint: Optional[str] = None):\n        self.config = config\n        self.model = init_detector(config, checkpoint) if checkpoint else None\n\n    def train(self, config_file: str, work_dir: str):\n        \"\"\"Train MMDet model.\"\"\"\n        from mmcv import Config\n        cfg = Config.fromfile(config_file)\n        train_detector(self.model, cfg, distributed=False)\n</code></pre>"},{"location":"architecture/wildtrain/#classification-models","title":"Classification Models","text":"<pre><code># src/wildtrain/models/classification/classifier.py\nimport pytorch_lightning as pl\nimport torchvision.models as models\n\nclass ImageClassifier(pl.LightningModule):\n    \"\"\"PyTorch Lightning classifier.\"\"\"\n\n    def __init__(\n        self,\n        architecture: str = \"resnet50\",\n        num_classes: int = 10,\n        learning_rate: float = 0.001\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Load pretrained model\n        self.model = getattr(models, architecture)(pretrained=True)\n\n        # Replace final layer\n        in_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(in_features, num_classes)\n\n        self.criterion = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n</code></pre>"},{"location":"architecture/wildtrain/#2-data-modules","title":"2. Data Modules","text":""},{"location":"architecture/wildtrain/#detection-datamodule","title":"Detection DataModule","text":"<pre><code># src/wildtrain/data/detection_datamodule.py\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader\n\nclass DetectionDataModule(LightningDataModule):\n    \"\"\"DataModule for detection datasets.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 32,\n        num_workers: int = 4\n    ):\n        super().__init__()\n        self.data_root = data_root\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n    def setup(self, stage: Optional[str] = None):\n        # Load datasets using WilData\n        from wildata import DataPipeline\n        pipeline = DataPipeline(self.data_root)\n\n        if stage == \"fit\":\n            self.train_dataset = pipeline.load_dataset(\"train\")\n            self.val_dataset = pipeline.load_dataset(\"val\")\n\n        if stage == \"test\":\n            self.test_dataset = pipeline.load_dataset(\"test\")\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            shuffle=True\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers\n        )\n</code></pre>"},{"location":"architecture/wildtrain/#classification-datamodule","title":"Classification DataModule","text":"<pre><code># src/wildtrain/data/classification_datamodule.py\nclass ClassificationDataModule(LightningDataModule):\n    \"\"\"DataModule for classification datasets.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        image_size: int = 224,\n        batch_size: int = 32,\n        num_workers: int = 4\n    ):\n        super().__init__()\n        self.data_root = data_root\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n        # Define transforms\n        self.train_transform = transforms.Compose([\n            transforms.Resize((image_size, image_size)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n</code></pre>"},{"location":"architecture/wildtrain/#3-training-orchestration","title":"3. Training Orchestration","text":""},{"location":"architecture/wildtrain/#main-trainer","title":"Main Trainer","text":"<pre><code># src/wildtrain/trainers/trainer.py\nfrom hydra.utils import instantiate\nimport mlflow\n\nclass Trainer:\n    \"\"\"Main training orchestrator.\"\"\"\n\n    def __init__(self, config: DictConfig):\n        self.config = config\n        self.model = instantiate(config.model)\n        self.datamodule = instantiate(config.data)\n\n    def train(self):\n        \"\"\"Execute training loop.\"\"\"\n        # Setup MLflow\n        mlflow.set_experiment(self.config.experiment_name)\n\n        with mlflow.start_run():\n            # Log parameters\n            mlflow.log_params(OmegaConf.to_container(self.config))\n\n            # Create PyTorch Lightning trainer\n            pl_trainer = pl.Trainer(\n                max_epochs=self.config.training.epochs,\n                accelerator=self.config.training.accelerator,\n                devices=self.config.training.devices,\n                callbacks=self._create_callbacks()\n            )\n\n            # Train\n            pl_trainer.fit(self.model, self.datamodule)\n\n            # Log metrics\n            metrics = pl_trainer.callback_metrics\n            mlflow.log_metrics({k: v.item() for k, v in metrics.items()})\n\n            # Save model\n            model_path = \"trained_model\"\n            pl_trainer.save_checkpoint(model_path)\n            mlflow.pytorch.log_model(self.model, \"model\")\n\n        return self.model\n\n    def _create_callbacks(self):\n        \"\"\"Create training callbacks.\"\"\"\n        from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\n        return [\n            ModelCheckpoint(\n                monitor='val_loss',\n                mode='min',\n                save_top_k=3\n            ),\n            EarlyStopping(\n                monitor='val_loss',\n                patience=10,\n                mode='min'\n            )\n        ]\n</code></pre>"},{"location":"architecture/wildtrain/#4-evaluation-system","title":"4. Evaluation System","text":""},{"location":"architecture/wildtrain/#metrics-computation","title":"Metrics Computation","text":"<pre><code># src/wildtrain/evaluation/metrics.py\nfrom torchmetrics import Accuracy, Precision, Recall, F1Score\n\nclass ClassificationMetrics:\n    \"\"\"Compute classification metrics.\"\"\"\n\n    def __init__(self, num_classes: int):\n        self.accuracy = Accuracy(num_classes=num_classes)\n        self.precision = Precision(num_classes=num_classes, average='macro')\n        self.recall = Recall(num_classes=num_classes, average='macro')\n        self.f1 = F1Score(num_classes=num_classes, average='macro')\n\n    def compute(self, predictions, targets):\n        \"\"\"Compute all metrics.\"\"\"\n        return {\n            'accuracy': self.accuracy(predictions, targets),\n            'precision': self.precision(predictions, targets),\n            'recall': self.recall(predictions, targets),\n            'f1': self.f1(predictions, targets)\n        }\n</code></pre>"},{"location":"architecture/wildtrain/#detection-metrics","title":"Detection Metrics","text":"<pre><code># src/wildtrain/evaluation/detection_metrics.py\nfrom torchmetrics.detection import MeanAveragePrecision\n\nclass DetectionMetrics:\n    \"\"\"Compute detection metrics.\"\"\"\n\n    def __init__(self):\n        self.map_metric = MeanAveragePrecision()\n\n    def compute(self, predictions, targets):\n        \"\"\"Compute mAP and related metrics.\"\"\"\n        self.map_metric.update(predictions, targets)\n        results = self.map_metric.compute()\n\n        return {\n            'mAP': results['map'],\n            'mAP_50': results['map_50'],\n            'mAP_75': results['map_75']\n        }\n</code></pre>"},{"location":"architecture/wildtrain/#5-hyperparameter-optimization","title":"5. Hyperparameter Optimization","text":""},{"location":"architecture/wildtrain/#optuna-integration","title":"Optuna Integration","text":"<pre><code># src/wildtrain/tuning/optuna_tuner.py\nimport optuna\nfrom optuna.integration import PyTorchLightningPruningCallback\n\nclass OptunaTuner:\n    \"\"\"Hyperparameter tuning with Optuna.\"\"\"\n\n    def __init__(self, config: DictConfig):\n        self.config = config\n        self.study = optuna.create_study(\n            direction=\"minimize\",\n            study_name=config.study_name,\n            storage=config.storage\n        )\n\n    def objective(self, trial: optuna.Trial) -&gt; float:\n        \"\"\"Optimization objective.\"\"\"\n        # Suggest hyperparameters\n        lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n\n        # Update config\n        self.config.model.learning_rate = lr\n        self.config.data.batch_size = batch_size\n\n        # Train model\n        trainer = Trainer(self.config)\n        model = trainer.train()\n\n        # Return validation loss\n        return trainer.pl_trainer.callback_metrics['val_loss'].item()\n\n    def tune(self, n_trials: int = 50):\n        \"\"\"Run hyperparameter tuning.\"\"\"\n        self.study.optimize(self.objective, n_trials=n_trials)\n\n        print(f\"Best trial: {self.study.best_trial.params}\")\n        return self.study.best_params\n</code></pre>"},{"location":"architecture/wildtrain/#6-model-registration","title":"6. Model Registration","text":""},{"location":"architecture/wildtrain/#mlflow-model-registry","title":"MLflow Model Registry","text":"<pre><code># src/wildtrain/registry/model_registry.py\nimport mlflow\nfrom mlflow.tracking import MlflowClient\n\nclass ModelRegistry:\n    \"\"\"Manage models in MLflow registry.\"\"\"\n\n    def __init__(self, tracking_uri: str):\n        mlflow.set_tracking_uri(tracking_uri)\n        self.client = MlflowClient()\n\n    def register_model(\n        self,\n        model_path: str,\n        model_name: str,\n        description: Optional[str] = None,\n        tags: Optional[Dict] = None\n    ) -&gt; str:\n        \"\"\"Register model to MLflow.\"\"\"\n        # Log model\n        with mlflow.start_run():\n            mlflow.pytorch.log_model(model_path, \"model\")\n            run_id = mlflow.active_run().info.run_id\n\n        # Register\n        model_uri = f\"runs:/{run_id}/model\"\n        mv = mlflow.register_model(model_uri, model_name)\n\n        # Add description and tags\n        if description:\n            self.client.update_model_version(\n                name=model_name,\n                version=mv.version,\n                description=description\n            )\n\n        if tags:\n            for key, value in tags.items():\n                self.client.set_model_version_tag(\n                    name=model_name,\n                    version=mv.version,\n                    key=key,\n                    value=value\n                )\n\n        return mv.version\n\n    def load_model(self, model_name: str, version: Optional[str] = None):\n        \"\"\"Load model from registry.\"\"\"\n        if version:\n            model_uri = f\"models:/{model_name}/{version}\"\n        else:\n            model_uri = f\"models:/{model_name}/latest\"\n\n        return mlflow.pytorch.load_model(model_uri)\n\n    def promote_model(self, model_name: str, version: str, stage: str):\n        \"\"\"Promote model to production stage.\"\"\"\n        self.client.transition_model_version_stage(\n            name=model_name,\n            version=version,\n            stage=stage  # \"Staging\", \"Production\", \"Archived\"\n        )\n</code></pre>"},{"location":"architecture/wildtrain/#configuration-system","title":"Configuration System","text":""},{"location":"architecture/wildtrain/#hydra-configuration","title":"Hydra Configuration","text":"<p>WildTrain uses Hydra for flexible configuration management.</p> <pre><code># src/wildtrain/main.py\nimport hydra\nfrom omegaconf import DictConfig\n\n@hydra.main(config_path=\"configs\", config_name=\"main\", version_base=\"1.3\")\ndef main(cfg: DictConfig):\n    trainer = Trainer(cfg)\n    trainer.train()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"architecture/wildtrain/#configuration-structure","title":"Configuration Structure","text":"<pre><code># configs/main.yaml\ndefaults:\n  - model: yolo\n  - data: detection\n  - training: default\n  - _self_\n\nexperiment_name: wildlife_detection\nseed: 42\n\n# Override from CLI:\n# python main.py model=custom data.batch_size=64\n</code></pre>"},{"location":"architecture/wildtrain/#model-configs","title":"Model Configs","text":"<pre><code># configs/detection/yolo.yaml\nmodel:\n  framework: \"yolo\"\n  size: \"n\"  # n, s, m, l, x\n  pretrained: true\n\ntraining:\n  epochs: 100\n  imgsz: 640\n  batch: 16\n  optimizer: \"AdamW\"\n  lr0: 0.001\n</code></pre>"},{"location":"architecture/wildtrain/#cli-interface","title":"CLI Interface","text":"<pre><code># src/wildtrain/cli/train.py\nimport typer\napp = typer.Typer()\n\n@app.command()\ndef train(\n    task: str = typer.Option(..., help=\"Task type: classifier or detector\"),\n    config: str = typer.Option(..., \"-c\", \"--config\"),\n    override: List[str] = typer.Option(None, \"-o\", \"--override\")\n):\n    \"\"\"Train a model.\"\"\"\n    # Load config with overrides\n    # Start training\n\n@app.command()\ndef evaluate(\n    task: str,\n    config: str,\n    checkpoint: str\n):\n    \"\"\"Evaluate a trained model.\"\"\"\n\n@app.command()\ndef tune(\n    config: str,\n    n_trials: int = 50\n):\n    \"\"\"Run hyperparameter tuning.\"\"\"\n</code></pre>"},{"location":"architecture/wildtrain/#use-cases","title":"Use Cases","text":""},{"location":"architecture/wildtrain/#1-train-yolo-detector","title":"1. Train YOLO Detector","text":"<pre><code># Using CLI\nwildtrain train detector -c configs/detection/yolo.yaml\n\n# Using Python\nfrom wildtrain import Trainer\ntrainer = Trainer.from_config(\"configs/detection/yolo.yaml\")\nmodel = trainer.train()\n</code></pre>"},{"location":"architecture/wildtrain/#2-train-classifier","title":"2. Train Classifier","text":"<pre><code>import pytorch_lightning as pl\nfrom wildtrain import ImageClassifier, ClassificationDataModule\n\n# Create model and data\nmodel = ImageClassifier(architecture=\"resnet50\", num_classes=10)\ndatamodule = ClassificationDataModule(data_root=\"data/classification\")\n\n# Create trainer\ntrainer = pl.Trainer(max_epochs=50, accelerator=\"gpu\")\n\n# Train\ntrainer.fit(model, datamodule)\n</code></pre>"},{"location":"architecture/wildtrain/#3-hyperparameter-tuning","title":"3. Hyperparameter Tuning","text":"<pre><code>from wildtrain.tuning import OptunaTuner\n\ntuner = OptunaTuner(config)\nbest_params = tuner.tune(n_trials=50)\n\nprint(f\"Best hyperparameters: {best_params}\")\n</code></pre>"},{"location":"architecture/wildtrain/#4-model-registration","title":"4. Model Registration","text":"<pre><code>from wildtrain.registry import ModelRegistry\n\nregistry = ModelRegistry(tracking_uri=\"http://localhost:5000\")\n\n# Register model\nversion = registry.register_model(\n    model_path=\"checkpoints/best.ckpt\",\n    model_name=\"wildlife_detector\",\n    description=\"YOLO model trained on aerial images\",\n    tags={\"framework\": \"yolo\", \"dataset\": \"wildlife_v1\"}\n)\n\n# Promote to production\nregistry.promote_model(\"wildlife_detector\", version, \"Production\")\n</code></pre>"},{"location":"architecture/wildtrain/#next-steps","title":"Next Steps","text":"<ul> <li>WildDetect Architecture \u2192</li> <li>Data Flow Details \u2192</li> <li>Training Tutorial \u2192</li> <li>WildTrain Scripts \u2192</li> </ul>"},{"location":"configs/wildata/","title":"WilData Configuration Reference","text":"<p>Documentation for all WilData configuration files used in data management operations.</p>"},{"location":"configs/wildata/#configuration-files","title":"Configuration Files","text":"File Purpose import-config-example.yaml Dataset import configuration bulk-import-*.yaml Bulk import configurations roi-create-config.yaml ROI dataset creation bulk-roi-create-config.yaml Bulk ROI creation gps-update-config-example.yaml GPS metadata update label_studio_config.xml Label Studio interface"},{"location":"configs/wildata/#import-config-exampleyaml","title":"import-config-example.yaml","text":"<p>Purpose: Configure dataset import with transformations</p> <p>Location: <code>wildata/configs/import-config-example.yaml</code></p>"},{"location":"configs/wildata/#complete-configuration","title":"Complete Configuration","text":"<pre><code># Required: Source Information\nsource_path: \"D:/annotations/dataset.json\"\nsource_format: \"coco\"  # coco, yolo, ls (Label Studio)\ndataset_name: \"my_dataset\"\n\n# Pipeline Configuration\nroot: \"D:/data\"\nsplit_name: \"train\"  # train, val, test\nenable_dvc: false\nprocessing_mode: \"batch\"  # batch, streaming\ntrack_with_dvc: false\nbbox_tolerance: 5\n\n# Label Studio Options (for ls format)\ndotenv_path: \".env\"\nls_xml_config: \"configs/label_studio_config.xml\"\nls_parse_config: false\n\n# ROI Configuration\ndisable_roi: false\nroi_config:\n  random_roi_count: 2              # Background samples per image\n  roi_box_size: 384                # ROI size (pixels)\n  min_roi_size: 32                 # Minimum object size\n  dark_threshold: 0.7              # Dark image threshold\n  background_class: \"background\"\n  save_format: \"jpg\"               # jpg, png\n  quality: 95                      # JPEG quality\n  sample_background: true          # Sample background regions\n\n# Transformation Pipeline\ntransformations:\n  # Bbox Clipping\n  enable_bbox_clipping: true\n  bbox_clipping:\n    tolerance: 5                   # Pixels outside image allowed\n    skip_invalid: false            # Skip invalid bboxes\n\n  # Data Augmentation\n  enable_augmentation: false\n  augmentation:\n    rotation_range: [-45, 45]     # Rotation degrees\n    probability: 1.0               # Augmentation probability\n    brightness_range: [-0.2, 0.4]\n    scale: [1.0, 2.0]\n    translate: [-0.1, 0.2]\n    shear: [-5, 5]\n    contrast_range: [-0.2, 0.4]\n    noise_std: [0.01, 0.1]\n    seed: 41\n    num_transforms: 2              # Augmentations per image\n\n  # Image Tiling\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640                    # Tile stride\n    min_visibility: 0.7            # Min object visibility\n    max_negative_tiles_in_negative_image: 2\n    negative_positive_ratio: 1.0\n    dark_threshold: 0.7\n</code></pre>"},{"location":"configs/wildata/#bulk-import-configs","title":"bulk-import Configs","text":"<p>Purpose: Configure batch import of multiple datasets</p> <p>Files: - <code>bulk-import-train.yaml</code> - <code>bulk-import-val.yaml</code> - <code>bulk-import-config-example.yaml</code></p>"},{"location":"configs/wildata/#configuration-format","title":"Configuration Format","text":"<pre><code>source_paths:\n  - \"D:/annotations/dataset1.json\"\n  - \"D:/annotations/dataset2.json\"\n  - \"D:/annotations/dataset3.json\"\n\nsource_format: \"coco\"\nroot: \"D:/data\"\nsplit_name: \"train\"\n\n# Shared settings (same as import-config)\nprocessing_mode: \"batch\"\nbbox_tolerance: 5\n\ntransformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n</code></pre>"},{"location":"configs/wildata/#roi-create-configyaml","title":"roi-create-config.yaml","text":"<p>Purpose: Configure ROI dataset creation</p> <p>Location: <code>wildata/configs/roi-create-config.yaml</code></p>"},{"location":"configs/wildata/#configuration","title":"Configuration","text":"<pre><code>source_path: \"annotations.json\"\nsource_format: \"coco\"\ndataset_name: \"roi_dataset\"\n\nroot: \"data\"\nsplit_name: \"val\"  # Usually val or test\nbbox_tolerance: 5\n\nroi_config:\n  roi_box_size: 128              # ROI crop size\n  min_roi_size: 32               # Min object size to extract\n  random_roi_count: 10           # Background samples per image\n  dark_threshold: 0.7\n  background_class: \"background\"\n  save_format: \"jpg\"\n  quality: 95\n  padding: 10                    # Padding around object\n  sample_background: true\n\n  # Advanced options\n  aspect_ratio_range: [0.5, 2.0]  # Valid aspect ratios\n  min_object_area: 32             # Min area (pixels\u00b2)\n\nls_xml_config: null\nls_parse_config: false\ndraw_original_bboxes: false\n</code></pre>"},{"location":"configs/wildata/#bulk-roi-create-configyaml","title":"bulk-roi-create-config.yaml","text":"<p>Purpose: Bulk ROI dataset creation</p>"},{"location":"configs/wildata/#configuration_1","title":"Configuration","text":"<pre><code>source_paths:\n  - \"dataset1.json\"\n  - \"dataset2.json\"\n\nsource_format: \"coco\"\nsplit_name: \"val\"\n\nroi_config:\n  roi_box_size: 128\n  random_roi_count: 5\n  background_class: \"background\"\n</code></pre>"},{"location":"configs/wildata/#gps-update-config-exampleyaml","title":"gps-update-config-example.yaml","text":"<p>Purpose: Update image GPS from CSV</p> <p>Location: <code>wildata/configs/gps-update-config-example.yaml</code></p>"},{"location":"configs/wildata/#configuration_2","title":"Configuration","text":"<pre><code>image_folder: \"D:/images/\"\ncsv_path: \"gps_coordinates.csv\"\noutput_dir: \"D:/images_with_gps/\"\n\n# CSV Parsing\nskip_rows: 0\nfilename_col: \"filename\"\nlat_col: \"latitude\"\nlon_col: \"longitude\"\nalt_col: \"altitude\"\n\n# Options\noverwrite_existing: false        # Overwrite existing GPS\ncreate_backup: true              # Backup original files\nvalidate_coordinates: true       # Validate GPS coordinates\n</code></pre>"},{"location":"configs/wildata/#csv-format","title":"CSV Format","text":"<pre><code>filename,latitude,longitude,altitude\nimage001.jpg,40.7128,-74.0060,10.5\nimage002.jpg,40.7589,-73.9851,15.2\n</code></pre>"},{"location":"configs/wildata/#label_studio_configxml","title":"label_studio_config.xml","text":"<p>Purpose: Label Studio annotation interface configuration</p> <p>Location: <code>wildata/configs/label_studio_config.xml</code></p>"},{"location":"configs/wildata/#example-configuration","title":"Example Configuration","text":"<pre><code>&lt;View&gt;\n  &lt;Image name=\"image\" value=\"$image\"/&gt;\n  &lt;RectangleLabels name=\"label\" toName=\"image\"&gt;\n    &lt;Label value=\"elephant\" background=\"red\"/&gt;\n    &lt;Label value=\"giraffe\" background=\"blue\"/&gt;\n    &lt;Label value=\"zebra\" background=\"green\"/&gt;\n    &lt;Label value=\"buffalo\" background=\"yellow\"/&gt;\n  &lt;/RectangleLabels&gt;\n&lt;/View&gt;\n</code></pre>"},{"location":"configs/wildata/#configuration-examples","title":"Configuration Examples","text":""},{"location":"configs/wildata/#import-coco-with-tiling","title":"Import COCO with Tiling","text":"<pre><code>source_path: \"D:/coco/annotations.json\"\nsource_format: \"coco\"\ndataset_name: \"wildlife_tiled\"\n\nroot: \"D:/data\"\nsplit_name: \"train\"\n\ntransformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n    min_visibility: 0.7\n</code></pre>"},{"location":"configs/wildata/#import-label-studio","title":"Import Label Studio","text":"<pre><code>source_path: \"D:/label_studio/export.json\"\nsource_format: \"ls\"\ndataset_name: \"annotated_data\"\n\nls_xml_config: \"configs/label_studio_config.xml\"\nls_parse_config: true\n\nroi_config:\n  roi_box_size: 128\n  random_roi_count: 5\n</code></pre>"},{"location":"configs/wildata/#import-with-full-pipeline","title":"Import with Full Pipeline","text":"<pre><code>source_path: \"raw_annotations.json\"\nsource_format: \"coco\"\ndataset_name: \"processed_dataset\"\n\ntransformations:\n  enable_bbox_clipping: true\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n  enable_augmentation: true\n  augmentation:\n    num_transforms: 2\n    probability: 0.8\n\nroi_config:\n  roi_box_size: 384\n  random_roi_count: 10\n</code></pre>"},{"location":"configs/wildata/#best-practices","title":"Best Practices","text":"<ol> <li>Use absolute paths for cross-platform compatibility</li> <li>Enable bbox_clipping to fix annotation errors</li> <li>Tile large images for better training</li> <li>Sample background ROIs for balanced datasets</li> <li>Version control configuration changes</li> </ol>"},{"location":"configs/wildata/#next-steps","title":"Next Steps","text":"<ul> <li>WilData Scripts</li> <li>Dataset Preparation Tutorial</li> <li>WilData API Reference</li> </ul>"},{"location":"configs/wildetect/","title":"WildDetect Configuration Reference","text":"<p>This page documents all configuration files used by WildDetect for detection, census, and related operations.</p>"},{"location":"configs/wildetect/#overview","title":"Overview","text":"<p>Configuration files are located in the <code>config/</code> directory and use YAML format for easy editing.</p>"},{"location":"configs/wildetect/#configuration-files","title":"Configuration Files","text":"File Purpose detection.yaml Main detection configuration census.yaml Census campaign configuration benchmark.yaml Performance benchmarking visualization.yaml Visualization settings extract-gps.yaml GPS extraction configuration detector_registration.yaml Model registration config.yaml Main application config class_mapping.json Class ID to name mapping"},{"location":"configs/wildetect/#detectionyaml","title":"detection.yaml","text":"<p>Purpose: Configure wildlife detection pipeline parameters.</p> <p>Location: <code>config/detection.yaml</code></p>"},{"location":"configs/wildetect/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code># Model Configuration\nmodel:\n  mlflow_model_name: \"detector\"          # Model name in MLflow registry\n  mlflow_model_alias: \"production\"       # Model version alias\n  mlflow_model_version: null             # Specific version (overrides alias)\n  model_path: null                       # Direct path to model file (alternative to MLflow)\n  device: \"cuda\"                         # Device: \"cuda\", \"cpu\", \"auto\"\n\n# Input Sources (choose one)\nimage_paths:                             # List of specific image paths\n  - \"path/to/image1.jpg\"\n  - \"path/to/image2.tif\"\n\nimage_dir: null                          # Directory containing images\n\n# EXIF GPS Update (optional)\nexif_gps_update:\n  image_folder: null                     # Folder with images to update\n  csv_path: null                         # CSV with GPS coordinates\n  skip_rows: 4                           # Rows to skip in CSV\n  filename_col: \"filename\"               # Column name for filenames\n  lat_col: \"latitude\"                    # Column name for latitude\n  lon_col: \"longitude\"                   # Column name for longitude\n  alt_col: \"altitude\"                    # Column name for altitude\n\n# Label Studio Integration (optional)\nlabelstudio:\n  url: null                              # Label Studio URL\n  api_key: null                          # API key\n  project_id: null                       # Project ID\n  download_resources: false              # Download images from LS\n\n# Processing Configuration\nprocessing:\n  batch_size: 32                         # Batch size for inference\n  tile_size: 800                         # Tile size for large images (pixels)\n  overlap_ratio: 0.2                     # Overlap ratio between tiles (0.0-1.0)\n  pipeline_type: \"raster\"                # Pipeline: \"raster\", \"multithreaded\", \"simple\", \"default\"\n  queue_size: 64                         # Queue size for multithreaded pipeline\n  num_data_workers: 1                    # Number of data loading workers\n  num_inference_workers: 1               # Number of inference workers (multiprocessing)\n  prefetch_factor: 2                     # Batches to prefetch per worker\n  pin_memory: true                       # Pin memory for faster GPU transfer\n  nms_threshold: 0.5                     # NMS threshold for detection stitching\n  max_errors: 5                          # Max errors before stopping\n  confidence_threshold: 0.5              # Minimum confidence for detections\n\n# Flight Specifications\nflight_specs:\n  sensor_height: 24                      # Camera sensor height (mm)\n  focal_length: 35.0                     # Lens focal length (mm)\n  flight_height: 180.0                   # Flight altitude (meters)\n  gsd: 2.38                              # Ground Sample Distance (cm/pixel) - REQUIRED for raster\n\n# Inference Service (optional)\ninference_service:\n  url: null                              # URL for external inference service\n  # Example: \"http://localhost:4141/predict\"\n  timeout: 60                            # Request timeout (seconds)\n\n# Profiling Configuration\nprofiling:\n  enable: false                          # Enable profiling\n  memory_profile: false                  # Profile memory usage\n  line_profile: false                    # Line-by-line profiling\n  gpu_profile: false                     # GPU memory profiling\n\n# Output Configuration\noutput:\n  directory: \"results\"                   # Output directory for results\n  dataset_name: null                     # FiftyOne dataset name (null to disable)\n  save_visualizations: true              # Save visualization images\n  save_crops: false                      # Save detection crops\n  export_formats: [\"json\", \"csv\"]        # Export formats\n\n# Logging Configuration\nlogging:\n  verbose: false                         # Verbose logging\n  log_file: null                         # Log file path (null for default)\n  log_level: \"INFO\"                      # Log level: DEBUG, INFO, WARNING, ERROR\n</code></pre>"},{"location":"configs/wildetect/#example-configurations","title":"Example Configurations","text":""},{"location":"configs/wildetect/#basic-detection","title":"Basic Detection","text":"<pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nimage_dir: \"D:/images/survey/\"\n\nprocessing:\n  batch_size: 32\n  pipeline_type: \"simple\"\n\noutput:\n  directory: \"results/detections\"\n</code></pre>"},{"location":"configs/wildetect/#raster-detection-large-geotiff","title":"Raster Detection (Large GeoTIFF)","text":"<pre><code>model:\n  mlflow_model_name: \"detector\"\n  device: \"cuda\"\n\nimage_paths:\n  - \"D:/orthomosaics/ortho_large.tif\"\n\nprocessing:\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"raster\"\n  nms_threshold: 0.5\n\nflight_specs:\n  gsd: 2.38  # Required for raster\n\noutput:\n  directory: \"results/raster_detections\"\n</code></pre>"},{"location":"configs/wildetect/#censusyaml","title":"census.yaml","text":"<p>Purpose: Configure wildlife census campaigns with statistics and analysis.</p> <p>Location: <code>config/census.yaml</code></p>"},{"location":"configs/wildetect/#complete-parameter-reference_1","title":"Complete Parameter Reference","text":"<pre><code># Campaign Information\ncampaign:\n  name: \"Summer_2024_Survey\"             # Campaign name\n  target_species: [\"elephant\", \"giraffe\", \"zebra\"]  # Target species list\n  area_name: \"Serengeti_North\"           # Survey area name\n  start_date: \"2024-06-01\"               # Campaign start date\n  end_date: \"2024-06-15\"                 # Campaign end date\n  pilot_name: null                       # Pilot name\n  notes: null                            # Additional notes\n\n# Model Configuration (same as detection.yaml)\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\n# Image Sources\nimage_dir: \"D:/census_images/\"           # Directory with survey images\n\n# Processing (same as detection.yaml)\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"raster\"\n  nms_threshold: 0.5\n\n# Flight Specifications\nflight_specs:\n  sensor_height: 24\n  focal_length: 35.0\n  flight_height: 120.0                   # Flight altitude (meters)\n  gsd: 2.38                              # Ground Sample Distance (cm/pixel)\n\n# Analysis Configuration\nanalysis:\n  calculate_density: true                # Calculate population density\n  density_unit: \"per_km2\"                # Density unit: \"per_km2\", \"per_hectare\"\n  detect_hotspots: true                  # Identify concentration hotspots\n  hotspot_radius: 500                    # Hotspot radius (meters)\n  create_maps: true                      # Generate geographic maps\n  coverage_analysis: true                # Analyze survey coverage\n  species_distribution: true             # Species distribution analysis\n  co_occurrence_analysis: true           # Species co-occurrence\n\n# Statistics Configuration\nstatistics:\n  confidence_bins: [0.5, 0.7, 0.9]       # Confidence bins for analysis\n  size_bins: [50, 100, 200]              # Object size bins (pixels)\n  group_size_analysis: true              # Analyze group sizes\n\n# Visualization Settings\nvisualization:\n  create_heatmaps: true                  # Create density heatmaps\n  create_distribution_maps: true         # Create distribution maps\n  create_flight_path_map: true           # Show flight path\n  overlay_detections: true               # Overlay detections on maps\n  color_by_species: true                 # Color code by species\n\n# Output Configuration\noutput:\n  directory: \"census_results\"            # Output directory\n  dataset_name: \"census_2024\"            # FiftyOne dataset name\n  generate_pdf_report: true              # Generate PDF report\n  generate_excel: true                   # Generate Excel statistics\n  save_individual_reports: true          # Save per-image reports\n\n# Reporting\nreport:\n  include_methodology: true              # Include methodology section\n  include_confidence_analysis: true      # Include confidence analysis\n  include_temporal_analysis: false       # Include time-based analysis\n  executive_summary: true                # Include executive summary\n  detailed_statistics: true              # Include detailed stats\n</code></pre>"},{"location":"configs/wildetect/#example-configurations_1","title":"Example Configurations","text":""},{"location":"configs/wildetect/#basic-census","title":"Basic Census","text":"<pre><code>campaign:\n  name: \"Quick_Survey_2024\"\n  target_species: [\"elephant\"]\n\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n\nimage_dir: \"D:/survey/\"\n\nanalysis:\n  calculate_density: true\n  create_maps: true\n\noutput:\n  directory: \"census_results\"\n  generate_pdf_report: true\n</code></pre>"},{"location":"configs/wildetect/#comprehensive-census","title":"Comprehensive Census","text":"<pre><code>campaign:\n  name: \"Annual_Census_2024\"\n  target_species: [\"elephant\", \"giraffe\", \"zebra\", \"buffalo\"]\n  area_name: \"Protected_Area_North\"\n  start_date: \"2024-06-01\"\n  pilot_name: \"John Doe\"\n\nmodel:\n  mlflow_model_name: \"detector\"\n  device: \"cuda\"\n\nimage_dir: \"D:/annual_census/images/\"\n\nflight_specs:\n  flight_height: 150.0\n  gsd: 3.0\n\nanalysis:\n  calculate_density: true\n  detect_hotspots: true\n  species_distribution: true\n  co_occurrence_analysis: true\n\noutput:\n  directory: \"census_2024\"\n  generate_pdf_report: true\n  generate_excel: true\n</code></pre>"},{"location":"configs/wildetect/#benchmarkyaml","title":"benchmark.yaml","text":"<p>Purpose: Configure performance benchmarking tests.</p> <p>Location: <code>config/benchmark.yaml</code></p>"},{"location":"configs/wildetect/#parameter-reference","title":"Parameter Reference","text":"<pre><code>benchmark:\n  test_images: [\"test1.jpg\", \"test2.tif\"]  # Images to benchmark\n  iterations: 10                            # Number of iterations\n  warmup_iterations: 2                      # Warmup runs\n  measure_memory: true                      # Measure memory usage\n  measure_gpu: true                         # Measure GPU utilization\n\nmodels:\n  - name: \"yolo11n\"\n    path: \"models/yolo11n.pt\"\n  - name: \"yolo11s\"\n    path: \"models/yolo11s.pt\"\n\nconfigurations:\n  - batch_size: 1\n  - batch_size: 8\n  - batch_size: 32\n</code></pre>"},{"location":"configs/wildetect/#visualizationyaml","title":"visualization.yaml","text":"<p>Purpose: Configure visualization settings.</p> <p>Location: <code>config/visualization.yaml</code></p>"},{"location":"configs/wildetect/#parameter-reference_1","title":"Parameter Reference","text":"<pre><code>visualization:\n  bbox_color: \"red\"                      # Bounding box color\n  bbox_thickness: 2                      # Box line thickness\n  show_labels: true                      # Show class labels\n  show_confidence: true                  # Show confidence scores\n  font_size: 12                          # Font size for labels\n  dpi: 300                               # Output DPI for images\n\nmaps:\n  basemap: \"OpenStreetMap\"               # Basemap provider\n  zoom_level: 12                         # Default zoom level\n  marker_size: 8                         # Marker size\n</code></pre>"},{"location":"configs/wildetect/#extract-gpsyaml","title":"extract-gps.yaml","text":"<p>Purpose: Configure GPS coordinate extraction from images.</p> <p>Location: <code>config/extract-gps.yaml</code></p>"},{"location":"configs/wildetect/#parameter-reference_2","title":"Parameter Reference","text":"<pre><code>gps_extraction:\n  image_directory: \"D:/images/\"          # Directory with images\n  output_file: \"gps_coordinates.csv\"     # Output CSV file\n  recursive: true                        # Search subdirectories\n  include_images_without_gps: false      # Include images without GPS\n\nformat:\n  decimal_places: 6                      # GPS coordinate precision\n  date_format: \"%Y-%m-%d %H:%M:%S\"       # Date format\n</code></pre>"},{"location":"configs/wildetect/#detector_registrationyaml","title":"detector_registration.yaml","text":"<p>Purpose: Configure model registration to MLflow.</p> <p>Location: <code>config/detector_registration.yaml</code></p>"},{"location":"configs/wildetect/#parameter-reference_3","title":"Parameter Reference","text":"<pre><code>registration:\n  model_path: \"models/best.pt\"           # Path to model weights\n  model_name: \"wildlife_detector\"        # Model name in registry\n  model_type: \"detector\"                 # \"detector\" or \"classifier\"\n\n  description: |\n    YOLO11n model trained on aerial wildlife images\n    Dataset: Wildlife Aerial v2.0\n    Training date: 2024-01-15\n\n  tags:\n    framework: \"yolo\"\n    version: \"11n\"\n    dataset: \"wildlife_v2\"\n    map50: \"0.89\"\n    map50_95: \"0.76\"\n\n  aliases:\n    - \"production\"\n    - \"latest\"\n    - \"v2.0\"\n\n  artifacts:\n    - \"configs/training_config.yaml\"\n    - \"logs/training.log\"\n</code></pre>"},{"location":"configs/wildetect/#configyaml","title":"config.yaml","text":"<p>Purpose: Main application configuration.</p> <p>Location: <code>config/config.yaml</code></p>"},{"location":"configs/wildetect/#parameter-reference_4","title":"Parameter Reference","text":"<pre><code>app:\n  name: \"WildDetect\"\n  version: \"1.0.0\"\n\nmlflow:\n  tracking_uri: \"http://localhost:5000\"\n  experiment_name: \"wildlife_detection\"\n\npaths:\n  data_root: \"D:/data/\"\n  models_root: \"D:/models/\"\n  results_root: \"D:/results/\"\n\ndefaults:\n  device: \"cuda\"\n  batch_size: 32\n  confidence_threshold: 0.5\n</code></pre>"},{"location":"configs/wildetect/#class_mappingjson","title":"class_mapping.json","text":"<p>Purpose: Map class IDs to class names.</p> <p>Location: <code>config/class_mapping.json</code></p>"},{"location":"configs/wildetect/#format","title":"Format","text":"<pre><code>{\n  \"0\": \"elephant\",\n  \"1\": \"giraffe\",\n  \"2\": \"zebra\",\n  \"3\": \"buffalo\",\n  \"4\": \"wildebeest\"\n}\n</code></pre>"},{"location":"configs/wildetect/#configuration-best-practices","title":"Configuration Best Practices","text":""},{"location":"configs/wildetect/#1-use-environment-variables","title":"1. Use Environment Variables","text":"<p>For sensitive data:</p> <pre><code>mlflow:\n  tracking_uri: ${MLFLOW_TRACKING_URI}\n\nlabelstudio:\n  api_key: ${LABEL_STUDIO_API_KEY}\n</code></pre>"},{"location":"configs/wildetect/#2-create-config-variants","title":"2. Create Config Variants","text":"<p>For different scenarios:</p> <pre><code>config/\n\u251c\u2500\u2500 detection.yaml          # Default\n\u251c\u2500\u2500 detection_dev.yaml      # Development\n\u251c\u2500\u2500 detection_prod.yaml     # Production\n\u2514\u2500\u2500 detection_test.yaml     # Testing\n</code></pre>"},{"location":"configs/wildetect/#3-document-custom-settings","title":"3. Document Custom Settings","text":"<p>Add comments to configs:</p> <pre><code>processing:\n  batch_size: 16  # Reduced for 8GB GPU\n  tile_size: 640  # Smaller tiles for memory efficiency\n</code></pre>"},{"location":"configs/wildetect/#4-version-control","title":"4. Version Control","text":"<p>Track configuration changes:</p> <pre><code>git add config/\ngit commit -m \"Update detection config for new model\"\n</code></pre>"},{"location":"configs/wildetect/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configs/wildetect/#invalid-configuration","title":"Invalid Configuration","text":"<p>Issue: Config validation fails</p> <p>Solutions: 1. Check YAML syntax (indentation, colons) 2. Verify required fields are present 3. Check data types match expected types 4. Use YAML validator online</p>"},{"location":"configs/wildetect/#path-not-found","title":"Path Not Found","text":"<p>Issue: Image paths or model paths not found</p> <p>Solutions: 1. Use absolute paths 2. Check path separators (use forward slashes) 3. Verify files exist 4. Check permissions</p>"},{"location":"configs/wildetect/#model-loading-fails","title":"Model Loading Fails","text":"<p>Issue: Can't load model from MLflow</p> <p>Solutions: 1. Verify MLflow server is running 2. Check <code>mlflow_model_name</code> is correct 3. Verify model exists in registry 4. Check <code>MLFLOW_TRACKING_URI</code> environment variable</p>"},{"location":"configs/wildetect/#next-steps","title":"Next Steps","text":"<ul> <li>WildDetect Scripts</li> <li>Detection Tutorial</li> <li>Census Tutorial</li> <li>CLI Reference</li> </ul>"},{"location":"configs/wildetect/benchmark/","title":"Benchmark Configuration","text":"<p>Location: <code>config/benchmark.yaml</code></p> <p>Purpose: Configuration file for performance benchmarking and hyperparameter optimization of the detection pipeline. Uses Optuna for automated hyperparameter search to find optimal settings for batch size, tile size, worker counts, and other processing parameters.</p>"},{"location":"configs/wildetect/benchmark/#configuration-structure","title":"Configuration Structure","text":""},{"location":"configs/wildetect/benchmark/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code># Benchmark Configuration for WildDetect\n# This file configures the benchmarking of the detection pipeline\n\n# Core benchmark execution settings\nexecution:\n  n_trials: 30                    # Number of optimization trials\n  timeout: 3600                   # Maximum time for optimization in seconds\n  direction: \"minimize\"           # Optimization direction: \"minimize\" or \"maximize\"\n  sampler: \"TPE\"                  # Optuna sampler: \"TPE\", \"Random\", or \"Grid\"\n  seed: 42                        # Random seed for reproducibility\n\n# Test data configuration\ntest_images:\n  path: \"test_images\"             # Path to test images directory\n  recursive: true                 # Search recursively for images\n  max_images: 100                # Maximum number of images to use\n  supported_formats:              # Supported image formats\n    - \".jpg\"\n    - \".jpeg\"\n    - \".png\"\n    - \".tiff\"\n    - \".tif\"\n    - \".bmp\"\n\n# Hyperparameter search space\nhyperparameters:\n  batch_size:                     # Batch sizes to test\n    - 8\n    - 16\n    - 32\n    - 64\n    - 128\n    - 256\n    - 512\n  num_workers:                    # Number of workers to test\n    - 0\n    - 2\n    - 4\n    - 8\n    - 16\n  tile_size:                      # Tile sizes to test\n    - 400\n    - 800\n    - 1200\n    - 1600\n  overlap_ratio:                  # Overlap ratios to test\n    - 0.1\n    - 0.2\n    - 0.3\n\n# Output configuration\noutput:\n  directory: \"results/benchmarks\" # Output directory for results\n  save_plots: true               # Save performance plots\n  save_results: true             # Save detailed results\n  format: \"json\"                 # Output format: \"json\", \"csv\", or \"both\"\n  include_optimization_history: true  # Include optimization history\n  auto_open: false               # Auto-open results after completion\n\n# Model configuration (inherits from existing models)\nmodel:\n  mlflow_model_name: null        # Will use environment variable if null\n  mlflow_model_alias: null       # Will use environment variable if null\n  device: \"auto\"                 # Device to run inference on\n\n# Processing configuration\nprocessing:\n  tile_size: 800                 # Default tile size for processing\n  overlap_ratio: 0.2             # Default overlap ratio\n  pipeline_type: \"single\"        # Pipeline type: \"single\", \"multi\", or \"async\"\n  queue_size: 64                 # Queue size for multi-threaded pipeline\n  batch_size: 32                 # Default batch size for inference\n  num_workers: 0                 # Default number of workers\n  max_concurrent: 4              # Maximum concurrent inference tasks\n\n# Flight specifications\nflight_specs:\n  sensor_height: 24.0            # Sensor height in mm\n  focal_length: 35.0             # Focal length in mm\n  flight_height: 180.0           # Flight height in meters\n\n# Inference service configuration\ninference_service:\n  url: null                      # Inference service URL (if using external service)\n  timeout: 60                    # Timeout for inference in seconds\n\n# Logging configuration\nlogging:\n  verbose: false                 # Verbose logging\n  log_file: null                 # Log file path\n\n# Profiling configuration\nprofiling:\n  enable: false                  # Enable profiling\n  memory_profile: false          # Enable memory profiling\n  line_profile: false            # Enable line-by-line profiling\n  gpu_profile: false             # Enable GPU profiling\n</code></pre>"},{"location":"configs/wildetect/benchmark/#parameter-descriptions","title":"Parameter Descriptions","text":""},{"location":"configs/wildetect/benchmark/#execution","title":"<code>execution</code>","text":"<p>Core benchmark execution settings using Optuna optimization framework.</p> <ul> <li><code>n_trials</code> (int): Number of optimization trials to run (more trials = better results, but slower)</li> <li><code>timeout</code> (int): Maximum time in seconds for the entire optimization process</li> <li><code>direction</code> (string): Optimization direction. Options: <code>\"minimize\"</code> (for latency), <code>\"maximize\"</code> (for throughput)</li> <li><code>sampler</code> (string): Optuna sampler algorithm. Options: <code>\"TPE\"</code> (Tree-structured Parzen Estimator), <code>\"Random\"</code>, <code>\"Grid\"</code></li> <li><code>seed</code> (int): Random seed for reproducibility</li> </ul>"},{"location":"configs/wildetect/benchmark/#test_images","title":"<code>test_images</code>","text":"<p>Test data configuration for benchmarking.</p> <ul> <li><code>path</code> (string): Path to directory containing test images</li> <li><code>recursive</code> (bool): Whether to search subdirectories recursively</li> <li><code>max_images</code> (int): Maximum number of images to use for benchmarking</li> <li><code>supported_formats</code> (list): List of supported image file extensions</li> </ul>"},{"location":"configs/wildetect/benchmark/#hyperparameters","title":"<code>hyperparameters</code>","text":"<p>Hyperparameter search space - defines which values to test for each parameter.</p> <ul> <li><code>batch_size</code> (list): List of batch sizes to test (typically powers of 2: 8, 16, 32, 64, ...)</li> <li><code>num_workers</code> (list): List of worker counts to test</li> <li><code>tile_size</code> (list): List of tile sizes to test (in pixels)</li> <li><code>overlap_ratio</code> (list): List of overlap ratios to test (0.0-1.0)</li> </ul>"},{"location":"configs/wildetect/benchmark/#output","title":"<code>output</code>","text":"<p>Output configuration for benchmark results.</p> <ul> <li><code>directory</code> (string): Output directory for benchmark results</li> <li><code>save_plots</code> (bool): Whether to save performance visualization plots</li> <li><code>save_results</code> (bool): Whether to save detailed results</li> <li><code>format</code> (string): Output format. Options: <code>\"json\"</code>, <code>\"csv\"</code>, <code>\"both\"</code></li> <li><code>include_optimization_history</code> (bool): Whether to include full optimization history</li> <li><code>auto_open</code> (bool): Whether to automatically open results after completion</li> </ul>"},{"location":"configs/wildetect/benchmark/#model","title":"<code>model</code>","text":"<p>Model configuration (can use environment variables).</p> <ul> <li><code>mlflow_model_name</code> (string, optional): Model name. If <code>null</code>, uses environment variable</li> <li><code>mlflow_model_alias</code> (string, optional): Model alias. If <code>null</code>, uses environment variable</li> <li><code>device</code> (string): Device for inference. Options: <code>\"auto\"</code>, <code>\"cpu\"</code>, <code>\"cuda\"</code></li> </ul>"},{"location":"configs/wildetect/benchmark/#processing","title":"<code>processing</code>","text":"<p>Default processing configuration (used as baseline).</p> <ul> <li><code>tile_size</code> (int): Default tile size</li> <li><code>overlap_ratio</code> (float): Default overlap ratio</li> <li><code>pipeline_type</code> (string): Pipeline type. Options: <code>\"single\"</code>, <code>\"multi\"</code>, <code>\"async\"</code></li> <li><code>queue_size</code> (int): Queue size for multi-threaded pipeline</li> <li><code>batch_size</code> (int): Default batch size</li> <li><code>num_workers</code> (int): Default number of workers</li> <li><code>max_concurrent</code> (int): Maximum concurrent inference tasks</li> </ul>"},{"location":"configs/wildetect/benchmark/#flight_specs","title":"<code>flight_specs</code>","text":"<p>Flight specifications (for geographic calculations if needed).</p> <ul> <li><code>sensor_height</code> (float): Camera sensor height in millimeters</li> <li><code>focal_length</code> (float): Lens focal length in millimeters</li> <li><code>flight_height</code> (float): Flight altitude in meters</li> </ul>"},{"location":"configs/wildetect/benchmark/#inference_service-logging-profiling","title":"<code>inference_service</code>, <code>logging</code>, <code>profiling</code>","text":"<p>Same as detection configuration. See Detection Config for details.</p>"},{"location":"configs/wildetect/benchmark/#example-configurations","title":"Example Configurations","text":""},{"location":"configs/wildetect/benchmark/#quick-benchmark-few-trials","title":"Quick Benchmark (Few Trials)","text":"<pre><code>execution:\n  n_trials: 10\n  timeout: 1800\n  direction: \"minimize\"\n  sampler: \"TPE\"\n  seed: 42\n\ntest_images:\n  path: \"test_images\"\n  recursive: true\n  max_images: 20\n\nhyperparameters:\n  batch_size: [8, 16, 32, 64]\n  num_workers: [0, 2, 4]\n  tile_size: [400, 800, 1200]\n\noutput:\n  directory: \"results/quick_benchmark\"\n  save_plots: true\n  format: \"json\"\n</code></pre>"},{"location":"configs/wildetect/benchmark/#comprehensive-benchmark","title":"Comprehensive Benchmark","text":"<pre><code>execution:\n  n_trials: 50\n  timeout: 7200  # 2 hours\n  direction: \"minimize\"\n  sampler: \"TPE\"\n  seed: 42\n\ntest_images:\n  path: \"D:/benchmark_images/\"\n  recursive: true\n  max_images: 200\n\nhyperparameters:\n  batch_size: [4, 8, 16, 32, 64, 128]\n  num_workers: [0, 2, 4, 8, 16]\n  tile_size: [400, 600, 800, 1000, 1200]\n  overlap_ratio: [0.1, 0.2, 0.3, 0.4]\n\noutput:\n  directory: \"results/comprehensive_benchmark\"\n  save_plots: true\n  save_results: true\n  format: \"both\"\n  include_optimization_history: true\n</code></pre>"},{"location":"configs/wildetect/benchmark/#gpu-specific-benchmark","title":"GPU-Specific Benchmark","text":"<pre><code>execution:\n  n_trials: 30\n  direction: \"maximize\"  # Maximize throughput\n  sampler: \"TPE\"\n\ntest_images:\n  path: \"test_images\"\n  max_images: 100\n\nhyperparameters:\n  batch_size: [16, 32, 64, 128, 256]  # Larger batches for GPU\n  num_workers: [2, 4, 8]\n  tile_size: [800, 1200, 1600]\n\nmodel:\n  device: \"cuda\"\n\nprocessing:\n  pipeline_type: \"multi\"\n  pin_memory: true\n</code></pre>"},{"location":"configs/wildetect/benchmark/#cpu-only-benchmark","title":"CPU-Only Benchmark","text":"<pre><code>execution:\n  n_trials: 20\n  direction: \"minimize\"\n  sampler: \"Random\"\n\ntest_images:\n  path: \"test_images\"\n  max_images: 50\n\nhyperparameters:\n  batch_size: [1, 2, 4, 8]  # Smaller batches for CPU\n  num_workers: [0, 1, 2, 4]\n  tile_size: [400, 600, 800]\n\nmodel:\n  device: \"cpu\"\n\nprocessing:\n  pipeline_type: \"single\"\n</code></pre>"},{"location":"configs/wildetect/benchmark/#best-practices","title":"Best Practices","text":"<ol> <li>Number of Trials: </li> <li>Start with 10-20 trials for quick results</li> <li>Use 30-50 trials for comprehensive optimization</li> <li> <p>More trials = better results but slower</p> </li> <li> <p>Test Images:</p> </li> <li>Use representative images from your actual use case</li> <li>Include variety in image sizes and content</li> <li> <p>Don't use too many images (50-100 is usually sufficient)</p> </li> <li> <p>Hyperparameter Ranges:</p> </li> <li>Start with wide ranges, then narrow based on results</li> <li>Consider hardware constraints (GPU memory, CPU cores)</li> <li> <p>Test powers of 2 for batch sizes (8, 16, 32, 64)</p> </li> <li> <p>Optimization Direction:</p> </li> <li>Use <code>\"minimize\"</code> to optimize for latency (faster processing)</li> <li> <p>Use <code>\"maximize\"</code> to optimize for throughput (more images/second)</p> </li> <li> <p>Sampler Selection:</p> </li> <li>Use <code>\"TPE\"</code> for most cases (efficient exploration)</li> <li>Use <code>\"Random\"</code> for quick baseline</li> <li> <p>Use <code>\"Grid\"</code> for exhaustive search (only with few parameters)</p> </li> <li> <p>Output Analysis:</p> </li> <li>Enable <code>save_plots</code> to visualize performance</li> <li>Use <code>format: \"both\"</code> to get JSON and CSV outputs</li> <li> <p>Review optimization history to understand parameter relationships</p> </li> <li> <p>Reproducibility:</p> </li> <li>Set <code>seed</code> for reproducible results</li> <li>Save results for comparison across runs</li> <li>Document optimal parameters found</li> </ol>"},{"location":"configs/wildetect/benchmark/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configs/wildetect/benchmark/#benchmark-takes-too-long","title":"Benchmark Takes Too Long","text":"<p>Issue: Optimization runs for hours without completing</p> <p>Solutions: 1. Reduce <code>n_trials</code> (try 10-20 instead of 50+) 2. Set <code>timeout</code> to limit maximum time 3. Reduce <code>max_images</code> in test data 4. Narrow hyperparameter search space 5. Use <code>sampler: \"Random\"</code> for faster sampling</p>"},{"location":"configs/wildetect/benchmark/#out-of-memory-during-benchmark","title":"Out of Memory During Benchmark","text":"<p>Issue: Memory errors when testing large batch sizes</p> <p>Solutions: 1. Remove large batch sizes from <code>hyperparameters.batch_size</code> 2. Reduce <code>max_images</code> in test data 3. Test smaller tile sizes first 4. Close other applications 5. Use CPU device if GPU memory is limited</p>"},{"location":"configs/wildetect/benchmark/#no-improvement-found","title":"No Improvement Found","text":"<p>Issue: Benchmark doesn't find better parameters</p> <p>Solutions: 1. Increase <code>n_trials</code> for more exploration 2. Widen hyperparameter ranges 3. Check if baseline configuration is already optimal 4. Verify test images are representative 5. Try different sampler (<code>\"TPE\"</code> vs <code>\"Random\"</code>)</p>"},{"location":"configs/wildetect/benchmark/#results-not-saved","title":"Results Not Saved","text":"<p>Issue: Benchmark completes but no output files</p> <p>Solutions: 1. Verify <code>output.directory</code> exists or can be created 2. Check <code>save_results: true</code> is enabled 3. Check file permissions for output directory 4. Review logs for error messages 5. Ensure sufficient disk space</p>"},{"location":"configs/wildetect/benchmark/#inconsistent-results","title":"Inconsistent Results","text":"<p>Issue: Results vary between benchmark runs</p> <p>Solutions: 1. Set <code>seed</code> for reproducibility 2. Use same test images across runs 3. Ensure consistent hardware state (no other processes) 4. Use fixed model version (not \"latest\" alias) 5. Run multiple trials and average results</p>"},{"location":"configs/wildetect/benchmark/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Overview</li> <li>Detection Config</li> <li>Profiling Script</li> <li>Performance Optimization</li> </ul>"},{"location":"configs/wildetect/census/","title":"Census Configuration","text":"<p>Location: <code>config/census.yaml</code></p> <p>Purpose: Configuration file for wildlife census campaigns. Extends detection configuration with campaign-specific settings, analysis options, and export configurations for generating census reports and statistics.</p>"},{"location":"configs/wildetect/census/#configuration-structure","title":"Configuration Structure","text":""},{"location":"configs/wildetect/census/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code># Census Campaign Configuration\n# This file contains all parameters needed for the census command\n\n# Campaign Configuration\ncampaign:\n  id: \"Sabie_granite\"\n  pilot_name: \"Unknown\"\n  target_species: null  # List of target species for detection\n\n# Detection Configuration (inherits from detection.yaml)\ndetection:\n  # list of image paths\n  image_paths: null\n\n  # or image directory path\n  image_dir: null\n\n  # or exif gps update configuration\n  exif_gps_update:\n    image_folder: D:\\PhD\\Harvard Kruger River Surveys\\20231012_SabieRiver_GraniteB\\cam0\n    csv_path: D:\\PhD\\Harvard Kruger River Surveys\\20231012_SabieRiver_GraniteB\\SE_SabieRiver_GraniteB_Rectimage.csv\n    skip_rows: 4\n    filename_col: \"filename\"\n    lat_col: \"latitude\"\n    lon_col: \"longitude\"\n    alt_col: \"altitude\"\n\n  # Model Configuration\n  model:\n    mlflow_model_name: \"detector\"\n    mlflow_model_alias: \"by-paul\"\n    device: \"cuda\"  # auto, cpu, cuda\n\n  # Processing Configuration\n  processing:\n    batch_size: 8\n    tile_size: 800\n    overlap_ratio: 0.2\n    pipeline_type: \"mt\"  # default, multi, async, mt, mt_simple, simple, raster\n    queue_size: 64\n    num_data_workers: 1\n    num_inference_workers: 1\n    pin_memory: true\n    prefetch_factor: 2\n    nms_threshold: 0.5\n    max_errors: 5\n\n  # Label Studio Configuration\n  labelstudio:\n    url: null\n    api_key: null\n    project_id: null\n    download_resources: false\n\n  # Flight Specifications\n  flight_specs:\n    sensor_height: 15.6  # mm\n    focal_length: 16.0   # mm\n    flight_height: 120.0  # meters\n    gsd: null # cm/px  *MANDATORY FOR RASTER DETECTION*\n\n  # Inference Service Configuration\n  inference_service:\n    url: null\n    timeout: 60\n\n  # Profiling Configuration\n  profiling:\n    enable: false\n    memory_profile: false\n    line_profile: false\n    gpu_profile: false\n\n# Export Configuration\nexport:\n  to_fiftyone: false\n  create_map: true\n  output_directory: null  # Will use campaign_id if null\n  export_to_labelstudio: true\n\n# Logging Configuration\nlogging:\n  verbose: false\n  log_file: null  # Will use default log path if null\n</code></pre>"},{"location":"configs/wildetect/census/#parameter-descriptions","title":"Parameter Descriptions","text":""},{"location":"configs/wildetect/census/#campaign","title":"<code>campaign</code>","text":"<p>Campaign metadata and identification.</p> <ul> <li><code>id</code> (string): Unique campaign identifier (e.g., \"Sabie_granite\", \"Summer_2024\")</li> <li><code>pilot_name</code> (string, optional): Name of the pilot or survey operator</li> <li><code>target_species</code> (list, optional): List of target species for detection (e.g., <code>[\"elephant\", \"giraffe\", \"zebra\"]</code>)</li> </ul>"},{"location":"configs/wildetect/census/#detection","title":"<code>detection</code>","text":"<p>Detection configuration (same structure as <code>detection.yaml</code>). See Detection Config for detailed parameter descriptions.</p> <p>Key parameters: - <code>image_paths</code>, <code>image_dir</code>, or <code>exif_gps_update</code>: Input source configuration - <code>model</code>: Model configuration for detection - <code>processing</code>: Processing pipeline settings - <code>flight_specs</code>: Flight and camera specifications - <code>labelstudio</code>: Label Studio integration (optional) - <code>inference_service</code>: External inference service (optional) - <code>profiling</code>: Performance profiling options</p>"},{"location":"configs/wildetect/census/#export","title":"<code>export</code>","text":"<p>Export and output configuration for census results.</p> <ul> <li><code>to_fiftyone</code> (bool): Whether to export results to FiftyOne dataset</li> <li><code>create_map</code> (bool): Whether to create geographic visualization maps</li> <li><code>output_directory</code> (string, optional): Output directory for census results. If <code>null</code>, uses <code>campaign.id</code></li> <li><code>export_to_labelstudio</code> (bool): Whether to export detections to Label Studio for review</li> </ul>"},{"location":"configs/wildetect/census/#logging","title":"<code>logging</code>","text":"<p>Logging configuration.</p> <ul> <li><code>verbose</code> (bool): Enable verbose logging</li> <li><code>log_file</code> (string, optional): Log file path. If <code>null</code>, uses default log path</li> </ul>"},{"location":"configs/wildetect/census/#example-configurations","title":"Example Configurations","text":""},{"location":"configs/wildetect/census/#basic-census-campaign","title":"Basic Census Campaign","text":"<pre><code>campaign:\n  id: \"Summer_2024_Survey\"\n  pilot_name: \"John Doe\"\n  target_species: [\"elephant\", \"giraffe\", \"zebra\"]\n\ndetection:\n  image_dir: \"D:/census_images/2024/\"\n\n  model:\n    mlflow_model_name: \"detector\"\n    mlflow_model_alias: \"production\"\n    device: \"cuda\"\n\n  processing:\n    batch_size: 32\n    tile_size: 800\n    pipeline_type: \"mt\"\n\n  flight_specs:\n    flight_height: 120.0\n    gsd: 2.38\n\nexport:\n  to_fiftyone: true\n  create_map: true\n  export_to_labelstudio: false\n\nlogging:\n  verbose: true\n</code></pre>"},{"location":"configs/wildetect/census/#census-with-gps-update","title":"Census with GPS Update","text":"<pre><code>campaign:\n  id: \"Sabie_River_2024\"\n  pilot_name: \"Jane Smith\"\n  target_species: [\"buffalo\", \"waterbuck\", \"impala\"]\n\ndetection:\n  exif_gps_update:\n    image_folder: \"D:/survey_images/cam0\"\n    csv_path: \"D:/gps_data/survey_coordinates.csv\"\n    skip_rows: 1\n    filename_col: \"filename\"\n    lat_col: \"latitude\"\n    lon_col: \"longitude\"\n    alt_col: \"altitude\"\n\n  model:\n    mlflow_model_name: \"detector\"\n    mlflow_model_alias: \"production\"\n    device: \"cuda\"\n\n  processing:\n    batch_size: 16\n    pipeline_type: \"mt\"\n\n  flight_specs:\n    sensor_height: 15.6\n    focal_length: 16.0\n    flight_height: 120.0\n\nexport:\n  to_fiftyone: true\n  create_map: true\n  output_directory: \"census_results/sabie_river_2024\"\n  export_to_labelstudio: true\n</code></pre>"},{"location":"configs/wildetect/census/#raster-census-large-orthomosaic","title":"Raster Census (Large Orthomosaic)","text":"<pre><code>campaign:\n  id: \"Ortho_Census_2024\"\n  target_species: [\"elephant\", \"giraffe\"]\n\ndetection:\n  image_paths:\n    - \"D:/orthomosaics/large_survey.tif\"\n\n  model:\n    mlflow_model_name: \"detector\"\n    device: \"cuda\"\n\n  processing:\n    batch_size: 32\n    tile_size: 800\n    overlap_ratio: 0.2\n    pipeline_type: \"raster\"\n    nms_threshold: 0.5\n\n  flight_specs:\n    sensor_height: 24\n    focal_length: 35.0\n    flight_height: 180.0\n    gsd: 2.38  # Required for raster\n\nexport:\n  to_fiftyone: false\n  create_map: true\n  output_directory: \"census_results/ortho_2024\"\n</code></pre>"},{"location":"configs/wildetect/census/#census-with-label-studio-integration","title":"Census with Label Studio Integration","text":"<pre><code>campaign:\n  id: \"Labeled_Census_2024\"\n  target_species: [\"zebra\", \"wildebeest\"]\n\ndetection:\n  image_dir: \"D:/labeled_images/\"\n\n  model:\n    mlflow_model_name: \"detector\"\n    device: \"cuda\"\n\n  labelstudio:\n    url: \"http://localhost:8080\"\n    api_key: ${LABEL_STUDIO_API_KEY}\n    project_id: 1\n    download_resources: false\n\n  processing:\n    batch_size: 32\n    pipeline_type: \"simple\"\n\nexport:\n  to_fiftyone: true\n  create_map: true\n  export_to_labelstudio: true\n</code></pre>"},{"location":"configs/wildetect/census/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Campaign ID: Use descriptive, unique campaign IDs that include location and date (e.g., \"Sabie_River_2024_06\")</p> </li> <li> <p>Target Species: Specify <code>target_species</code> to focus analysis on specific animals and improve filtering</p> </li> <li> <p>Output Directory: Let the system use <code>campaign.id</code> as output directory, or specify a custom path</p> </li> <li> <p>Maps: Enable <code>create_map: true</code> for geographic visualization of detections</p> </li> <li> <p>FiftyOne Export: Use <code>to_fiftyone: true</code> for interactive dataset exploration and validation</p> </li> <li> <p>Label Studio: Enable <code>export_to_labelstudio: true</code> to review and correct detections</p> </li> <li> <p>GPS Data: Use <code>exif_gps_update</code> when images lack GPS metadata but you have coordinate data</p> </li> <li> <p>Flight Specs: Ensure flight specifications match actual survey parameters for accurate geographic calculations</p> </li> <li> <p>Processing: Use <code>pipeline_type: \"mt\"</code> for faster processing of multiple images</p> </li> <li> <p>GSD for Raster: MANDATORY when using raster detection - calculate from flight parameters</p> </li> </ol>"},{"location":"configs/wildetect/census/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configs/wildetect/census/#campaign-id-conflicts","title":"Campaign ID Conflicts","text":"<p>Issue: Output directory already exists with same campaign ID</p> <p>Solutions: 1. Use unique campaign IDs 2. Specify custom <code>output_directory</code> 3. Remove or rename existing output directory</p>"},{"location":"configs/wildetect/census/#no-detections-found","title":"No Detections Found","text":"<p>Issue: Census completes but finds no animals</p> <p>Solutions: 1. Check <code>target_species</code> matches class names in model 2. Verify model is appropriate for the survey area 3. Lower confidence thresholds in processing 4. Check images contain wildlife 5. Verify model alias/version is correct</p>"},{"location":"configs/wildetect/census/#map-generation-fails","title":"Map Generation Fails","text":"<p>Issue: Geographic maps not created</p> <p>Solutions: 1. Ensure GPS data is available (EXIF or CSV) 2. Verify <code>flight_specs</code> are set correctly 3. Check <code>create_map: true</code> is enabled 4. Ensure sufficient disk space 5. Verify map libraries are installed</p>"},{"location":"configs/wildetect/census/#label-studio-export-fails","title":"Label Studio Export Fails","text":"<p>Issue: Cannot export to Label Studio</p> <p>Solutions: 1. Verify Label Studio server is running 2. Check <code>labelstudio.url</code> is correct 3. Verify API key is set in <code>.env</code> file 4. Check project ID exists in Label Studio 5. Ensure network connectivity</p>"},{"location":"configs/wildetect/census/#memory-issues-with-large-campaigns","title":"Memory Issues with Large Campaigns","text":"<p>Issue: Out of memory errors during census</p> <p>Solutions: 1. Reduce <code>batch_size</code> in processing 2. Use smaller <code>tile_size</code> 3. Process images in smaller batches 4. Use <code>pipeline_type: \"simple\"</code> for lower memory 5. Close other applications</p>"},{"location":"configs/wildetect/census/#gps-coordinates-missing","title":"GPS Coordinates Missing","text":"<p>Issue: Maps show no geographic data</p> <p>Solutions: 1. Verify images have EXIF GPS data 2. Or configure <code>exif_gps_update</code> with CSV 3. Check CSV format matches expected columns 4. Verify <code>flight_specs</code> are set 5. Ensure GPS data is valid (lat/lon ranges)</p>"},{"location":"configs/wildetect/census/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Overview</li> <li>Detection Config</li> <li>Census Script</li> <li>Census Campaign Tutorial</li> <li>Visualization Config</li> </ul>"},{"location":"configs/wildetect/class_mapping/","title":"Class Mapping Configuration","text":"<p>Location: <code>config/class_mapping.json</code></p> <p>Purpose: JSON file that maps class IDs (numeric) to class names (strings) for wildlife species and other detected objects. This mapping is used throughout the detection pipeline to convert between numeric class IDs and human-readable names.</p>"},{"location":"configs/wildetect/class_mapping/#configuration-structure","title":"Configuration Structure","text":""},{"location":"configs/wildetect/class_mapping/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code>{\n  \"0\": \"Tsessebe\",\n  \"1\": \"Waterbuck\",\n  \"2\": \"buffalo\",\n  \"3\": \"bushbuck\",\n  \"4\": \"colour impala\",\n  \"5\": \"duiker\",\n  \"6\": \"giraffe\",\n  \"7\": \"impala\",\n  \"8\": \"kudu\",\n  \"9\": \"label\",\n  \"10\": \"lechwe\",\n  \"11\": \"nyala\",\n  \"12\": \"nyala(m)\",\n  \"13\": \"other\",\n  \"14\": \"other animal\",\n  \"15\": \"reedbuck\",\n  \"16\": \"roan\",\n  \"17\": \"rocks\",\n  \"18\": \"sable\",\n  \"19\": \"termite mound\",\n  \"20\": \"vegetation\",\n  \"21\": \"warthog\",\n  \"22\": \"wildebeest\",\n  \"23\": \"zebra\",\n  \"24\": \"wildlife\"\n}\n</code></pre>"},{"location":"configs/wildetect/class_mapping/#format-description","title":"Format Description","text":"<p>The class mapping is a JSON object where: - Keys: String representations of class IDs (e.g., <code>\"0\"</code>, <code>\"1\"</code>, <code>\"2\"</code>) - Values: Class names as strings (e.g., <code>\"elephant\"</code>, <code>\"giraffe\"</code>, <code>\"zebra\"</code>)</p>"},{"location":"configs/wildetect/class_mapping/#parameter-descriptions","title":"Parameter Descriptions","text":"<p>Each entry in the mapping follows the format: <pre><code>\"&lt;class_id&gt;\": \"&lt;class_name&gt;\"\n</code></pre></p> <ul> <li><code>&lt;class_id&gt;</code>: Numeric class ID used by the model (as string key)</li> <li><code>&lt;class_name&gt;</code>: Human-readable name for the class</li> </ul> <p>Common Classes in Wildlife Detection: - Wildlife species: <code>\"elephant\"</code>, <code>\"giraffe\"</code>, <code>\"zebra\"</code>, <code>\"buffalo\"</code>, <code>\"wildebeest\"</code>, <code>\"impala\"</code>, etc. - Background objects: <code>\"vegetation\"</code>, <code>\"rocks\"</code>, <code>\"termite mound\"</code>, <code>\"other\"</code> - Generic categories: <code>\"wildlife\"</code>, <code>\"other animal\"</code>, <code>\"label\"</code></p>"},{"location":"configs/wildetect/class_mapping/#example-configurations","title":"Example Configurations","text":""},{"location":"configs/wildetect/class_mapping/#minimal-class-mapping","title":"Minimal Class Mapping","text":"<pre><code>{\n  \"0\": \"elephant\",\n  \"1\": \"giraffe\",\n  \"2\": \"zebra\",\n  \"3\": \"buffalo\",\n  \"4\": \"wildebeest\"\n}\n</code></pre>"},{"location":"configs/wildetect/class_mapping/#extended-wildlife-mapping","title":"Extended Wildlife Mapping","text":"<pre><code>{\n  \"0\": \"elephant\",\n  \"1\": \"giraffe\",\n  \"2\": \"zebra\",\n  \"3\": \"buffalo\",\n  \"4\": \"wildebeest\",\n  \"5\": \"impala\",\n  \"6\": \"kudu\",\n  \"7\": \"waterbuck\",\n  \"8\": \"warthog\",\n  \"9\": \"vegetation\",\n  \"10\": \"rocks\",\n  \"11\": \"other\"\n}\n</code></pre>"},{"location":"configs/wildetect/class_mapping/#mapping-with-background-classes","title":"Mapping with Background Classes","text":"<pre><code>{\n  \"0\": \"elephant\",\n  \"1\": \"giraffe\",\n  \"2\": \"zebra\",\n  \"3\": \"buffalo\",\n  \"4\": \"vegetation\",\n  \"5\": \"rocks\",\n  \"6\": \"termite_mound\",\n  \"7\": \"other\"\n}\n</code></pre>"},{"location":"configs/wildetect/class_mapping/#best-practices","title":"Best Practices","text":"<ol> <li>Consistent Naming: Use consistent naming conventions (e.g., lowercase, snake_case, or camelCase)</li> <li>Complete Mapping: Ensure all class IDs used by your model are included in the mapping</li> <li>No Gaps: Class IDs should be sequential starting from 0 (0, 1, 2, 3, ...)</li> <li>Descriptive Names: Use clear, descriptive names that match your dataset annotations</li> <li>Case Sensitivity: Be aware that class names are case-sensitive</li> <li>Special Characters: Avoid special characters that might cause issues in file paths or URLs</li> <li>Version Control: Keep the mapping file in version control and update it when model classes change</li> </ol>"},{"location":"configs/wildetect/class_mapping/#usage-in-code","title":"Usage in Code","text":"<p>The class mapping is typically loaded and used as follows:</p> <pre><code>import json\n\n# Load class mapping\nwith open('config/class_mapping.json', 'r') as f:\n    class_mapping = json.load(f)\n\n# Convert class ID to name\nclass_id = 0\nclass_name = class_mapping[str(class_id)]  # \"Tsessebe\"\n\n# Convert name to ID (reverse lookup)\nclass_name = \"giraffe\"\nclass_id = [k for k, v in class_mapping.items() if v == class_name][0]  # \"6\"\n</code></pre>"},{"location":"configs/wildetect/class_mapping/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configs/wildetect/class_mapping/#class-id-not-found","title":"Class ID Not Found","text":"<p>Issue: Error when looking up class ID in mapping</p> <p>Solutions: 1. Verify class ID exists in the mapping 2. Ensure class IDs are strings (JSON keys are always strings) 3. Check for typos in class ID 4. Add missing class IDs to the mapping</p>"},{"location":"configs/wildetect/class_mapping/#class-name-mismatch","title":"Class Name Mismatch","text":"<p>Issue: Class names don't match between mapping and annotations</p> <p>Solutions: 1. Verify class names match exactly (case-sensitive) 2. Check for whitespace or special characters 3. Ensure mapping matches the training dataset class names 4. Update mapping to match your dataset</p>"},{"location":"configs/wildetect/class_mapping/#missing-classes","title":"Missing Classes","text":"<p>Issue: Model predicts classes not in the mapping</p> <p>Solutions: 1. Add missing class IDs and names to the mapping 2. Verify model was trained with the same class set 3. Check if class IDs are zero-indexed (starting from 0) 4. Ensure mapping covers all model output classes</p>"},{"location":"configs/wildetect/class_mapping/#json-parse-error","title":"JSON Parse Error","text":"<p>Issue: Cannot parse class_mapping.json</p> <p>Solutions: 1. Validate JSON syntax using a JSON validator 2. Check for trailing commas 3. Ensure all keys and values are properly quoted 4. Verify file encoding is UTF-8</p>"},{"location":"configs/wildetect/class_mapping/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Overview</li> <li>Detection Config</li> <li>Census Config</li> <li>Model Training</li> </ul>"},{"location":"configs/wildetect/config/","title":"Main Configuration","text":"<p>Location: <code>config/config.yaml</code></p> <p>Purpose: Main application configuration file that defines model paths, processing settings, and MLflow tracking configuration for WildDetect.</p>"},{"location":"configs/wildetect/config/#configuration-structure","title":"Configuration Structure","text":""},{"location":"configs/wildetect/config/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code>classifier:\n  weights: config/classifier.ckpt\n  processing:\n    batch_size: 8\n    export_format: torchscript\nlocalizer:\n  yolo:\n    weights: D:\\workspace\\repos\\wildetect\\config\\localizer.pt\n    imgsz: 800\n    device: cpu\n    conf_thres: 0.1\n    iou_thres: 0.3\n    max_det: 300\n    overlap_metric: IOU\n    task: detect\n  mmdet: null\n  processing:\n    export_format: pt\n    batch_size: 32\n    dynamic: false\nprocessing:\n  name: detector\n  mlflow_tracking_uri: http://localhost:5000\n</code></pre>"},{"location":"configs/wildetect/config/#parameter-descriptions","title":"Parameter Descriptions","text":""},{"location":"configs/wildetect/config/#classifier","title":"<code>classifier</code>","text":"<p>Configuration for the classifier model used in the detection pipeline.</p> <ul> <li><code>weights</code> (string): Path to the classifier model weights file (<code>.ckpt</code> format)</li> <li><code>processing.batch_size</code> (int): Batch size used for classifier inference or export</li> <li><code>processing.export_format</code> (string): Export format for the classifier model. Options: <code>torchscript</code>, <code>onnx</code></li> </ul>"},{"location":"configs/wildetect/config/#localizer","title":"<code>localizer</code>","text":"<p>Configuration for the localizer (detector) model. Supports both YOLO and MMDetection frameworks.</p> <ul> <li><code>yolo.weights</code> (string): Path to YOLO model weights file (<code>.pt</code> format)</li> <li><code>yolo.imgsz</code> (int): Input image size for YOLO inference (pixels)</li> <li><code>yolo.device</code> (string): Device to run inference on. Options: <code>cpu</code>, <code>cuda</code>, <code>auto</code></li> <li><code>yolo.conf_thres</code> (float): Confidence threshold for detections (0.0-1.0)</li> <li><code>yolo.iou_thres</code> (float): IoU threshold for Non-Maximum Suppression (0.0-1.0)</li> <li><code>yolo.max_det</code> (int): Maximum number of detections per image</li> <li><code>yolo.overlap_metric</code> (string): Metric used for overlap calculation. Options: <code>IOU</code>, <code>DIOU</code>, <code>CIOU</code></li> <li><code>yolo.task</code> (string): Task type. Options: <code>detect</code>, <code>segment</code>, <code>classify</code></li> <li><code>mmdet</code> (null or dict): MMDetection model configuration (currently not used, set to <code>null</code>)</li> <li><code>processing.export_format</code> (string): Export format for localizer. Options: <code>pt</code>, <code>torchscript</code>, <code>onnx</code></li> <li><code>processing.batch_size</code> (int): Batch size for localizer processing</li> <li><code>processing.dynamic</code> (bool): Whether to use dynamic batch sizing</li> </ul>"},{"location":"configs/wildetect/config/#processing","title":"<code>processing</code>","text":"<p>General processing configuration.</p> <ul> <li><code>name</code> (string): Name identifier for the processing pipeline</li> <li><code>mlflow_tracking_uri</code> (string): MLflow tracking server URI for model registry and experiment tracking</li> </ul>"},{"location":"configs/wildetect/config/#example-configurations","title":"Example Configurations","text":""},{"location":"configs/wildetect/config/#basic-configuration","title":"Basic Configuration","text":"<pre><code>classifier:\n  weights: config/classifier.ckpt\n  processing:\n    batch_size: 8\n    export_format: torchscript\n\nlocalizer:\n  yolo:\n    weights: config/localizer.pt\n    imgsz: 800\n    device: cuda\n    conf_thres: 0.25\n    iou_thres: 0.45\n    max_det: 300\n    overlap_metric: IOU\n    task: detect\n  mmdet: null\n  processing:\n    export_format: pt\n    batch_size: 32\n    dynamic: false\n\nprocessing:\n  name: detector\n  mlflow_tracking_uri: http://localhost:5000\n</code></pre>"},{"location":"configs/wildetect/config/#cpu-only-configuration","title":"CPU-Only Configuration","text":"<pre><code>localizer:\n  yolo:\n    device: cpu\n    conf_thres: 0.1\n    iou_thres: 0.3\n    max_det: 200\n  processing:\n    batch_size: 16  # Reduced for CPU\n</code></pre>"},{"location":"configs/wildetect/config/#best-practices","title":"Best Practices","text":"<ol> <li>Model Paths: Use relative paths when possible, or absolute paths that are consistent across environments</li> <li>Device Selection: Set <code>device: \"auto\"</code> to let the system choose the best available device</li> <li>MLflow URI: Use environment variables for MLflow tracking URI in production:    <pre><code>processing:\n  mlflow_tracking_uri: ${MLFLOW_TRACKING_URI}\n</code></pre></li> <li>Batch Sizes: Adjust batch sizes based on available GPU memory. Start with smaller values and increase if memory allows</li> <li>Confidence Thresholds: Lower <code>conf_thres</code> values (0.1-0.2) for initial detection, then filter in post-processing</li> </ol>"},{"location":"configs/wildetect/config/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configs/wildetect/config/#model-not-found","title":"Model Not Found","text":"<p>Issue: Error loading model weights</p> <p>Solutions: 1. Verify the path to model weights is correct 2. Check that model files exist at the specified location 3. Ensure file permissions allow reading</p>"},{"location":"configs/wildetect/config/#mlflow-connection-failed","title":"MLflow Connection Failed","text":"<p>Issue: Cannot connect to MLflow tracking server</p> <p>Solutions: 1. Verify MLflow server is running: <code>scripts/launch_mlflow.bat</code> 2. Check <code>mlflow_tracking_uri</code> is correct 3. Verify network connectivity to MLflow server 4. Check firewall settings</p>"},{"location":"configs/wildetect/config/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Issue: GPU memory errors during inference</p> <p>Solutions: 1. Reduce <code>batch_size</code> in processing settings 2. Set <code>device: \"cpu\"</code> to use CPU instead 3. Reduce <code>imgsz</code> for smaller input images 4. Lower <code>max_det</code> to limit number of detections</p>"},{"location":"configs/wildetect/config/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Overview</li> <li>Detection Config</li> <li>Census Config</li> <li>Scripts Reference</li> </ul>"},{"location":"configs/wildetect/detection/","title":"Detection Configuration","text":"<p>Location: <code>config/detection.yaml</code></p> <p>Purpose: Main configuration file for the wildlife detection pipeline. Defines model settings, input sources, processing parameters, flight specifications, and output options for running detection on aerial imagery.</p>"},{"location":"configs/wildetect/detection/#configuration-structure","title":"Configuration Structure","text":""},{"location":"configs/wildetect/detection/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code># Model Configuration\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"by-paul\"\n  device: \"cuda\"  # auto, cpu, cuda\n\n# list of image paths\nimage_paths: \n - D:\\PhD\\Data per camp\\Orthos\\Dry season orthos\\ortho_k_1_4_5_rep_1.tif\n\n# or image directory path\nimage_dir: null\n\n# or exif gps update configuration\nexif_gps_update:\n  image_folder: null  # Path to folder containing images\n  csv_path: null  # Path to CSV file with GPS coordinates\n  skip_rows: 4  # Number of rows to skip in CSV (e.g., header row)\n  filename_col: \"filename\"  # CSV column name for filenames\n  lat_col: \"latitude\"  # CSV column name for latitude\n  lon_col: \"longitude\"  # CSV column name for longitude\n  alt_col: \"altitude\"  # CSV column name for altitude\n\n# Label Studio Configuration\nlabelstudio:\n  url: null\n  api_key: null \n  project_id: null\n  download_resources: false\n\n# Processing Configuration\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"raster\"  # mt, mp, async, mt_simple, simple, default, raster\n  queue_size: 64  # for multi-threaded pipeline\n  num_data_workers: 1\n  num_inference_workers: 1 # for multi-processing pipeline\n  prefetch_factor: 2\n  pin_memory: true\n  nms_threshold: 0.5 # NMS threshold for detections stitching\n  max_errors: 5\n\n# Flight Specifications\nflight_specs:\n  sensor_height: 24  # mm\n  focal_length: 35.0   # mm\n  flight_height: 180.0  # meters\n  gsd: 2.38 # cm/px  MANDATORY FOR RASTER DETECTION\n\n# Inference Service Configuration\ninference_service:\n  url: null #http://localhost:4141/predict  # URL for external inference service\n  timeout: 60\n\n# Profiling Configuration\nprofiling:\n  enable: false\n  memory_profile: false\n  line_profile: false\n  gpu_profile: false\n\n# Output Configuration\noutput:\n  directory: \"results-raster\"\n  dataset_name: null  # if null, fiftyone dataset upload is disabled\n\n# Logging Configuration\nlogging:\n  verbose: false\n  log_file: null  # Will use default log path if null\n</code></pre>"},{"location":"configs/wildetect/detection/#parameter-descriptions","title":"Parameter Descriptions","text":""},{"location":"configs/wildetect/detection/#model","title":"<code>model</code>","text":"<p>Model configuration for loading the detection model.</p> <ul> <li><code>mlflow_model_name</code> (string): Name of the model in MLflow registry</li> <li><code>mlflow_model_alias</code> (string): Model version alias (e.g., \"production\", \"latest\", \"by-paul\")</li> <li><code>device</code> (string): Device for inference. Options: <code>\"auto\"</code>, <code>\"cpu\"</code>, <code>\"cuda\"</code></li> </ul>"},{"location":"configs/wildetect/detection/#input-sources-choose-one","title":"Input Sources (choose one)","text":"<p>Three ways to specify input images:</p> <p>Option 1: <code>image_paths</code> - Type: list of strings - Description: List of specific image file paths to process - Example: <code>[\"path/to/image1.tif\", \"path/to/image2.tif\"]</code></p> <p>Option 2: <code>image_dir</code> - Type: string or null - Description: Directory containing images to process - Example: <code>\"D:/survey_images/\"</code></p> <p>Option 3: <code>exif_gps_update</code> - Type: dict or null - Description: Configuration for updating EXIF GPS data from CSV before processing - <code>image_folder</code> (string): Path to folder with images - <code>csv_path</code> (string): Path to CSV with GPS coordinates - <code>skip_rows</code> (int): Rows to skip in CSV (header rows) - <code>filename_col</code> (string): CSV column name for image filenames - <code>lat_col</code> (string): CSV column name for latitude - <code>lon_col</code> (string): CSV column name for longitude - <code>alt_col</code> (string): CSV column name for altitude</p>"},{"location":"configs/wildetect/detection/#labelstudio","title":"<code>labelstudio</code>","text":"<p>Label Studio integration for importing annotations.</p> <ul> <li><code>url</code> (string, optional): Label Studio server URL</li> <li><code>api_key</code> (string, optional): Label Studio API key</li> <li><code>project_id</code> (int, optional): Label Studio project ID to import from</li> <li><code>download_resources</code> (bool): Whether to download images from Label Studio</li> </ul>"},{"location":"configs/wildetect/detection/#processing","title":"<code>processing</code>","text":"<p>Processing pipeline configuration.</p> <ul> <li><code>batch_size</code> (int): Batch size for model inference</li> <li><code>tile_size</code> (int): Tile size in pixels for processing large images</li> <li><code>overlap_ratio</code> (float): Overlap ratio between tiles (0.0-1.0)</li> <li><code>pipeline_type</code> (string): Pipeline strategy. Options: <code>\"raster\"</code>, <code>\"mt\"</code> (multi-threaded), <code>\"mp\"</code> (multi-process), <code>\"async\"</code>, <code>\"mt_simple\"</code>, <code>\"simple\"</code>, <code>\"default\"</code></li> <li><code>queue_size</code> (int): Queue size for multi-threaded pipeline</li> <li><code>num_data_workers</code> (int): Number of data loading workers</li> <li><code>num_inference_workers</code> (int): Number of inference workers (for multi-processing)</li> <li><code>prefetch_factor</code> (int): Number of batches to prefetch per worker</li> <li><code>pin_memory</code> (bool): Pin memory for faster GPU transfer</li> <li><code>nms_threshold</code> (float): Non-Maximum Suppression threshold for stitching detections across tiles (0.0-1.0)</li> <li><code>max_errors</code> (int): Maximum errors before stopping processing</li> </ul>"},{"location":"configs/wildetect/detection/#flight_specs","title":"<code>flight_specs</code>","text":"<p>Flight and camera specifications for geographic calculations.</p> <ul> <li><code>sensor_height</code> (float): Camera sensor height in millimeters</li> <li><code>focal_length</code> (float): Lens focal length in millimeters</li> <li><code>flight_height</code> (float): Flight altitude in meters</li> <li><code>gsd</code> (float): Ground Sample Distance in cm/pixel. MANDATORY for raster detection</li> </ul>"},{"location":"configs/wildetect/detection/#inference_service","title":"<code>inference_service</code>","text":"<p>External inference service configuration.</p> <ul> <li><code>url</code> (string, optional): URL for external inference API (e.g., <code>\"http://localhost:4141/predict\"</code>)</li> <li><code>timeout</code> (int): Request timeout in seconds</li> </ul>"},{"location":"configs/wildetect/detection/#profiling","title":"<code>profiling</code>","text":"<p>Performance profiling options.</p> <ul> <li><code>enable</code> (bool): Enable general profiling</li> <li><code>memory_profile</code> (bool): Profile memory usage</li> <li><code>line_profile</code> (bool): Line-by-line profiling</li> <li><code>gpu_profile</code> (bool): GPU memory profiling</li> </ul>"},{"location":"configs/wildetect/detection/#output","title":"<code>output</code>","text":"<p>Output configuration.</p> <ul> <li><code>directory</code> (string): Output directory for results</li> <li><code>dataset_name</code> (string, optional): FiftyOne dataset name. If <code>null</code>, dataset upload is disabled</li> </ul>"},{"location":"configs/wildetect/detection/#logging","title":"<code>logging</code>","text":"<p>Logging configuration.</p> <ul> <li><code>verbose</code> (bool): Enable verbose logging</li> <li><code>log_file</code> (string, optional): Log file path. If <code>null</code>, uses default log path</li> </ul>"},{"location":"configs/wildetect/detection/#example-configurations","title":"Example Configurations","text":""},{"location":"configs/wildetect/detection/#basic-detection-single-image","title":"Basic Detection (Single Image)","text":"<pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nimage_paths:\n  - \"D:/images/survey_2024/image1.tif\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"simple\"\n\noutput:\n  directory: \"results/detections\"\n</code></pre>"},{"location":"configs/wildetect/detection/#raster-detection-large-geotiff","title":"Raster Detection (Large GeoTIFF)","text":"<pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nimage_paths:\n  - \"D:/orthomosaics/large_ortho.tif\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"raster\"\n  nms_threshold: 0.5\n\nflight_specs:\n  sensor_height: 24\n  focal_length: 35.0\n  flight_height: 180.0\n  gsd: 2.38  # Required for raster\n\noutput:\n  directory: \"results/raster_detections\"\n  dataset_name: \"raster_survey_2024\"\n</code></pre>"},{"location":"configs/wildetect/detection/#directory-based-detection","title":"Directory-Based Detection","text":"<pre><code>model:\n  mlflow_model_name: \"detector\"\n  device: \"auto\"\n\nimage_dir: \"D:/survey_images/2024/\"\n\nprocessing:\n  batch_size: 16\n  pipeline_type: \"mt\"  # Multi-threaded\n  queue_size: 64\n  num_data_workers: 2\n\noutput:\n  directory: \"results/batch_detections\"\n</code></pre>"},{"location":"configs/wildetect/detection/#detection-with-gps-update","title":"Detection with GPS Update","text":"<pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nexif_gps_update:\n  image_folder: \"D:/images/survey/\"\n  csv_path: \"D:/gps_coordinates.csv\"\n  skip_rows: 1\n  filename_col: \"filename\"\n  lat_col: \"latitude\"\n  lon_col: \"longitude\"\n  alt_col: \"altitude\"\n\nprocessing:\n  batch_size: 32\n  pipeline_type: \"simple\"\n\noutput:\n  directory: \"results/gps_updated_detections\"\n</code></pre>"},{"location":"configs/wildetect/detection/#multi-threaded-detection","title":"Multi-Threaded Detection","text":"<pre><code>model:\n  mlflow_model_name: \"detector\"\n  device: \"cuda\"\n\nimage_dir: \"D:/large_dataset/\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  pipeline_type: \"mt\"  # Multi-threaded\n  queue_size: 128\n  num_data_workers: 4\n  prefetch_factor: 2\n  pin_memory: true\n\noutput:\n  directory: \"results/mt_detections\"\n</code></pre>"},{"location":"configs/wildetect/detection/#best-practices","title":"Best Practices","text":"<ol> <li>Pipeline Type Selection:</li> <li>Use <code>\"raster\"</code> for large GeoTIFF files (orthomosaics)</li> <li>Use <code>\"mt\"</code> (multi-threaded) for many small images</li> <li>Use <code>\"simple\"</code> for basic single-image processing</li> <li> <p>Use <code>\"mp\"</code> (multi-process) for CPU-bound workloads</p> </li> <li> <p>Batch Size: </p> </li> <li>Start with 32, reduce if GPU memory errors occur</li> <li> <p>Increase for faster processing if memory allows</p> </li> <li> <p>Tile Size:</p> </li> <li>800 pixels is a good default</li> <li>Reduce for lower memory usage</li> <li> <p>Increase for better context (if memory allows)</p> </li> <li> <p>Overlap Ratio:</p> </li> <li>0.2 (20%) is recommended for most cases</li> <li>Increase to 0.3 for better edge detection</li> <li> <p>Decrease to 0.1 for faster processing</p> </li> <li> <p>GSD for Raster:</p> </li> <li>MANDATORY for raster detection pipeline</li> <li>Calculate from flight height, focal length, and sensor size</li> <li> <p>Critical for accurate geographic positioning</p> </li> <li> <p>NMS Threshold:</p> </li> <li>0.5 is a good default</li> <li>Lower (0.3-0.4) for more detections</li> <li> <p>Higher (0.6-0.7) for fewer, higher-confidence detections</p> </li> <li> <p>Device Selection:</p> </li> <li>Use <code>\"auto\"</code> to let the system choose</li> <li>Use <code>\"cuda\"</code> explicitly if you have GPU</li> <li>Use <code>\"cpu\"</code> for CPU-only systems</li> </ol>"},{"location":"configs/wildetect/detection/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configs/wildetect/detection/#model-not-found-in-mlflow","title":"Model Not Found in MLflow","text":"<p>Issue: Cannot load model from MLflow registry</p> <p>Solutions: 1. Verify MLflow server is running: <code>scripts/launch_mlflow.bat</code> 2. Check <code>mlflow_model_name</code> matches registry name 3. Verify <code>mlflow_model_alias</code> exists (e.g., \"production\", \"latest\") 4. Check MLflow tracking URI is correct 5. List available models: <code>mlflow models list</code></p>"},{"location":"configs/wildetect/detection/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Issue: GPU memory errors during inference</p> <p>Solutions: 1. Reduce <code>batch_size</code> (try 16, 8, or 4) 2. Reduce <code>tile_size</code> (try 640 or 512) 3. Set <code>device: \"cpu\"</code> to use CPU instead 4. Close other GPU applications 5. Use <code>pipeline_type: \"simple\"</code> for lower memory usage</p>"},{"location":"configs/wildetect/detection/#images-not-found","title":"Images Not Found","text":"<p>Issue: Cannot find input images</p> <p>Solutions: 1. Verify <code>image_paths</code> or <code>image_dir</code> paths are correct 2. Use absolute paths instead of relative paths 3. Check file permissions 4. Ensure image files exist at specified locations 5. Verify image formats are supported (TIFF, JPEG, PNG)</p>"},{"location":"configs/wildetect/detection/#raster-detection-fails","title":"Raster Detection Fails","text":"<p>Issue: Raster pipeline errors</p> <p>Solutions: 1. Ensure <code>gsd</code> is set - this is mandatory for raster detection 2. Verify input is a valid GeoTIFF file 3. Check file is not corrupted 4. Ensure sufficient disk space for processing 5. Try smaller <code>tile_size</code> if memory issues</p>"},{"location":"configs/wildetect/detection/#slow-processing","title":"Slow Processing","text":"<p>Issue: Detection is very slow</p> <p>Solutions: 1. Increase <code>batch_size</code> if memory allows 2. Use <code>pipeline_type: \"mt\"</code> for multi-threading 3. Increase <code>num_data_workers</code> for parallel data loading 4. Enable <code>pin_memory: true</code> for faster GPU transfer 5. Use GPU (<code>device: \"cuda\"</code>) instead of CPU 6. Reduce <code>tile_size</code> for faster tile processing</p>"},{"location":"configs/wildetect/detection/#detections-missing-at-tile-edges","title":"Detections Missing at Tile Edges","text":"<p>Issue: Objects at tile boundaries are not detected</p> <p>Solutions: 1. Increase <code>overlap_ratio</code> (try 0.3 or 0.4) 2. Lower <code>nms_threshold</code> to keep more detections 3. Use smaller <code>tile_size</code> for more overlap coverage 4. Check NMS stitching is working correctly</p>"},{"location":"configs/wildetect/detection/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Overview</li> <li>Census Config</li> <li>Detection Script</li> <li>End-to-End Detection Tutorial</li> <li>CLI Reference</li> </ul>"},{"location":"configs/wildetect/detector_registration/","title":"Detector Registration Configuration","text":"<p>Location: <code>config/detector_registration.yaml</code></p> <p>Purpose: Configuration file for registering trained detector and classifier models to the MLflow model registry. This file defines model paths, metadata, tags, and aliases for model versioning.</p>"},{"location":"configs/wildetect/detector_registration/#configuration-structure","title":"Configuration Structure","text":""},{"location":"configs/wildetect/detector_registration/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code>classifier:\n  weights: D:\\PhD\\workspace\\wildetect\\models-registry\\detector\\4\\artifacts\\best-v6.ckpt\n  processing:\n    batch_size: 8  # used for onnx export\n    export_format: \"torchscript\"  # torchscript or onnx\n\nlocalizer:\n  yolo:\n    weights: D:\\PhD\\workspace\\wildetect\\models-registry\\detector\\4\\artifacts\\best.pt\n    imgsz: 800\n    device: \"cpu\"\n    conf_thres: 0.1\n    iou_thres: 0.3\n    max_det: 300\n    overlap_metric: \"IOU\"\n    task: \"detect\"\n  mmdet: null\n  processing:\n    export_format: \"pt\"\n    batch_size: 128\n    dynamic: false\n\nprocessing:\n  name: \"detector\"\n  mlflow_tracking_uri: \"http://localhost:5000\"\n</code></pre>"},{"location":"configs/wildetect/detector_registration/#parameter-descriptions","title":"Parameter Descriptions","text":""},{"location":"configs/wildetect/detector_registration/#classifier","title":"<code>classifier</code>","text":"<p>Configuration for registering a classifier model.</p> <ul> <li><code>weights</code> (string): Path to the classifier model weights file (<code>.ckpt</code> format)</li> <li><code>processing.batch_size</code> (int): Batch size used during model export/conversion</li> <li><code>processing.export_format</code> (string): Target export format. Options: <code>torchscript</code>, <code>onnx</code></li> </ul>"},{"location":"configs/wildetect/detector_registration/#localizer","title":"<code>localizer</code>","text":"<p>Configuration for registering a detector/localizer model. Supports YOLO and MMDetection frameworks.</p> <ul> <li><code>yolo.weights</code> (string): Path to YOLO model weights file (<code>.pt</code> format)</li> <li><code>yolo.imgsz</code> (int): Input image size the model expects (pixels)</li> <li><code>yolo.device</code> (string): Device used for model export/testing. Options: <code>cpu</code>, <code>cuda</code></li> <li><code>yolo.conf_thres</code> (float): Default confidence threshold (0.0-1.0)</li> <li><code>yolo.iou_thres</code> (float): Default IoU threshold for NMS (0.0-1.0)</li> <li><code>yolo.max_det</code> (int): Maximum detections per image</li> <li><code>yolo.overlap_metric</code> (string): Overlap metric used. Options: <code>IOU</code>, <code>DIOU</code>, <code>CIOU</code></li> <li><code>yolo.task</code> (string): Model task type. Options: <code>detect</code>, <code>segment</code>, <code>classify</code></li> <li><code>mmdet</code> (null or dict): MMDetection configuration (set to <code>null</code> if not using MMDetection)</li> <li><code>processing.export_format</code> (string): Export format for the model. Options: <code>pt</code>, <code>torchscript</code>, <code>onnx</code></li> <li><code>processing.batch_size</code> (int): Batch size for processing</li> <li><code>processing.dynamic</code> (bool): Whether to use dynamic batch sizing</li> </ul>"},{"location":"configs/wildetect/detector_registration/#processing","title":"<code>processing</code>","text":"<p>General processing and MLflow configuration.</p> <ul> <li><code>name</code> (string): Model name in MLflow registry</li> <li><code>mlflow_tracking_uri</code> (string): MLflow tracking server URI</li> </ul>"},{"location":"configs/wildetect/detector_registration/#example-configurations","title":"Example Configurations","text":""},{"location":"configs/wildetect/detector_registration/#register-yolo-detector","title":"Register YOLO Detector","text":"<pre><code>localizer:\n  yolo:\n    weights: models/best.pt\n    imgsz: 800\n    device: \"cuda\"\n    conf_thres: 0.25\n    iou_thres: 0.45\n    max_det: 300\n    overlap_metric: \"IOU\"\n    task: \"detect\"\n  mmdet: null\n  processing:\n    export_format: \"pt\"\n    batch_size: 32\n    dynamic: false\n\nprocessing:\n  name: \"wildlife_detector_v2\"\n  mlflow_tracking_uri: \"http://localhost:5000\"\n</code></pre>"},{"location":"configs/wildetect/detector_registration/#register-classifier","title":"Register Classifier","text":"<pre><code>classifier:\n  weights: models/classifier_best.ckpt\n  processing:\n    batch_size: 8\n    export_format: \"torchscript\"\n\nprocessing:\n  name: \"wildlife_classifier\"\n  mlflow_tracking_uri: \"http://localhost:5000\"\n</code></pre>"},{"location":"configs/wildetect/detector_registration/#register-both-models","title":"Register Both Models","text":"<pre><code>classifier:\n  weights: models/classifier.ckpt\n  processing:\n    batch_size: 8\n    export_format: \"torchscript\"\n\nlocalizer:\n  yolo:\n    weights: models/detector.pt\n    imgsz: 800\n    device: \"cuda\"\n    conf_thres: 0.25\n    iou_thres: 0.45\n    max_det: 300\n    overlap_metric: \"IOU\"\n    task: \"detect\"\n  processing:\n    export_format: \"pt\"\n    batch_size: 32\n\nprocessing:\n  name: \"detector\"\n  mlflow_tracking_uri: \"http://localhost:5000\"\n</code></pre>"},{"location":"configs/wildetect/detector_registration/#best-practices","title":"Best Practices","text":"<ol> <li>Model Paths: Use absolute paths or paths relative to the project root</li> <li>Model Naming: Use descriptive names in <code>processing.name</code> that indicate model version or purpose</li> <li>Export Formats: </li> <li>Use <code>torchscript</code> for PyTorch deployment</li> <li>Use <code>onnx</code> for cross-platform deployment</li> <li>Use <code>pt</code> for PyTorch-only environments</li> <li>MLflow URI: Ensure MLflow server is running before registration</li> <li>Model Testing: Test model loading before registration to catch errors early</li> <li>Version Control: Tag models with meaningful aliases (e.g., \"production\", \"latest\", \"v1.0\")</li> </ol>"},{"location":"configs/wildetect/detector_registration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configs/wildetect/detector_registration/#model-file-not-found","title":"Model File Not Found","text":"<p>Issue: Cannot find model weights file</p> <p>Solutions: 1. Verify the path in <code>weights</code> field is correct 2. Check file permissions 3. Use absolute paths if relative paths fail 4. Ensure model file exists before registration</p>"},{"location":"configs/wildetect/detector_registration/#mlflow-connection-error","title":"MLflow Connection Error","text":"<p>Issue: Cannot connect to MLflow tracking server</p> <p>Solutions: 1. Start MLflow server: <code>scripts/launch_mlflow.bat</code> 2. Verify <code>mlflow_tracking_uri</code> is correct 3. Check network connectivity 4. Verify MLflow server is accessible from your machine</p>"},{"location":"configs/wildetect/detector_registration/#model-registration-fails","title":"Model Registration Fails","text":"<p>Issue: Registration process fails with error</p> <p>Solutions: 1. Check model file format is correct 2. Verify model can be loaded independently 3. Check MLflow server logs for detailed error messages 4. Ensure sufficient disk space for model artifacts 5. Verify MLflow database is accessible</p>"},{"location":"configs/wildetect/detector_registration/#export-format-not-supported","title":"Export Format Not Supported","text":"<p>Issue: Export format conversion fails</p> <p>Solutions: 1. Verify model framework supports the export format 2. Check required dependencies are installed (ONNX, TorchScript) 3. Try different export format 4. Ensure model is in correct state (eval mode, etc.)</p>"},{"location":"configs/wildetect/detector_registration/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Overview</li> <li>Model Registration Script</li> <li>MLflow Setup</li> <li>Main Config</li> </ul>"},{"location":"configs/wildetect/extract-gps/","title":"GPS Extraction Configuration","text":"<p>Location: <code>config/extract-gps.yaml</code></p> <p>Purpose: Configuration file for extracting GPS coordinates from images and visualizing detection results with geographic information. This config is used for the <code>extract-gps-coordinates</code> visualization command.</p>"},{"location":"configs/wildetect/extract-gps/#configuration-structure","title":"Configuration Structure","text":""},{"location":"configs/wildetect/extract-gps/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code># Visualization Configuration\n# This file contains all parameters needed for visualization commands\n\nlabelstudio:\n  url: http://localhost:8080\n  api_key: null  # &lt;&lt;&lt; TO change HERE\n  download_resources: false\n  project_id: null\n  json_path: null  # &lt;&lt;&lt; TO change HERE\n  dotenv_path: .env\n  parse_ls_config: true\n  ls_xml_config: null\n\ncsv_output_path: D:\\PhD\\Harvard Kruger River Surveys\\20231017_LetabaRiver_BasaltA\\annotations\\detections_v2.csv\ndetection_type: \"annotations\"  # annotations, predictions\n\n# Flight Specifications\nflight_specs:\n  sensor_height: 15.6  # mm\n  focal_length: 16.0   # mm\n  flight_height: 120.0  # meters\n\n# Logging Configuration\nlogging:\n  verbose: false\n  log_file: null  # Will use default log path if null\n</code></pre>"},{"location":"configs/wildetect/extract-gps/#parameter-descriptions","title":"Parameter Descriptions","text":""},{"location":"configs/wildetect/extract-gps/#labelstudio","title":"<code>labelstudio</code>","text":"<p>Label Studio integration configuration for importing annotations or predictions.</p> <ul> <li><code>url</code> (string): Label Studio server URL</li> <li><code>api_key</code> (string): Label Studio API key for authentication (set in <code>.env</code> file)</li> <li><code>download_resources</code> (bool): Whether to download images from Label Studio</li> <li><code>project_id</code> (int, optional): Label Studio project ID to import from</li> <li><code>json_path</code> (string, optional): Path to Label Studio JSON export file</li> <li><code>dotenv_path</code> (string): Path to <code>.env</code> file containing environment variables</li> <li><code>parse_ls_config</code> (bool): Whether to parse Label Studio XML configuration</li> <li><code>ls_xml_config</code> (string, optional): Path to Label Studio XML configuration file</li> </ul>"},{"location":"configs/wildetect/extract-gps/#csv_output_path","title":"<code>csv_output_path</code>","text":"<ul> <li>Type: string</li> <li>Description: Path where GPS coordinates and detection data will be exported as CSV</li> </ul>"},{"location":"configs/wildetect/extract-gps/#detection_type","title":"<code>detection_type</code>","text":"<ul> <li>Type: string</li> <li>Options: <code>\"annotations\"</code>, <code>\"predictions\"</code></li> <li>Description: Type of detections to process. Use <code>\"annotations\"</code> for ground truth data, <code>\"predictions\"</code> for model outputs</li> </ul>"},{"location":"configs/wildetect/extract-gps/#flight_specs","title":"<code>flight_specs</code>","text":"<p>Flight and camera specifications for geographic calculations.</p> <ul> <li><code>sensor_height</code> (float): Camera sensor height in millimeters</li> <li><code>focal_length</code> (float): Lens focal length in millimeters</li> <li><code>flight_height</code> (float): Flight altitude in meters</li> </ul>"},{"location":"configs/wildetect/extract-gps/#logging","title":"<code>logging</code>","text":"<p>Logging configuration.</p> <ul> <li><code>verbose</code> (bool): Enable verbose logging output</li> <li><code>log_file</code> (string, optional): Path to log file. If <code>null</code>, uses default log path</li> </ul>"},{"location":"configs/wildetect/extract-gps/#example-configurations","title":"Example Configurations","text":""},{"location":"configs/wildetect/extract-gps/#extract-gps-from-label-studio-annotations","title":"Extract GPS from Label Studio Annotations","text":"<pre><code>labelstudio:\n  url: http://localhost:8080\n  api_key: ${LABEL_STUDIO_API_KEY}  # From .env file\n  download_resources: false\n  project_id: 1\n  dotenv_path: .env\n  parse_ls_config: true\n\ncsv_output_path: results/gps_coordinates.csv\ndetection_type: \"annotations\"\n\nflight_specs:\n  sensor_height: 15.6\n  focal_length: 16.0\n  flight_height: 120.0\n\nlogging:\n  verbose: true\n  log_file: null\n</code></pre>"},{"location":"configs/wildetect/extract-gps/#extract-gps-from-predictions-file","title":"Extract GPS from Predictions File","text":"<pre><code>labelstudio:\n  json_path: results/predictions.json\n  download_resources: false\n\ncsv_output_path: results/predictions_gps.csv\ndetection_type: \"predictions\"\n\nflight_specs:\n  sensor_height: 24.0\n  focal_length: 35.0\n  flight_height: 180.0\n\nlogging:\n  verbose: false\n</code></pre>"},{"location":"configs/wildetect/extract-gps/#extract-gps-with-custom-label-studio-config","title":"Extract GPS with Custom Label Studio Config","text":"<pre><code>labelstudio:\n  url: http://localhost:8080\n  api_key: null\n  project_id: 2\n  ls_xml_config: configs/label_studio_config.xml\n  parse_ls_config: true\n\ncsv_output_path: results/annotations_gps.csv\ndetection_type: \"annotations\"\n\nflight_specs:\n  sensor_height: 15.6\n  focal_length: 16.0\n  flight_height: 120.0\n</code></pre>"},{"location":"configs/wildetect/extract-gps/#best-practices","title":"Best Practices","text":"<ol> <li>API Key Security: Store Label Studio API key in <code>.env</code> file, not in the config file</li> <li>Output Paths: Use absolute paths or paths relative to project root for <code>csv_output_path</code></li> <li>Flight Specs: Ensure flight specifications match the actual survey parameters for accurate GPS calculations</li> <li>Detection Type: Use <code>\"annotations\"</code> for ground truth data analysis, <code>\"predictions\"</code> for model evaluation</li> <li>Label Studio: If using Label Studio, ensure the server is running and accessible</li> <li>CSV Output: The output CSV will contain columns for image paths, GPS coordinates, and detection information</li> </ol>"},{"location":"configs/wildetect/extract-gps/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configs/wildetect/extract-gps/#label-studio-connection-failed","title":"Label Studio Connection Failed","text":"<p>Issue: Cannot connect to Label Studio server</p> <p>Solutions: 1. Verify Label Studio is running: <code>scripts/launch_labelstudio.bat</code> 2. Check <code>url</code> is correct (default: <code>http://localhost:8080</code>) 3. Verify API key is set correctly in <code>.env</code> file 4. Check network connectivity</p>"},{"location":"configs/wildetect/extract-gps/#api-key-not-found","title":"API Key Not Found","text":"<p>Issue: Label Studio API key error</p> <p>Solutions: 1. Set <code>LABEL_STUDIO_API_KEY</code> in <code>.env</code> file 2. Or set <code>api_key</code> directly in config (not recommended for production) 3. Verify API key has correct permissions</p>"},{"location":"configs/wildetect/extract-gps/#gps-coordinates-missing","title":"GPS Coordinates Missing","text":"<p>Issue: GPS coordinates not extracted from images</p> <p>Solutions: 1. Verify images have EXIF GPS data 2. Check image file formats support EXIF (JPEG, TIFF) 3. Ensure images were taken with GPS-enabled camera 4. Check <code>flight_specs</code> are correct for geographic calculations</p>"},{"location":"configs/wildetect/extract-gps/#csv-output-not-created","title":"CSV Output Not Created","text":"<p>Issue: CSV file not generated</p> <p>Solutions: 1. Verify <code>csv_output_path</code> directory exists or can be created 2. Check file permissions for output directory 3. Ensure input data (annotations/predictions) is valid 4. Check logs for error messages</p>"},{"location":"configs/wildetect/extract-gps/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Overview</li> <li>GPS Extraction Script</li> <li>Visualization Config</li> <li>Label Studio Setup</li> </ul>"},{"location":"configs/wildetect/visualization/","title":"Visualization Configuration","text":"<p>Location: <code>config/visualization.yaml</code></p> <p>Purpose: Configuration file for geographic visualization of detection results. Creates interactive maps showing detection locations, generates statistics, and visualizes wildlife distribution patterns.</p>"},{"location":"configs/wildetect/visualization/#configuration-structure","title":"Configuration Structure","text":""},{"location":"configs/wildetect/visualization/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code># Visualization Configuration\n# This file contains all parameters needed for visualization commands\n\nimage_dir: null\n\n# Geographic Visualization of predictions\ngeographic:\n  create_map: true\n  show_confidence: false\n  output_directory: \"visualizations\"\n  map_type: \"OpenStreetMap\"  # OpenStreetMap, folium, etc.\n  zoom_level: 12\n  center_on_data: true\n\n# Flight Specifications\nflight_specs:\n  sensor_height: 15.6  # mm\n  focal_length: 16.0   # mm\n  flight_height: 120.0  # meters\n\n# Visualization Options\nvisualization:\n  show_detections: true\n  show_footprints: true\n  show_statistics: true\n  color_by_confidence: false\n  confidence_threshold: 0.2\n\n# Output Configuration\noutput:\n  format: \"html\"  # html, png, pdf\n  include_legend: true\n  include_statistics: true\n  auto_open: false  # Open in browser automatically\n\n# Logging Configuration\nlogging:\n  verbose: false\n  log_file: null  # Will use default log path if null\n</code></pre>"},{"location":"configs/wildetect/visualization/#parameter-descriptions","title":"Parameter Descriptions","text":""},{"location":"configs/wildetect/visualization/#image_dir","title":"<code>image_dir</code>","text":"<ul> <li>Type: string or null</li> <li>Description: Directory containing images to visualize. If <code>null</code>, uses detection results from previous runs.</li> </ul>"},{"location":"configs/wildetect/visualization/#geographic","title":"<code>geographic</code>","text":"<p>Geographic visualization settings for creating maps.</p> <ul> <li><code>create_map</code> (bool): Whether to create an interactive geographic map</li> <li><code>show_confidence</code> (bool): Whether to display confidence scores on the map</li> <li><code>output_directory</code> (string): Directory where visualization outputs will be saved</li> <li><code>map_type</code> (string): Basemap provider. Options: <code>\"OpenStreetMap\"</code>, <code>\"folium\"</code>, or other supported providers</li> <li><code>zoom_level</code> (int): Default zoom level for the map (1-20, higher = more zoomed in)</li> <li><code>center_on_data</code> (bool): Whether to automatically center map on detection data</li> </ul>"},{"location":"configs/wildetect/visualization/#flight_specs","title":"<code>flight_specs</code>","text":"<p>Flight and camera specifications for accurate geographic positioning.</p> <ul> <li><code>sensor_height</code> (float): Camera sensor height in millimeters</li> <li><code>focal_length</code> (float): Lens focal length in millimeters</li> <li><code>flight_height</code> (float): Flight altitude in meters</li> </ul>"},{"location":"configs/wildetect/visualization/#visualization","title":"<code>visualization</code>","text":"<p>Visualization display options.</p> <ul> <li><code>show_detections</code> (bool): Whether to show detection markers on the map</li> <li><code>show_footprints</code> (bool): Whether to show image footprints/coverage areas</li> <li><code>show_statistics</code> (bool): Whether to display statistics overlay</li> <li><code>color_by_confidence</code> (bool): Whether to color-code detections by confidence score</li> <li><code>confidence_threshold</code> (float): Minimum confidence threshold for displaying detections (0.0-1.0)</li> </ul>"},{"location":"configs/wildetect/visualization/#output","title":"<code>output</code>","text":"<p>Output format and options.</p> <ul> <li><code>format</code> (string): Output file format. Options: <code>\"html\"</code> (interactive), <code>\"png\"</code> (static image), <code>\"pdf\"</code> (document)</li> <li><code>include_legend</code> (bool): Whether to include a legend in the visualization</li> <li><code>include_statistics</code> (bool): Whether to include statistics panel</li> <li><code>auto_open</code> (bool): Whether to automatically open the visualization in a browser after creation</li> </ul>"},{"location":"configs/wildetect/visualization/#logging","title":"<code>logging</code>","text":"<p>Logging configuration.</p> <ul> <li><code>verbose</code> (bool): Enable verbose logging output</li> <li><code>log_file</code> (string, optional): Path to log file. If <code>null</code>, uses default log path</li> </ul>"},{"location":"configs/wildetect/visualization/#example-configurations","title":"Example Configurations","text":""},{"location":"configs/wildetect/visualization/#basic-geographic-visualization","title":"Basic Geographic Visualization","text":"<pre><code>image_dir: \"results/detections/\"\n\ngeographic:\n  create_map: true\n  show_confidence: false\n  output_directory: \"visualizations\"\n  map_type: \"OpenStreetMap\"\n  zoom_level: 12\n  center_on_data: true\n\nflight_specs:\n  sensor_height: 15.6\n  focal_length: 16.0\n  flight_height: 120.0\n\nvisualization:\n  show_detections: true\n  show_footprints: true\n  show_statistics: true\n  color_by_confidence: false\n  confidence_threshold: 0.2\n\noutput:\n  format: \"html\"\n  include_legend: true\n  include_statistics: true\n  auto_open: true\n</code></pre>"},{"location":"configs/wildetect/visualization/#high-confidence-visualization","title":"High-Confidence Visualization","text":"<pre><code>geographic:\n  create_map: true\n  show_confidence: true\n  output_directory: \"visualizations/high_confidence\"\n  zoom_level: 14\n\nvisualization:\n  show_detections: true\n  show_footprints: false\n  show_statistics: true\n  color_by_confidence: true\n  confidence_threshold: 0.7  # Only high-confidence detections\n\noutput:\n  format: \"html\"\n  include_legend: true\n  auto_open: false\n</code></pre>"},{"location":"configs/wildetect/visualization/#static-map-export","title":"Static Map Export","text":"<pre><code>geographic:\n  create_map: true\n  output_directory: \"visualizations/static\"\n  map_type: \"OpenStreetMap\"\n  zoom_level: 11\n\nvisualization:\n  show_detections: true\n  show_footprints: true\n  show_statistics: false\n  confidence_threshold: 0.5\n\noutput:\n  format: \"png\"  # Static image\n  include_legend: true\n  include_statistics: false\n  auto_open: false\n</code></pre>"},{"location":"configs/wildetect/visualization/#pdf-report-generation","title":"PDF Report Generation","text":"<pre><code>geographic:\n  create_map: true\n  output_directory: \"visualizations/report\"\n  zoom_level: 10\n\nvisualization:\n  show_detections: true\n  show_footprints: true\n  show_statistics: true\n  confidence_threshold: 0.3\n\noutput:\n  format: \"pdf\"  # PDF document\n  include_legend: true\n  include_statistics: true\n  auto_open: false\n</code></pre>"},{"location":"configs/wildetect/visualization/#detailed-analysis-visualization","title":"Detailed Analysis Visualization","text":"<pre><code>geographic:\n  create_map: true\n  show_confidence: true\n  output_directory: \"visualizations/detailed\"\n  map_type: \"OpenStreetMap\"\n  zoom_level: 15  # High zoom for detail\n  center_on_data: true\n\nflight_specs:\n  sensor_height: 24.0\n  focal_length: 35.0\n  flight_height: 180.0\n\nvisualization:\n  show_detections: true\n  show_footprints: true\n  show_statistics: true\n  color_by_confidence: true\n  confidence_threshold: 0.2\n\noutput:\n  format: \"html\"\n  include_legend: true\n  include_statistics: true\n  auto_open: true\n\nlogging:\n  verbose: true\n</code></pre>"},{"location":"configs/wildetect/visualization/#best-practices","title":"Best Practices","text":"<ol> <li>Output Format Selection:</li> <li>Use <code>\"html\"</code> for interactive exploration and sharing</li> <li>Use <code>\"png\"</code> for static images in reports</li> <li> <p>Use <code>\"pdf\"</code> for formal documentation</p> </li> <li> <p>Zoom Level:</p> </li> <li>Lower zoom (8-10) for overview of large areas</li> <li>Medium zoom (12-13) for regional analysis</li> <li> <p>Higher zoom (14-16) for detailed local views</p> </li> <li> <p>Confidence Threshold:</p> </li> <li>Lower threshold (0.2-0.3) to see all detections</li> <li>Medium threshold (0.5) for balanced view</li> <li> <p>Higher threshold (0.7+) for high-confidence only</p> </li> <li> <p>Color Coding: Enable <code>color_by_confidence: true</code> to visually distinguish detection quality</p> </li> <li> <p>Statistics: Include statistics for quantitative analysis and reporting</p> </li> <li> <p>Flight Specs: Ensure flight specifications match actual survey for accurate positioning</p> </li> <li> <p>Auto-Open: Use <code>auto_open: true</code> for quick viewing, <code>false</code> for batch processing</p> </li> <li> <p>Output Directory: Organize visualizations by campaign or date for easy management</p> </li> </ol>"},{"location":"configs/wildetect/visualization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configs/wildetect/visualization/#map-not-created","title":"Map Not Created","text":"<p>Issue: Visualization completes but no map file is generated</p> <p>Solutions: 1. Verify <code>create_map: true</code> is enabled 2. Check <code>output_directory</code> exists or can be created 3. Ensure detection results contain GPS coordinates 4. Check file permissions for output directory 5. Review logs for error messages</p>"},{"location":"configs/wildetect/visualization/#no-detections-shown-on-map","title":"No Detections Shown on Map","text":"<p>Issue: Map is created but shows no detection markers</p> <p>Solutions: 1. Check <code>show_detections: true</code> is enabled 2. Verify detection results contain valid GPS coordinates 3. Lower <code>confidence_threshold</code> to show more detections 4. Check if detections are outside map bounds (adjust zoom/center) 5. Verify detection results file format is correct</p>"},{"location":"configs/wildetect/visualization/#gps-coordinates-missing","title":"GPS Coordinates Missing","text":"<p>Issue: Cannot create map because GPS data is missing</p> <p>Solutions: 1. Ensure images have EXIF GPS metadata 2. Or use detection results that include GPS coordinates 3. Verify <code>flight_specs</code> are set for geographic calculations 4. Check if GPS data was included in detection pipeline 5. Use <code>exif_gps_update</code> in detection config if GPS is in CSV</p>"},{"location":"configs/wildetect/visualization/#map-too-zoomed-inout","title":"Map Too Zoomed In/Out","text":"<p>Issue: Map zoom level is inappropriate</p> <p>Solutions: 1. Adjust <code>zoom_level</code> (lower = zoomed out, higher = zoomed in) 2. Enable <code>center_on_data: true</code> to auto-center 3. Manually set map center coordinates if needed 4. Test different zoom levels (8-16 range)</p>"},{"location":"configs/wildetect/visualization/#html-file-wont-open","title":"HTML File Won't Open","text":"<p>Issue: Generated HTML file doesn't open or display correctly</p> <p>Solutions: 1. Check browser compatibility (use modern browser) 2. Verify file path is correct 3. Check if file was fully written (file size &gt; 0) 4. Try opening in different browser 5. Check browser console for JavaScript errors</p>"},{"location":"configs/wildetect/visualization/#static-export-fails","title":"Static Export Fails","text":"<p>Issue: PNG or PDF export doesn't work</p> <p>Solutions: 1. Verify required libraries are installed (PIL, reportlab) 2. Check <code>format</code> is set correctly (<code>\"png\"</code> or <code>\"pdf\"</code>) 3. Ensure sufficient disk space 4. Check file permissions 5. Try HTML format first to verify visualization works</p>"},{"location":"configs/wildetect/visualization/#performance-issues","title":"Performance Issues","text":"<p>Issue: Visualization is slow or hangs</p> <p>Solutions: 1. Reduce number of detections (increase <code>confidence_threshold</code>) 2. Disable <code>show_footprints</code> if not needed 3. Use lower <code>zoom_level</code> for faster rendering 4. Process in smaller batches 5. Check available system memory</p>"},{"location":"configs/wildetect/visualization/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Overview</li> <li>Detection Config</li> <li>Census Config</li> <li>GPS Extraction Config</li> <li>Visualization Tutorial</li> </ul>"},{"location":"configs/wildtrain/","title":"WildTrain Configuration Reference","text":"<p>Documentation for all WildTrain configuration files used in model training.</p>"},{"location":"configs/wildtrain/#configuration-structure","title":"Configuration Structure","text":"<p>WildTrain uses Hydra for hierarchical configuration management.</p> <pre><code>configs/\n\u251c\u2500\u2500 classification/          # Classification configs\n\u2502   \u251c\u2500\u2500 classification_train.yaml\n\u2502   \u251c\u2500\u2500 classification_eval.yaml\n\u2502   \u251c\u2500\u2500 classification_sweep.yaml\n\u2502   \u2514\u2500\u2500 classification_pipeline_config.yaml\n\u251c\u2500\u2500 detection/              # Detection configs\n\u2502   \u251c\u2500\u2500 yolo_configs/\n\u2502   \u2502   \u251c\u2500\u2500 yolo.yaml\n\u2502   \u2502   \u251c\u2500\u2500 yolo_eval.yaml\n\u2502   \u2502   \u2514\u2500\u2500 data/demo.yaml\n\u2502   \u2514\u2500\u2500 mmdet_configs/\n\u2502       \u251c\u2500\u2500 mmdet.yaml\n\u2502       \u2514\u2500\u2500 [various model configs]\n\u251c\u2500\u2500 datapreparation/        # Data prep configs\n\u2502   \u251c\u2500\u2500 import-config-example.yaml\n\u2502   \u2514\u2500\u2500 savmap.yaml\n\u251c\u2500\u2500 registration/           # Model registration\n\u2502   \u251c\u2500\u2500 classifier_registration_example.yaml\n\u2502   \u2514\u2500\u2500 detector_registration_example.yaml\n\u251c\u2500\u2500 main.yaml              # Main config\n\u2514\u2500\u2500 inference.yaml         # Inference config\n</code></pre>"},{"location":"configs/wildtrain/#classification-configs","title":"Classification Configs","text":""},{"location":"configs/wildtrain/#classification_trainyaml","title":"classification_train.yaml","text":"<p>Purpose: Configure classification model training</p> <pre><code>model:\n  architecture: \"resnet50\"\n  num_classes: 10\n  pretrained: true\n  learning_rate: 0.001\n  weight_decay: 0.0001\n  dropout: 0.5\n\ndata:\n  root_data_directory: \"D:/data/roi_dataset\"\n  split: \"train\"\n  batch_size: 32\n  num_workers: 4\n  image_size: 224\n\ntraining:\n  max_epochs: 100\n  accelerator: \"gpu\"\n  devices: 1\n  precision: 16\n  gradient_clip_val: 1.0\n\ncallbacks:\n  early_stopping:\n    monitor: \"val_loss\"\n    patience: 10\n  model_checkpoint:\n    monitor: \"val_acc\"\n    mode: \"max\"\n    save_top_k: 3\n\nmlflow:\n  experiment_name: \"classification\"\n  tracking_uri: \"http://localhost:5000\"\n</code></pre>"},{"location":"configs/wildtrain/#classification_evalyaml","title":"classification_eval.yaml","text":"<pre><code>model:\n  checkpoint_path: \"checkpoints/best.ckpt\"\n  mlflow_model_name: \"classifier\"\n\ndata:\n  root_data_directory: \"D:/data/roi_dataset\"\n  split: \"test\"\n  batch_size: 64\n\nevaluation:\n  save_predictions: true\n  generate_confusion_matrix: true\n</code></pre>"},{"location":"configs/wildtrain/#detection-configs","title":"Detection Configs","text":""},{"location":"configs/wildtrain/#yolo-configuration","title":"YOLO Configuration","text":"<p>File: <code>configs/detection/yolo_configs/yolo.yaml</code></p> <pre><code>model:\n  framework: \"yolo\"\n  size: \"n\"  # n, s, m, l, x\n  pretrained: true\n\ndata:\n  data_yaml: \"D:/data/wildlife/data.yaml\"\n\ntraining:\n  epochs: 100\n  imgsz: 640\n  batch: 16\n  optimizer: \"AdamW\"\n  lr0: 0.001\n  device: 0\n\naugmentation:\n  hsv_h: 0.015\n  hsv_s: 0.7\n  flipud: 0.0\n  fliplr: 0.5\n  mosaic: 1.0\n\nmlflow:\n  experiment_name: \"yolo_detection\"\n</code></pre> <p>YOLO Data YAML: <pre><code># data/wildlife/data.yaml\ntrain: ./images/train\nval: ./images/val\nnc: 3\nnames: ['elephant', 'giraffe', 'zebra']\n</code></pre></p>"},{"location":"configs/wildtrain/#mmdetection-configuration","title":"MMDetection Configuration","text":"<p>File: <code>configs/detection/mmdet_configs/mmdet.yaml</code></p> <pre><code>model:\n  framework: \"mmdet\"\n  config_file: \"configs/detection/mmdet_configs/faster_rcnn.py\"\n\ndata:\n  data_root: \"D:/data/coco_format\"\n  ann_file_train: \"train.json\"\n  ann_file_val: \"val.json\"\n\ntraining:\n  work_dir: \"work_dirs/faster_rcnn\"\n  max_epochs: 12\n</code></pre>"},{"location":"configs/wildtrain/#registration-configs","title":"Registration Configs","text":""},{"location":"configs/wildtrain/#classifier-registration","title":"Classifier Registration","text":"<p>File: <code>configs/registration/classifier_registration_example.yaml</code></p> <pre><code>model_path: \"checkpoints/best.ckpt\"\nmodel_name: \"wildlife_classifier\"\nmodel_type: \"classifier\"\n\ndescription: \"ResNet50 classifier for wildlife ROI\"\n\ntags:\n  architecture: \"resnet50\"\n  dataset: \"wildlife_roi_v1\"\n  accuracy: \"0.95\"\n\naliases:\n  - \"production\"\n</code></pre>"},{"location":"configs/wildtrain/#detector-registration","title":"Detector Registration","text":"<p>File: <code>configs/registration/detector_registration_example.yaml</code></p> <pre><code>model_path: \"runs/detect/train/weights/best.pt\"\nmodel_name: \"wildlife_detector\"\nmodel_type: \"detector\"\n\ndescription: \"YOLO11n for aerial wildlife\"\n\ntags:\n  framework: \"yolo\"\n  map50: \"0.89\"\n\naliases:\n  - \"production\"\n</code></pre>"},{"location":"configs/wildtrain/#inference-configuration","title":"Inference Configuration","text":"<p>File: <code>configs/inference.yaml</code></p> <pre><code>server:\n  host: \"0.0.0.0\"\n  port: 8000\n  workers: 2\n\nmodel:\n  mlflow_model_name: \"wildlife_detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\ninference:\n  batch_size: 8\n  confidence_threshold: 0.5\n  nms_threshold: 0.45\n</code></pre>"},{"location":"configs/wildtrain/#main-configuration","title":"Main Configuration","text":"<p>File: <code>configs/main.yaml</code></p> <pre><code>defaults:\n  - model: yolo\n  - data: detection\n  - training: default\n\nexperiment_name: \"wildlife_detection\"\nseed: 42\n</code></pre>"},{"location":"configs/wildtrain/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":""},{"location":"configs/wildtrain/#classification_sweepyaml","title":"classification_sweep.yaml","text":"<pre><code>method: bayes  # grid, random, bayes\nmetric:\n  name: val_acc\n  goal: maximize\n\nparameters:\n  learning_rate:\n    min: 0.0001\n    max: 0.01\n  batch_size:\n    values: [16, 32, 64]\n  architecture:\n    values: [\"resnet18\", \"resnet50\"]\n</code></pre>"},{"location":"configs/wildtrain/#best-practices","title":"Best Practices","text":"<ol> <li>Use MLflow for experiment tracking</li> <li>Save checkpoints frequently</li> <li>Enable early stopping to prevent overfitting</li> <li>Use mixed precision (precision: 16) for faster training</li> <li>Version your configs with git</li> </ol>"},{"location":"configs/wildtrain/#next-steps","title":"Next Steps","text":"<ul> <li>WildTrain Scripts</li> <li>Training Tutorial</li> <li>WildTrain CLI</li> </ul>"},{"location":"getting-started/environment-setup/","title":"Environment Setup","text":"<p>This guide covers setting up your environment for working with the WildDetect monorepo, including configuration files, environment variables, and external services.</p>"},{"location":"getting-started/environment-setup/#directory-structure","title":"Directory Structure","text":"<p>Create the following directory structure for your project:</p> <pre><code>your-project/\n\u251c\u2500\u2500 wildetect/          # Main package (cloned repo)\n\u251c\u2500\u2500 data/              # Data storage root\n\u2502   \u251c\u2500\u2500 raw/           # Original data\n\u2502   \u251c\u2500\u2500 processed/     # Processed datasets\n\u2502   \u2514\u2500\u2500 exports/       # Exported datasets\n\u251c\u2500\u2500 models/            # Trained models\n\u2502   \u251c\u2500\u2500 detectors/\n\u2502   \u2514\u2500\u2500 classifiers/\n\u251c\u2500\u2500 results/           # Detection results\n\u2502   \u251c\u2500\u2500 detections/\n\u2502   \u251c\u2500\u2500 census/\n\u2502   \u2514\u2500\u2500 visualizations/\n\u2514\u2500\u2500 mlruns/            # MLflow experiment tracking\n</code></pre>"},{"location":"getting-started/environment-setup/#environment-variables","title":"Environment Variables","text":""},{"location":"getting-started/environment-setup/#create-env-file","title":"Create .env File","text":"<p>Create a <code>.env</code> file in the root directory of each package:</p>"},{"location":"getting-started/environment-setup/#wilddetect-env","title":"WildDetect <code>.env</code>","text":"<pre><code># MLflow Configuration\nMLFLOW_TRACKING_URI=http://localhost:5000\nMLFLOW_EXPERIMENT_NAME=wilddetect\nMODEL_REGISTRY_PATH=models/\n\n# Data Paths\nDATA_ROOT=D:/data/\nRESULTS_ROOT=D:/results/\n\n# Label Studio (Optional)\nLABEL_STUDIO_URL=http://localhost:8080\nLABEL_STUDIO_API_KEY=your_api_key_here\nLABEL_STUDIO_PROJECT_ID=1\n\n# FiftyOne (Optional)\nFIFTYONE_DATABASE_DIR=D:/fiftyone/\nFIFTYONE_DEFAULT_DATASET_DIR=D:/data/fiftyone/\n\n# Inference Server\nINFERENCE_SERVER_HOST=0.0.0.0\nINFERENCE_SERVER_PORT=4141\n\n# GPU Configuration\nCUDA_VISIBLE_DEVICES=0\nPYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n\n# Logging\nLOG_LEVEL=INFO\nLOG_FILE=logs/wildetect.log\n</code></pre>"},{"location":"getting-started/environment-setup/#wildata-env","title":"WilData <code>.env</code>","text":"<pre><code># API Configuration\nWILDATA_API_HOST=0.0.0.0\nWILDATA_API_PORT=8441\nWILDATA_API_DEBUG=false\n\n# Data Storage\nDATA_ROOT=D:/data/\nDVC_REMOTE_URL=s3://my-bucket/datasets  # or local path\n\n# DVC Configuration\nDVC_CACHE_DIR=D:/.dvc/cache/\n\n# Label Studio Integration\nLABEL_STUDIO_URL=http://localhost:8080\nLABEL_STUDIO_API_KEY=your_api_key_here\n\n# Processing\nMAX_WORKERS=4\nBATCH_SIZE=32\n</code></pre>"},{"location":"getting-started/environment-setup/#wildtrain-env","title":"WildTrain <code>.env</code>","text":"<pre><code># MLflow Configuration\nMLFLOW_TRACKING_URI=http://localhost:5000\nMLFLOW_EXPERIMENT_NAME=wildtrain\n\n# Training Paths\nDATA_ROOT=D:/data/\nMODEL_OUTPUT_DIR=D:/models/\nCHECKPOINT_DIR=D:/checkpoints/\n\n# Hyperparameter Tuning\nOPTUNA_STORAGE=sqlite:///optuna.db\nN_TRIALS=50\n\n# Distributed Training (Optional)\nMASTER_ADDR=localhost\nMASTER_PORT=12355\nWORLD_SIZE=1\nRANK=0\n\n# GPU Configuration\nCUDA_VISIBLE_DEVICES=0,1  # Multiple GPUs\n</code></pre>"},{"location":"getting-started/environment-setup/#loading-environment-variables","title":"Loading Environment Variables","text":"<p>The packages automatically load <code>.env</code> files when using scripts:</p> <pre><code># Scripts automatically load .env\nscripts\\run_detection.bat\n\n# Or manually in Python\nfrom dotenv import load_dotenv\nload_dotenv()\n</code></pre>"},{"location":"getting-started/environment-setup/#external-services-setup","title":"External Services Setup","text":""},{"location":"getting-started/environment-setup/#mlflow-tracking-server","title":"MLflow Tracking Server","text":"<p>MLflow is used for experiment tracking and model registry.</p>"},{"location":"getting-started/environment-setup/#1-start-mlflow-server","title":"1. Start MLflow Server","text":"<pre><code># Launch using script\nscripts\\launch_mlflow.bat\n\n# Or manually\nmlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow.db\n</code></pre>"},{"location":"getting-started/environment-setup/#2-access-mlflow-ui","title":"2. Access MLflow UI","text":"<p>Open browser to: <code>http://localhost:5000</code></p>"},{"location":"getting-started/environment-setup/#3-configure-in-code","title":"3. Configure in Code","text":"<pre><code>import mlflow\n\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"my_experiment\")\n</code></pre>"},{"location":"getting-started/environment-setup/#label-studio-optional","title":"Label Studio (Optional)","text":"<p>For data annotation and labeling.</p>"},{"location":"getting-started/environment-setup/#1-install-label-studio","title":"1. Install Label Studio","text":"<pre><code>uv pip install label-studio\n</code></pre>"},{"location":"getting-started/environment-setup/#2-start-server","title":"2. Start Server","text":"<pre><code># Launch using script\nscripts\\launch_labelstudio.bat\n\n# Or manually\nlabel-studio start --port 8080\n</code></pre>"},{"location":"getting-started/environment-setup/#3-create-project","title":"3. Create Project","text":"<ol> <li>Navigate to <code>http://localhost:8080</code></li> <li>Create new project</li> <li>Upload images</li> <li>Configure labeling interface (use provided XML configs)</li> </ol>"},{"location":"getting-started/environment-setup/#4-get-api-key","title":"4. Get API Key","text":"<ol> <li>Go to Account &amp; Settings</li> <li>Copy your API token</li> <li>Add to <code>.env</code> file</li> </ol>"},{"location":"getting-started/environment-setup/#fiftyone-dataset-visualization","title":"FiftyOne (Dataset Visualization)","text":"<p>Interactive dataset viewer and analyzer.</p>"},{"location":"getting-started/environment-setup/#1-install-fiftyone","title":"1. Install FiftyOne","text":"<pre><code>uv pip install fiftyone\n</code></pre>"},{"location":"getting-started/environment-setup/#2-launch-viewer","title":"2. Launch Viewer","text":"<pre><code># Launch using script\nscripts\\launch_fiftyone.bat\n\n# Or using CLI\nwildetect fiftyone --action launch --dataset my_dataset\n</code></pre>"},{"location":"getting-started/environment-setup/#3-configure-database","title":"3. Configure Database","text":"<pre><code># Set database directory\nfiftyone config database_dir D:/fiftyone/db\n\n# Set default dataset directory\nfiftyone config default_dataset_dir D:/data/fiftyone\n</code></pre>"},{"location":"getting-started/environment-setup/#dvc-data-version-control","title":"DVC (Data Version Control)","text":"<p>For versioning large datasets.</p>"},{"location":"getting-started/environment-setup/#1-initialize-dvc","title":"1. Initialize DVC","text":"<pre><code>cd wildata\nscripts\\dvc-setup.bat\n\n# Or manually\ndvc init\ndvc remote add -d myremote s3://my-bucket/datasets\n</code></pre>"},{"location":"getting-started/environment-setup/#2-configure-remote-storage","title":"2. Configure Remote Storage","text":"Local StorageAWS S3Google Cloud StorageAzure Blob <pre><code>dvc remote add -d local D:/dvc-storage\n</code></pre> <pre><code>dvc remote add -d s3remote s3://my-bucket/datasets\n\n# Set credentials\ndvc remote modify s3remote access_key_id YOUR_KEY\ndvc remote modify s3remote secret_access_key YOUR_SECRET\n</code></pre> <pre><code>dvc remote add -d gcs gs://my-bucket/datasets\n\n# Set credentials\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json\n</code></pre> <pre><code>dvc remote add -d azure azure://container/path\n\n# Set credentials\nexport AZURE_STORAGE_CONNECTION_STRING=your_connection_string\n</code></pre>"},{"location":"getting-started/environment-setup/#3-track-data","title":"3. Track Data","text":"<pre><code># Add data to DVC\ndvc add data/raw/\n\n# Commit changes\ngit add data/raw.dvc .gitignore\ngit commit -m \"Add raw data\"\n\n# Push to remote\ndvc push\n</code></pre>"},{"location":"getting-started/environment-setup/#configuration-files","title":"Configuration Files","text":""},{"location":"getting-started/environment-setup/#wilddetect-configurations","title":"WildDetect Configurations","text":"<p>Location: <code>config/</code></p>"},{"location":"getting-started/environment-setup/#detectionyaml","title":"detection.yaml","text":"<p>Main detection configuration. Edit based on your needs:</p> <pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  overlap_ratio: 0.2\n</code></pre> <p>See Detection Config Reference for all options.</p>"},{"location":"getting-started/environment-setup/#censusyaml","title":"census.yaml","text":"<p>Census campaign configuration:</p> <pre><code>campaign:\n  name: \"Summer_2024\"\n  target_species: [\"elephant\", \"giraffe\", \"zebra\"]\n\nflight_specs:\n  flight_height: 120.0\n  gsd: 2.38\n</code></pre> <p>See Census Config Reference.</p>"},{"location":"getting-started/environment-setup/#wildata-configurations","title":"WilData Configurations","text":"<p>Location: <code>wildata/configs/</code></p>"},{"location":"getting-started/environment-setup/#import-config-exampleyaml","title":"import-config-example.yaml","text":"<p>Dataset import configuration:</p> <pre><code>source_path: \"annotations.json\"\nsource_format: \"coco\"\ndataset_name: \"my_dataset\"\n\ntransformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n</code></pre> <p>See WilData Configs Reference for configuration details.</p>"},{"location":"getting-started/environment-setup/#wildtrain-configurations","title":"WildTrain Configurations","text":"<p>Location: <code>wildtrain/configs/</code></p>"},{"location":"getting-started/environment-setup/#training-config","title":"Training Config","text":"<p>For model training:</p> <pre><code># configs/classification/classification_train.yaml\nmodel:\n  architecture: \"resnet50\"\n  num_classes: 10\n\ntraining:\n  epochs: 100\n  batch_size: 32\n  learning_rate: 0.001\n</code></pre> <p>See WildTrain Configs Reference for configuration details.</p>"},{"location":"getting-started/environment-setup/#gpu-configuration","title":"GPU Configuration","text":""},{"location":"getting-started/environment-setup/#cuda-setup","title":"CUDA Setup","text":""},{"location":"getting-started/environment-setup/#check-cuda-availability","title":"Check CUDA Availability","text":"<pre><code>python -c \"import torch; print(f'CUDA: {torch.cuda.is_available()}')\"\npython -c \"import torch; print(f'Device: {torch.cuda.get_device_name(0)}')\"\n</code></pre>"},{"location":"getting-started/environment-setup/#set-gpu-device","title":"Set GPU Device","text":"<p>In <code>.env</code>: <pre><code>CUDA_VISIBLE_DEVICES=0  # Use first GPU\nCUDA_VISIBLE_DEVICES=0,1  # Use first two GPUs\n</code></pre></p> <p>In config files: <pre><code>device: \"cuda\"  # Use default GPU\ndevice: \"cuda:0\"  # Specific GPU\ndevice: \"cpu\"  # Force CPU\n</code></pre></p>"},{"location":"getting-started/environment-setup/#memory-management","title":"Memory Management","text":"<p>For large models or images:</p> <pre><code># In .env\nPYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n\n# Or in Python\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n</code></pre>"},{"location":"getting-started/environment-setup/#cpu-only-setup","title":"CPU-Only Setup","text":"<p>If you don't have a GPU:</p> <ol> <li>Install CPU-only PyTorch (see Installation)</li> <li>Set <code>device: \"cpu\"</code> in all config files</li> <li>Reduce batch sizes for memory efficiency</li> </ol>"},{"location":"getting-started/environment-setup/#testing-your-setup","title":"Testing Your Setup","text":""},{"location":"getting-started/environment-setup/#run-system-info","title":"Run System Info","text":"<pre><code>wildetect info\n</code></pre> <p>This will display: - Python version - Package versions - CUDA availability - GPU information - Memory available</p>"},{"location":"getting-started/environment-setup/#test-detection","title":"Test Detection","text":"<pre><code># Test with a single image\nwildetect detect test_image.jpg --model model.pt --output test_results/\n</code></pre>"},{"location":"getting-started/environment-setup/#test-data-import","title":"Test Data Import","text":"<pre><code># Test data import\nwildata import-dataset test_annotations.json --format coco --name test_dataset\n</code></pre>"},{"location":"getting-started/environment-setup/#test-training","title":"Test Training","text":"<pre><code># Test training setup\ncd wildtrain\nwildtrain train classifier -c configs/classification/classification_train.yaml --dry-run\n</code></pre>"},{"location":"getting-started/environment-setup/#ide-setup","title":"IDE Setup","text":""},{"location":"getting-started/environment-setup/#vscode-configuration","title":"VSCode Configuration","text":"<p>Create <code>.vscode/settings.json</code>:</p> <pre><code>{\n  \"python.defaultInterpreterPath\": \".venv/Scripts/python.exe\",\n  \"python.formatting.provider\": \"black\",\n  \"python.linting.enabled\": true,\n  \"python.linting.ruffEnabled\": true,\n  \"python.testing.pytestEnabled\": true,\n  \"python.testing.pytestArgs\": [\n    \"tests\",\n    \"-v\"\n  ],\n  \"files.exclude\": {\n    \"**/__pycache__\": true,\n    \"**/*.pyc\": true\n  }\n}\n</code></pre>"},{"location":"getting-started/environment-setup/#ruff-configuration","title":"Ruff Configuration","text":"<p>The project uses ruff for linting. Configuration is in <code>pyproject.toml</code>:</p> <pre><code># Run ruff on all files\nuv run ruff check src/ tests/\n\n# Auto-fix issues\nuv run ruff check --fix src/ tests/\n</code></pre>"},{"location":"getting-started/environment-setup/#directory-permissions-windows","title":"Directory Permissions (Windows)","text":"<p>Ensure you have write permissions for: - Data directories - Model directories - Results directories - Log directories</p> <p>Run PowerShell as Administrator if needed:</p> <pre><code># Grant full control to current user\nicacls \"D:\\data\" /grant %USERNAME%:F /t\n</code></pre>"},{"location":"getting-started/environment-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/environment-setup/#common-issues","title":"Common Issues","text":"MLflow server won't start <p>Check if port 5000 is already in use: <pre><code>netstat -ano | findstr :5000\n</code></pre></p> <p>Use a different port: <pre><code>mlflow server --port 5001\n</code></pre></p> DVC push fails <p>Verify remote credentials: <pre><code>dvc remote list\ndvc remote modify --local myremote access_key_id YOUR_KEY\n</code></pre></p> Out of memory errors <p>Reduce batch size and tile size: <pre><code>processing:\n  batch_size: 16  # Reduced\n  tile_size: 640  # Reduced\n</code></pre></p> Import errors <p>Verify virtual environment is activated: <pre><code>which python  # Should point to .venv\n</code></pre></p>"},{"location":"getting-started/environment-setup/#next-steps","title":"Next Steps","text":"<p>Now that your environment is set up:</p> <ol> <li>\u2705 Test your setup with the commands above</li> <li>\ud83d\udcda Follow the Quick Start Guide</li> <li>\ud83c\udfaf Try an End-to-End Detection tutorial</li> </ol> <p>Environment ready? Head to the Quick Start Guide to run your first detection!</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will help you install all three packages in the WildDetect monorepo: WilData, WildTrain, and WildDetect.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.9, 3.10, or 3.11</li> <li>Package Manager: uv (recommended) or pip</li> <li>Git: For cloning repositories</li> <li>GPU (optional): CUDA-capable GPU for faster inference and training</li> </ul>"},{"location":"getting-started/installation/#operating-system","title":"Operating System","text":"<ul> <li>Windows 10/11</li> <li>Linux (Ubuntu 20.04+ recommended)</li> <li>macOS (Intel or Apple Silicon)</li> </ul> <p>Windows Users</p> <p>This monorepo is developed and tested on Windows. All scripts use <code>.bat</code> format for Windows compatibility.</p>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":"Method 1: Install from Source (Recommended)Method 2: Install from GitHubMethod 3: Using uv sync <p>You can install packages directly from GitHub:</p> <pre><code># Install WilData\nuv pip install git+https://github.com/fadelmamar/wildata\n\n# Install WildTrain\nuv pip install git+https://github.com/fadelmamar/wildtrain\n\n# Install WildDetect\nuv pip install git+https://github.com/fadelmamar/wildetect\n</code></pre> <p>If packages have <code>uv.lock</code> files:</p> <pre><code># In each package directory\ncd wildata\nuv sync\n\ncd ../wildtrain\nuv sync\n\ncd ../\nuv sync\n</code></pre>"},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/fadelmamar/wildetect.git\ncd wildetect\n</code></pre>"},{"location":"getting-started/installation/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<p>Using <code>uv</code> (recommended): <pre><code>uv venv --python 3.10\n\n# Activate on Windows\n.venv\\Scripts\\activate\n\n# Activate on Linux/macOS\nsource .venv/bin/activate\n</code></pre></p>"},{"location":"getting-started/installation/#3-install-pytorch-gpu-or-cpu","title":"3. Install PyTorch (GPU or CPU)","text":"<p>With CUDA 11.8 (GPU): <pre><code>uv pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p> <p>CPU Only: <pre><code>uv pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n</code></pre></p>"},{"location":"getting-started/installation/#4-install-wilddetect-packages","title":"4. Install WildDetect Packages","text":"<p>Install all three packages in development mode: <pre><code># Install WilData\ncd wildata\nuv pip install -e .\ncd ..\n\n# Install WildTrain\ncd wildtrain\nuv pip install -e .\ncd ..\n\n# Install WildDetect (main package)\nuv pip install -e .\n</code></pre></p>"},{"location":"getting-started/installation/#5-install-mmdetection-optional","title":"5. Install MMDetection (Optional)","text":"<p>If you want to use MMDetection framework: <pre><code># Install OpenMMLab dependencies\nuv pip install -U openmim\nuv run mim install mmengine\n\n# Install MMCV (choose based on your setup)\n# For CPU:\nuv pip install mmcv==2.0.1 -f https://download.openmmlab.com/mmcv/dist/cpu/torch2.1/index.html\n\n# For CUDA 11.8:\nuv pip install mmcv==2.0.1 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.1/index.html\n\n# Install MMDetection\nuv run mim install mmdet\nuv pip install numpy==1.26.4\n</code></pre></p>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Verify your installation by checking package versions:</p> <pre><code># Check WilData\nwildata --version\n\n# Check WildTrain  \nwildtrain --version\n\n# Check WildDetect\nwildetect --version\n</code></pre> <p>You should also be able to import the packages in Python:</p> <pre><code>import wildata\nimport wildtrain\nimport wildetect\n\nprint(f\"WilData: {wildata.__version__}\")\nprint(f\"WildTrain: {wildtrain.__version__}\")\nprint(f\"WildDetect: {wildetect.__version__}\")\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#dvc-data-version-control","title":"DVC (Data Version Control)","text":"<p>For dataset versioning with WilData:</p> <pre><code># Basic DVC\nuv pip install \"wildata[dvc]\"\n\n# With cloud storage support\nuv pip install \"dvc[s3]\"      # AWS S3\nuv pip install \"dvc[gcs]\"     # Google Cloud Storage\nuv pip install \"dvc[azure]\"   # Azure Blob Storage\n</code></pre>"},{"location":"getting-started/installation/#label-studio-integration","title":"Label Studio Integration","text":"<p>For working with Label Studio annotations:</p> <pre><code>uv pip install label-studio-sdk\n</code></pre>"},{"location":"getting-started/installation/#fiftyone-visualization","title":"FiftyOne Visualization","text":"<p>For interactive dataset visualization:</p> <pre><code>uv pip install fiftyone\n</code></pre>"},{"location":"getting-started/installation/#gpu-setup","title":"GPU Setup","text":""},{"location":"getting-started/installation/#cuda-configuration","title":"CUDA Configuration","text":"<p>If you have an NVIDIA GPU, ensure CUDA is properly installed:</p> <ol> <li> <p>Check CUDA availability: <pre><code>python -c \"import torch; print(f'CUDA Available: {torch.cuda.is_available()}')\"\npython -c \"import torch; print(f'CUDA Version: {torch.version.cuda}')\"\n</code></pre></p> </li> <li> <p>Check GPU devices: <pre><code>python -c \"import torch; print(f'GPU Count: {torch.cuda.device_count()}')\"\npython -c \"import torch; print(f'GPU Name: {torch.cuda.get_device_name(0)}')\"\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#memory-requirements","title":"Memory Requirements","text":"Task Minimum RAM Recommended RAM GPU Memory Detection 8GB 16GB 4GB Training 16GB 32GB 8GB Large Rasters 32GB 64GB 8GB+"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"Import errors after installation <p>Make sure your virtual environment is activated: <pre><code># Windows\n.venv\\Scripts\\activate\n\n# Linux/macOS\nsource .venv/bin/activate\n</code></pre></p> CUDA out of memory <p>Reduce batch size or tile size in your configuration files: <pre><code>processing:\n  batch_size: 16  # Reduce from 32\n  tile_size: 640  # Reduce from 800\n</code></pre></p> MMDetection installation fails <p>Install dependencies in this order: 1. PyTorch 2. MMCV (matching your CUDA version) 3. MMEngine 4. MMDetection</p> uv command not found <p>Install uv package manager: <pre><code># Windows (PowerShell)\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Linux/macOS\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></p>"},{"location":"getting-started/installation/#windows-specific-issues","title":"Windows-Specific Issues","text":"<p>ProcessPool Not Supported</p> <p>On Windows, multiprocessing with <code>ProcessPoolExecutor</code> is not supported. The packages automatically use threading instead.</p>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the Troubleshooting Guide</li> <li>Search GitHub Issues</li> <li>Create a new issue with your error message and system info</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once installation is complete:</p> <ol> <li>\ud83d\udcda Set up your environment</li> <li>\ud83d\ude80 Follow the Quick Start guide</li> <li>\ud83d\udcd6 Explore tutorials</li> </ol> <p>Installation successful? Head to the Environment Setup to configure your workspace.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>Get up and running with WildDetect in minutes! This guide shows you the fastest path to running your first wildlife detection.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>\u2705 Installed all packages (Installation Guide)</li> <li>\u2705 Activated your Python environment</li> <li>\u2705 Some aerial images to process</li> <li>\u2705 A pre-trained model (or use our example)</li> </ul>"},{"location":"getting-started/quick-start/#quick-start-detection","title":"Quick Start: Detection","text":""},{"location":"getting-started/quick-start/#1-using-the-cli","title":"1. Using the CLI","text":"<p>The simplest way to run detection:</p> <pre><code>wildetect detect /path/to/images --model model.pt --output results/\n</code></pre>"},{"location":"getting-started/quick-start/#2-using-a-script-windows","title":"2. Using a Script (Windows)","text":"<p>Edit the configuration file, then run:</p> <pre><code>cd wildetect\nscripts\\run_detection.bat\n</code></pre>"},{"location":"getting-started/quick-start/#3-using-python","title":"3. Using Python","text":"<pre><code>from wildetect.core.detection import DetectionPipeline\n\n# Initialize pipeline\npipeline = DetectionPipeline(\n    model_path=\"model.pt\",\n    device=\"cuda\"  # or \"cpu\"\n)\n\n# Run detection\nresults = pipeline.detect_batch(\"/path/to/images\")\n\n# Save results\npipeline.save_results(results, \"results/detections.json\")\n</code></pre>"},{"location":"getting-started/quick-start/#quick-start-census-campaign","title":"Quick Start: Census Campaign","text":"<p>Run a complete census analysis:</p> <pre><code>wildetect census campaign_2024 /path/to/images \\\n    --model model.pt \\\n    --output campaign_results/ \\\n    --species \"elephant,giraffe,zebra\"\n</code></pre> <p>This will: - \u2705 Detect all animals in your images - \u2705 Generate population statistics - \u2705 Create geographic visualizations - \u2705 Export reports in JSON and CSV</p>"},{"location":"getting-started/quick-start/#quick-start-data-management","title":"Quick Start: Data Management","text":""},{"location":"getting-started/quick-start/#import-a-dataset","title":"Import a Dataset","text":"<pre><code># Import COCO format\nwildata import-dataset annotations.json \\\n    --format coco \\\n    --name my_dataset\n\n# Import YOLO format\nwildata import-dataset data.yaml \\\n    --format yolo \\\n    --name my_dataset\n</code></pre>"},{"location":"getting-started/quick-start/#visualize-data","title":"Visualize Data","text":"<pre><code># Launch FiftyOne viewer\nwildetect fiftyone --action launch --dataset my_dataset\n\n# Or use the script\nscripts\\launch_fiftyone.bat\n</code></pre>"},{"location":"getting-started/quick-start/#quick-start-model-training","title":"Quick Start: Model Training","text":""},{"location":"getting-started/quick-start/#train-a-classifier","title":"Train a Classifier","text":"<pre><code>cd wildtrain\nwildtrain train classifier -c configs/classification/classification_train.yaml\n</code></pre>"},{"location":"getting-started/quick-start/#train-a-detector-yolo","title":"Train a Detector (YOLO)","text":"<pre><code>wildtrain train detector -c configs/detection/yolo_configs/yolo.yaml\n</code></pre>"},{"location":"getting-started/quick-start/#using-the-web-ui","title":"Using the Web UI","text":"<p>Each package has a Streamlit-based web interface:</p>"},{"location":"getting-started/quick-start/#wilddetect-ui","title":"WildDetect UI","text":"<pre><code>wildetect ui\n# Or: scripts\\launch_ui.bat\n</code></pre> <p>Features: - Run detections interactively - Configure detection parameters - View results in real-time - Export to various formats</p>"},{"location":"getting-started/quick-start/#wildata-ui","title":"WilData UI","text":"<pre><code>cd wildata\nstreamlit run src/wildata/ui.py\n# Or: launch_ui.bat\n</code></pre> <p>Features: - Import and export datasets - Create ROI datasets - Update GPS metadata - Visualize data</p>"},{"location":"getting-started/quick-start/#wildtrain-ui","title":"WildTrain UI","text":"<pre><code>cd wildtrain\nstreamlit run src/wildtrain/ui.py\n# Or: launch_ui.bat\n</code></pre> <p>Features: - Configure training runs - Monitor training progress - Evaluate models - Register models to MLflow</p>"},{"location":"getting-started/quick-start/#configuration-files","title":"Configuration Files","text":"<p>All operations can be configured via YAML files:</p>"},{"location":"getting-started/quick-start/#detection-config-example","title":"Detection Config Example","text":"<p>Edit <code>config/detection.yaml</code>:</p> <pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"raster\"\n\noutput:\n  directory: \"results\"\n  dataset_name: \"my_detections\"\n</code></pre>"},{"location":"getting-started/quick-start/#dataset-import-config-example","title":"Dataset Import Config Example","text":"<p>Edit <code>wildata/configs/import-config-example.yaml</code>:</p> <pre><code>source_path: \"annotations.json\"\nsource_format: \"coco\"\ndataset_name: \"my_dataset\"\nroot: \"data\"\nsplit_name: \"train\"\n\ntransformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n    min_visibility: 0.7\n</code></pre>"},{"location":"getting-started/quick-start/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/quick-start/#workflow-1-detection-on-new-images","title":"Workflow 1: Detection on New Images","text":"<pre><code># 1. Run detection\nwildetect detect images/ --model model.pt --output results/\n\n# 2. View results\nwildetect fiftyone --action launch\n\n# 3. Export results\nwildetect analyze results/detections.json --output analysis/\n</code></pre>"},{"location":"getting-started/quick-start/#workflow-2-prepare-training-data","title":"Workflow 2: Prepare Training Data","text":"<pre><code># 1. Import annotations\nwildata import-dataset annotations.json --format coco --name train_data\n\n# 2. Apply transformations\nwildata import-dataset annotations.json \\\n    --format coco \\\n    --name augmented_data \\\n    --enable-tiling \\\n    --enable-augmentation\n\n# 3. Export for training\nwildata export-dataset augmented_data --format yolo\n</code></pre>"},{"location":"getting-started/quick-start/#workflow-3-train-and-deploy-model","title":"Workflow 3: Train and Deploy Model","text":"<pre><code># 1. Train model\ncd wildtrain\nwildtrain train detector -c configs/detection/yolo_configs/yolo.yaml\n\n# 2. Evaluate model\nscripts\\eval_detector.bat\n\n# 3. Register to MLflow\nscripts\\register_model.bat\n\n# 4. Use for detection\ncd ..\nwildetect detect images/ --model-name my_detector --output results/\n</code></pre>"},{"location":"getting-started/quick-start/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># MLflow Configuration\nMLFLOW_TRACKING_URI=http://localhost:5000\n\n# Label Studio (optional)\nLABEL_STUDIO_URL=http://localhost:8080\nLABEL_STUDIO_API_KEY=your_api_key\n\n# Model Storage\nMODEL_REGISTRY_PATH=models/\n\n# Data Storage\nDATA_ROOT=D:/data/\n\n# GPU Settings\nCUDA_VISIBLE_DEVICES=0\n</code></pre>"},{"location":"getting-started/quick-start/#launching-services","title":"Launching Services","text":""},{"location":"getting-started/quick-start/#mlflow-ui","title":"MLflow UI","text":"<p>Track experiments and manage models:</p> <pre><code>scripts\\launch_mlflow.bat\n# Access at http://localhost:5000\n</code></pre>"},{"location":"getting-started/quick-start/#label-studio","title":"Label Studio","text":"<p>Annotate images:</p> <pre><code>scripts\\launch_labelstudio.bat\n# Access at http://localhost:8080\n</code></pre>"},{"location":"getting-started/quick-start/#wildata-api","title":"WilData API","text":"<p>REST API for data operations:</p> <pre><code>cd wildata\nscripts\\launch_api.bat\n# Access at http://localhost:8441\n# Docs at http://localhost:8441/docs\n</code></pre>"},{"location":"getting-started/quick-start/#inference-server","title":"Inference Server","text":"<p>Deploy model as API:</p> <pre><code>scripts\\launch_inference_server.bat\n# Access at http://localhost:4141\n</code></pre>"},{"location":"getting-started/quick-start/#quick-reference","title":"Quick Reference","text":""},{"location":"getting-started/quick-start/#detection-commands","title":"Detection Commands","text":"<pre><code># Basic detection\nwildetect detect images/ --model model.pt\n\n# With tiling for large images\nwildetect detect large_image.tif --model model.pt --tile-size 800\n\n# Census with statistics\nwildetect census campaign images/ --model model.pt\n\n# Analyze results\nwildetect analyze results.json\n</code></pre>"},{"location":"getting-started/quick-start/#data-commands","title":"Data Commands","text":"<pre><code># Import\nwildata import-dataset source --format coco --name dataset\n\n# List datasets\nwildata dataset list\n\n# Export\nwildata dataset export dataset --format yolo\n\n# Create ROI dataset\nwildata create-roi annotations.json --format coco\n</code></pre>"},{"location":"getting-started/quick-start/#training-commands","title":"Training Commands","text":"<pre><code># Train classifier\nwildtrain train classifier -c config.yaml\n\n# Train detector\nwildtrain train detector -c config.yaml\n\n# Evaluate\nwildtrain eval classifier -c config.yaml\n\n# Register model\nwildtrain register model_path --name my_model\n</code></pre>"},{"location":"getting-started/quick-start/#getting-help","title":"Getting Help","text":""},{"location":"getting-started/quick-start/#command-help","title":"Command Help","text":"<p>Every command has a <code>--help</code> flag:</p> <pre><code>wildetect --help\nwildetect detect --help\nwildata import-dataset --help\nwildtrain train --help\n</code></pre>"},{"location":"getting-started/quick-start/#package-information","title":"Package Information","text":"<pre><code># System info\nwildetect info\n\n# Check installation\npython -c \"import wildetect; print(wildetect.__version__)\"\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you've run your first commands:</p> <ol> <li>\ud83d\udcd6 Deep Dive: Follow the End-to-End Detection Tutorial</li> <li>\ud83c\udfd7\ufe0f Understand Architecture: Read the Architecture Overview</li> <li>\ud83d\udd27 Configure: Explore Configuration Files</li> <li>\ud83d\udcda Learn More: Check out all Tutorials</li> </ol> <p>Questions? Check the Troubleshooting Guide or reach out via GitHub Issues.</p>"},{"location":"scripts/wildata/","title":"WilData Scripts Reference","text":"<p>This page documents all batch scripts available in the WilData package for data management operations.</p>"},{"location":"scripts/wildata/#overview","title":"Overview","text":"<p>All scripts are located in <code>wildata/scripts/</code> directory.</p>"},{"location":"scripts/wildata/#quick-reference","title":"Quick Reference","text":"Script Purpose Config File import-dataset-example.bat Import single dataset <code>configs/import-config-example.yaml</code> bulk-import-dataset.bat Bulk import datasets <code>configs/bulk-import-*.yaml</code> create-roi-dataset.bat Create ROI dataset <code>configs/roi-create-config.yaml</code> bulk-roi-create-config.bat Bulk create ROI datasets <code>configs/bulk-roi-create-config.yaml</code> update-gps-example.bat Update GPS from CSV <code>configs/gps-update-config-example.yaml</code> visualize_data.bat Visualize dataset None dvc-setup.bat Setup DVC None launch_api.bat Launch REST API <code>.env</code> running_tests.bat Run tests None"},{"location":"scripts/wildata/#data-import-scripts","title":"Data Import Scripts","text":""},{"location":"scripts/wildata/#import-dataset-examplebat","title":"import-dataset-example.bat","text":"<p>Purpose: Import a single dataset from COCO, YOLO, or Label Studio format.</p> <p>Location: <code>wildata/scripts/import-dataset-example.bat</code></p> <p>Command: <pre><code>uv run wildata import-dataset --config configs\\import-config-example.yaml\n</code></pre></p> <p>Configuration: <code>wildata/configs/import-config-example.yaml</code></p> <p>Key Parameters: <pre><code>source_path: \"path/to/annotations.json\"\nsource_format: \"coco\"  # coco, yolo, ls\ndataset_name: \"my_dataset\"\n\nroot: \"data\"\nsplit_name: \"train\"  # train, val, test\n\ntransformations:\n  enable_bbox_clipping: true\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n\nroi_config:\n  roi_box_size: 384\n  random_roi_count: 2\n</code></pre></p> <p>Example Usage: <pre><code>cd wildata\n\n# Edit config file first\nnotepad configs\\import-config-example.yaml\n\n# Run import\nscripts\\import-dataset-example.bat\n</code></pre></p> <p>Output: - Master format dataset in <code>data/datasets/</code> - Processed images (tiled if enabled) - ROI dataset (if configured)</p>"},{"location":"scripts/wildata/#bulk-import-datasetbat","title":"bulk-import-dataset.bat","text":"<p>Purpose: Import multiple datasets in batch mode.</p> <p>Location: <code>wildata/scripts/bulk-import-dataset.bat</code></p> <p>Command: <pre><code>uv run wildata bulk-import-datasets --config configs\\bulk-import-config-example.yaml -n 2\n</code></pre></p> <p>Configuration: <code>wildata/configs/bulk-import-train.yaml</code> or <code>bulk-import-val.yaml</code></p> <p>Parameters: - <code>-n 2</code>: Number of parallel workers (uses threading on Windows) - <code>--config</code>: Path to bulk import config</p> <p>Example Config: <pre><code># configs/bulk-import-train.yaml\nsource_paths:\n  - \"D:/annotations/dataset1.json\"\n  - \"D:/annotations/dataset2.json\"\n  - \"D:/annotations/dataset3.json\"\n\nsource_format: \"coco\"\nroot: \"D:/data\"\nsplit_name: \"train\"\n\n# Shared transformation settings\ntransformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n</code></pre></p> <p>Example Usage: <pre><code>cd wildata\nscripts\\bulk-import-dataset.bat\n</code></pre></p> <p>Features: - Parallel processing (thread-based) - Progress tracking - Error handling per dataset - Summary report</p>"},{"location":"scripts/wildata/#roi-dataset-scripts","title":"ROI Dataset Scripts","text":""},{"location":"scripts/wildata/#create-roi-datasetbat","title":"create-roi-dataset.bat","text":"<p>Purpose: Create Region of Interest (ROI) classification dataset from detection annotations.</p> <p>Location: <code>wildata/scripts/create-roi-dataset.bat</code></p> <p>Command: <pre><code>uv run wildata create-roi-dataset --config configs\\roi-create-config.yaml\n</code></pre></p> <p>Configuration: <code>wildata/configs/roi-create-config.yaml</code></p> <p>Key Parameters: <pre><code>source_path: \"annotations.json\"\nsource_format: \"coco\"\ndataset_name: \"roi_dataset\"\n\nroi_config:\n  roi_box_size: 128        # Size of extracted ROI\n  min_roi_size: 32         # Min object size to extract\n  random_roi_count: 10     # Background samples per image\n  background_class: \"background\"\n  save_format: \"jpg\"\n  quality: 95\n</code></pre></p> <p>Use Cases: - Hard sample mining - Error analysis - Training classification models - Creating balanced datasets</p> <p>Example Usage: <pre><code>cd wildata\nscripts\\create-roi-dataset.bat\n</code></pre></p> <p>Output: - ROI image crops - Classification labels - Class mapping JSON - Statistics file</p>"},{"location":"scripts/wildata/#bulk-roi-createbat","title":"bulk-roi-create.bat","text":"<p>Purpose: Create multiple ROI datasets in batch.</p> <p>Location: Script not shown, but referenced in configs</p> <p>Configuration: <code>wildata/configs/bulk-roi-create-config.yaml</code></p> <p>Example Config: <pre><code>source_paths:\n  - \"dataset1.json\"\n  - \"dataset2.json\"\n\nsource_format: \"coco\"\nsplit_name: \"val\"\n\nroi_config:\n  roi_box_size: 128\n  random_roi_count: 5\n</code></pre></p>"},{"location":"scripts/wildata/#gps-management-scripts","title":"GPS Management Scripts","text":""},{"location":"scripts/wildata/#update-gps-examplebat","title":"update-gps-example.bat","text":"<p>Purpose: Update image EXIF GPS data from CSV file.</p> <p>Location: <code>wildata/scripts/update-gps-example.bat</code></p> <p>Command: <pre><code>uv run wildata update-gps-from-csv --config configs\\gps-update-config-example.yaml\n</code></pre></p> <p>Configuration: <code>wildata/configs/gps-update-config-example.yaml</code></p> <p>Key Parameters: <pre><code>image_folder: \"path/to/images\"\ncsv_path: \"gps_coordinates.csv\"\noutput_dir: \"output/images\"\n\nskip_rows: 0\nfilename_col: \"filename\"\nlat_col: \"latitude\"\nlon_col: \"longitude\"\nalt_col: \"altitude\"\n</code></pre></p> <p>CSV Format: <pre><code>filename,latitude,longitude,altitude\nimage1.jpg,40.7128,-74.0060,10.5\nimage2.jpg,40.7589,-73.9851,15.2\n</code></pre></p> <p>Example Usage: <pre><code>cd wildata\n\n# Prepare CSV with GPS data\n# Edit config\nnotepad configs\\gps-update-config-example.yaml\n\n# Run update\nscripts\\update-gps-example.bat\n</code></pre></p> <p>Output: - Images with updated EXIF GPS - Summary report - Error log (if any)</p>"},{"location":"scripts/wildata/#visualization-scripts","title":"Visualization Scripts","text":""},{"location":"scripts/wildata/#visualize_databat","title":"visualize_data.bat","text":"<p>Purpose: Launch FiftyOne visualization for datasets.</p> <p>Location: <code>wildata/scripts/visualize_data.bat</code></p> <p>Command: <pre><code>uv run wildata visualize-dataset --dataset my_dataset --split train\n</code></pre></p> <p>Example Usage: <pre><code>cd wildata\n\n# Visualize training set\nuv run wildata visualize-dataset --dataset my_dataset --split train\n\n# Or use script\nscripts\\visualize_data.bat\n</code></pre></p> <p>Features: - Interactive dataset viewer - Annotation visualization - Filtering and search - Statistics display</p>"},{"location":"scripts/wildata/#dvc-scripts","title":"DVC Scripts","text":""},{"location":"scripts/wildata/#dvc-setupbat","title":"dvc-setup.bat","text":"<p>Purpose: Initialize and configure DVC for data versioning.</p> <p>Location: <code>wildata/scripts/dvc-setup.bat</code></p> <p>Command: <pre><code># Initialize DVC\ndvc init\n\n# Add remote storage\ndvc remote add -d myremote &lt;storage_path&gt;\n</code></pre></p> <p>Storage Options:</p> Local StorageAWS S3Google Cloud <pre><code>dvc remote add -d local D:\\dvc-storage\n</code></pre> <pre><code>dvc remote add -d s3remote s3://bucket/path\ndvc remote modify s3remote access_key_id YOUR_KEY\ndvc remote modify s3remote secret_access_key YOUR_SECRET\n</code></pre> <pre><code>dvc remote add -d gcs gs://bucket/path\nset GOOGLE_APPLICATION_CREDENTIALS=path\\to\\credentials.json\n</code></pre> <p>Example Usage: <pre><code>cd wildata\nscripts\\dvc-setup.bat\n\n# Track data\ndvc add data\\datasets\\my_dataset\n\n# Commit DVC file\ngit add data\\datasets\\my_dataset.dvc\ngit commit -m \"Add dataset\"\n\n# Push to remote\ndvc push\n</code></pre></p> <p>DVC Workflow: <pre><code># On another machine\ngit pull\ndvc pull  # Downloads data\n</code></pre></p>"},{"location":"scripts/wildata/#api-scripts","title":"API Scripts","text":""},{"location":"scripts/wildata/#launch_apibat","title":"launch_api.bat","text":"<p>Purpose: Launch WilData REST API server.</p> <p>Location: <code>wildata/scripts/launch_api.bat</code></p> <p>Command: <pre><code>uv run python -m wildata.api.main\n</code></pre></p> <p>Default Port: 8441</p> <p>Example Usage: <pre><code>cd wildata\nscripts\\launch_api.bat\n</code></pre></p> <p>Access: - API: <code>http://localhost:8441</code> - Docs: <code>http://localhost:8441/docs</code> - Redoc: <code>http://localhost:8441/redoc</code></p> <p>API Endpoints:</p>"},{"location":"scripts/wildata/#import-dataset","title":"Import Dataset","text":"<pre><code>POST /api/v1/datasets/import\nContent-Type: application/json\n\n{\n  \"source_path\": \"/path/to/data.json\",\n  \"source_format\": \"coco\",\n  \"dataset_name\": \"my_dataset\",\n  \"root\": \"data\"\n}\n</code></pre>"},{"location":"scripts/wildata/#list-datasets","title":"List Datasets","text":"<pre><code>GET /api/v1/datasets?root=data\n</code></pre>"},{"location":"scripts/wildata/#create-roi-dataset","title":"Create ROI Dataset","text":"<pre><code>POST /api/v1/roi/create\nContent-Type: application/json\n\n{\n  \"source_path\": \"/path/to/data.json\",\n  \"source_format\": \"coco\",\n  \"dataset_name\": \"roi_dataset\",\n  \"roi_config\": {\n    \"roi_box_size\": 128,\n    \"random_roi_count\": 10\n  }\n}\n</code></pre>"},{"location":"scripts/wildata/#job-status","title":"Job Status","text":"<pre><code>GET /api/v1/jobs/{job_id}\n</code></pre> <p>Environment Variables: <pre><code># In .env\nWILDATA_API_HOST=0.0.0.0\nWILDATA_API_PORT=8441\nWILDATA_API_DEBUG=false\n</code></pre></p> <p>Full API Documentation: See WilData API Reference</p>"},{"location":"scripts/wildata/#testing-scripts","title":"Testing Scripts","text":""},{"location":"scripts/wildata/#running_testsbat","title":"running_tests.bat","text":"<p>Purpose: Run WilData test suite.</p> <p>Location: <code>wildata/scripts/running_tests.bat</code></p> <p>Command: <pre><code>uv run pytest tests/ -v\n</code></pre></p> <p>Example Usage: <pre><code>cd wildata\nscripts\\running_tests.bat\n</code></pre></p> <p>Test Categories: - Format adapter tests - Transformation tests - Validation tests - API tests - Integration tests</p> <p>Run Specific Tests: <pre><code># Test imports\nuv run pytest tests/test_coco_import.py -v\n\n# Test transformations\nuv run pytest tests/test_transformations.py -v\n\n# Test API\nuv run pytest tests/api/ -v\n\n# With coverage\nuv run pytest --cov=wildata tests/\n</code></pre></p>"},{"location":"scripts/wildata/#common-workflows","title":"Common Workflows","text":""},{"location":"scripts/wildata/#dataset-preparation-workflow","title":"Dataset Preparation Workflow","text":"<pre><code># 1. Import dataset\ncd wildata\nscripts\\import-dataset-example.bat\n\n# 2. Visualize\nscripts\\visualize_data.bat\n\n# 3. Export for training\nuv run wildata dataset export my_dataset --format yolo\n</code></pre>"},{"location":"scripts/wildata/#roi-extraction-workflow","title":"ROI Extraction Workflow","text":"<pre><code># 1. Import detection dataset\nscripts\\import-dataset-example.bat\n\n# 2. Create ROI dataset\nscripts\\create-roi-dataset.bat\n\n# 3. Visualize ROI dataset\nuv run wildata visualize-dataset --dataset roi_dataset\n</code></pre>"},{"location":"scripts/wildata/#gps-management-workflow","title":"GPS Management Workflow","text":"<pre><code># 1. Extract GPS from images\n# (using WildDetect extract_gps.bat)\n\n# 2. Update GPS if needed\ncd wildata\nscripts\\update-gps-example.bat\n\n# 3. Verify GPS data\n# Check EXIF data in images\n</code></pre>"},{"location":"scripts/wildata/#dvc-workflow","title":"DVC Workflow","text":"<pre><code># Setup (once)\ncd wildata\nscripts\\dvc-setup.bat\n\n# After each dataset import\ndvc add data\\datasets\\new_dataset\ngit add data\\datasets\\new_dataset.dvc\ngit commit -m \"Add new dataset\"\ndvc push\n\n# On other machines\ngit pull\ndvc pull\n</code></pre>"},{"location":"scripts/wildata/#configuration-examples","title":"Configuration Examples","text":""},{"location":"scripts/wildata/#complete-import-config","title":"Complete Import Config","text":"<pre><code># configs/import-config-example.yaml\nsource_path: \"D:/annotations/dataset.json\"\nsource_format: \"coco\"\ndataset_name: \"wildlife_train\"\n\nroot: \"D:/data\"\nsplit_name: \"train\"\nprocessing_mode: \"batch\"\n\n# Label Studio integration\nls_xml_config: \"configs/label_studio_config.xml\"\nls_parse_config: false\n\n# ROI extraction\ndisable_roi: false\nroi_config:\n  random_roi_count: 2\n  roi_box_size: 384\n  min_roi_size: 32\n  background_class: \"background\"\n  sample_background: true\n\n# Transformations\ntransformations:\n  enable_bbox_clipping: true\n  bbox_clipping:\n    tolerance: 5\n    skip_invalid: false\n\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n    min_visibility: 0.7\n    max_negative_tiles_in_negative_image: 2\n    dark_threshold: 0.7\n\n  enable_augmentation: false\n  augmentation:\n    rotation_range: [-45, 45]\n    probability: 1.0\n    num_transforms: 2\n</code></pre>"},{"location":"scripts/wildata/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildata/#import-fails","title":"Import Fails","text":"<p>Issue: Dataset import fails with validation errors</p> <p>Solutions: 1. Check source file format is correct 2. Verify all image paths are valid 3. Check bbox coordinates are within image bounds 4. Use <code>--verbose</code> flag for detailed errors</p>"},{"location":"scripts/wildata/#dvc-push-fails","title":"DVC Push Fails","text":"<p>Issue: Can't push data to remote</p> <p>Solutions: 1. Verify remote credentials 2. Check network connection 3. Verify remote storage path exists 4. Use <code>dvc remote list</code> to check configuration</p>"},{"location":"scripts/wildata/#api-wont-start","title":"API Won't Start","text":"<p>Issue: API server fails to start</p> <p>Solutions: 1. Check port 8441 is not in use 2. Verify <code>.env</code> file configuration 3. Check all dependencies installed 4. Look at error logs</p>"},{"location":"scripts/wildata/#out-of-memory","title":"Out of Memory","text":"<p>Issue: Import fails with memory error</p> <p>Solutions: 1. Use <code>processing_mode: \"streaming\"</code> 2. Reduce number of parallel workers 3. Process datasets one at a time 4. Disable transformations temporarily</p>"},{"location":"scripts/wildata/#next-steps","title":"Next Steps","text":"<ul> <li>WilData Configuration Reference</li> <li>WilData CLI Reference</li> <li>Dataset Preparation Tutorial</li> <li>WilData API Documentation</li> </ul>"},{"location":"scripts/wildetect/","title":"WildDetect Scripts Reference","text":"<p>This page documents all batch scripts available in the WildDetect package. These scripts provide convenient ways to run common operations on Windows.</p>"},{"location":"scripts/wildetect/#overview","title":"Overview","text":"<p>All scripts are located in the <code>scripts/</code> directory and should be run from the project root.</p>"},{"location":"scripts/wildetect/#quick-reference","title":"Quick Reference","text":"Script Purpose Config File run_detection.bat Run wildlife detection <code>config/detection.yaml</code> run_census.bat Run census campaign <code>config/census.yaml</code> launch_ui.bat Launch Streamlit UI None launch_fiftyone.bat Launch FiftyOne viewer None launch_labelstudio.bat Launch Label Studio <code>.env</code> launch_mlflow.bat Launch MLflow UI None launch_inference_server.bat Launch inference API None register_model.bat Register model to MLflow <code>config/detector_registration.yaml</code> extract_gps.bat Extract GPS from images <code>config/extract-gps.yaml</code> profile_census.bat Profile census performance <code>config/census.yaml</code> run_integration_tests.bat Run integration tests None load_env.bat Load environment variables <code>.env</code>"},{"location":"scripts/wildetect/#detection-scripts","title":"Detection Scripts","text":""},{"location":"scripts/wildetect/#run_detectionbat","title":"run_detection.bat","text":"<p>Purpose: Run wildlife detection on images using a trained model.</p> <p>Location: <code>scripts/run_detection.bat</code></p> <p>Command: <pre><code>uv run --env-file .env wildetect detection detect -c config/detection.yaml\n</code></pre></p> <p>Configuration: <code>config/detection.yaml</code></p> <p>Key Parameters: - <code>model.mlflow_model_name</code>: Model name in MLflow registry - <code>model.mlflow_model_alias</code>: Model version/alias (e.g., \"production\") - <code>model.device</code>: Device to use (\"cuda\", \"cpu\", \"auto\") - <code>image_paths</code>: List of image paths to process - <code>image_dir</code>: Directory containing images - <code>processing.batch_size</code>: Batch size for inference - <code>processing.tile_size</code>: Tile size for large images - <code>processing.pipeline_type</code>: Pipeline strategy (\"raster\", \"multithreaded\", \"simple\") - <code>output.directory</code>: Output directory for results</p> <p>Example Usage: <pre><code># Edit config/detection.yaml first\ncd wildetect\nscripts\\run_detection.bat\n</code></pre></p> <p>Example Config: <pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nimage_dir: \"D:/images/survey_2024/\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  pipeline_type: \"raster\"\n\noutput:\n  directory: \"results/detections\"\n</code></pre></p> <p>Output: - Detection results: JSON and CSV files - Visualizations (if enabled) - FiftyOne dataset (if configured)</p>"},{"location":"scripts/wildetect/#run_censusbat","title":"run_census.bat","text":"<p>Purpose: Run a complete wildlife census campaign with statistics and reports.</p> <p>Location: <code>scripts/run_census.bat</code></p> <p>Command: <pre><code>uv run --env-file .env --no-sync wildetect detection census -c config/census.yaml\n</code></pre></p> <p>Configuration: <code>config/census.yaml</code></p> <p>Key Parameters: - <code>campaign.name</code>: Census campaign name - <code>campaign.target_species</code>: List of target species - <code>model</code>: Model configuration (same as detection) - <code>flight_specs.flight_height</code>: Flight altitude in meters - <code>flight_specs.gsd</code>: Ground Sample Distance (cm/px) - <code>analysis</code>: Analysis options (density, hotspots, maps) - <code>output.generate_pdf_report</code>: Generate PDF report</p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\run_census.bat\n</code></pre></p> <p>Example Config: <pre><code>campaign:\n  name: \"Summer_2024_Survey\"\n  target_species: [\"elephant\", \"giraffe\", \"zebra\"]\n\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n\nflight_specs:\n  flight_height: 120.0\n  gsd: 2.38\n\noutput:\n  directory: \"census_results\"\n  generate_pdf_report: true\n</code></pre></p> <p>Output: - Census statistics (counts, density) - Geographic analysis - Visualizations and maps - PDF report (if enabled)</p>"},{"location":"scripts/wildetect/#profile_censusbat","title":"profile_census.bat","text":"<p>Purpose: Profile census performance with detailed timing and memory analysis.</p> <p>Location: <code>scripts/profile_census.bat</code></p> <p>Command: <pre><code>uv run wildetect detection census -c config/census.yaml --profile --gpu-profile --line-profile\n</code></pre></p> <p>Flags: - <code>--profile</code>: Enable general profiling - <code>--gpu-profile</code>: Profile GPU memory usage - <code>--line-profile</code>: Line-by-line profiling</p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\profile_census.bat\n</code></pre></p> <p>Output: - Profiling reports - Performance metrics - Memory usage statistics - Bottleneck identification</p>"},{"location":"scripts/wildetect/#visualization-scripts","title":"Visualization Scripts","text":""},{"location":"scripts/wildetect/#extract_gpsbat","title":"extract_gps.bat","text":"<p>Purpose: Extract GPS coordinates from image EXIF data and create a summary.</p> <p>Location: <code>scripts/extract_gps.bat</code></p> <p>Command: <pre><code>uv run wildetect visualization extract-gps-coordinates -c config/extract-gps.yaml\n</code></pre></p> <p>Configuration: <code>config/extract-gps.yaml</code></p> <p>Key Parameters: - <code>image_directory</code>: Directory containing images with EXIF data - <code>output_file</code>: Output CSV file for GPS coordinates - <code>recursive</code>: Search subdirectories</p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\extract_gps.bat\n</code></pre></p> <p>Example Config: <pre><code>image_directory: \"D:/images/survey/\"\noutput_file: \"gps_coordinates.csv\"\nrecursive: true\n</code></pre></p> <p>Output: - CSV file with GPS coordinates - Summary statistics - Coverage map data</p>"},{"location":"scripts/wildetect/#ui-and-service-scripts","title":"UI and Service Scripts","text":""},{"location":"scripts/wildetect/#launch_uibat","title":"launch_ui.bat","text":"<p>Purpose: Launch the WildDetect Streamlit web interface.</p> <p>Location: <code>scripts/launch_ui.bat</code></p> <p>Command: <pre><code>uv run wildetect services ui\n</code></pre></p> <p>Features: - Interactive detection interface - Configuration editor - Results visualization - Real-time processing</p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\launch_ui.bat\n</code></pre></p> <p>Access: Opens browser at <code>http://localhost:8501</code></p>"},{"location":"scripts/wildetect/#launch_fiftyonebat","title":"launch_fiftyone.bat","text":"<p>Purpose: Launch FiftyOne app for interactive dataset visualization.</p> <p>Location: <code>scripts/launch_fiftyone.bat</code></p> <p>Command: <pre><code>uv run --no-sync --env-file .env fiftyone app launch\n</code></pre></p> <p>Prerequisites: - FiftyOne installed (<code>pip install fiftyone</code>) - Dataset loaded in FiftyOne</p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\launch_fiftyone.bat\n</code></pre></p> <p>Access: Opens browser at <code>http://localhost:5151</code></p> <p>Features: - Interactive dataset viewer - Detection visualization - Filtering and querying - Export capabilities</p>"},{"location":"scripts/wildetect/#launch_labelstudiobat","title":"launch_labelstudio.bat","text":"<p>Purpose: Launch Label Studio for data annotation.</p> <p>Location: <code>scripts/launch_labelstudio.bat</code></p> <p>Command: <pre><code># Activates separate venv and starts Label Studio\nlabel-studio start -p 8080\n</code></pre></p> <p>Prerequisites: - Label Studio venv configured - Label Studio installed in venv</p> <p>Example Usage: <pre><code>cd wildetect/scripts\nlaunch_labelstudio.bat\n</code></pre></p> <p>Access: Opens browser at <code>http://localhost:8080</code></p> <p>Configuration: - Set <code>LABEL_STUDIO_API_KEY</code> in <code>.env</code> - Configure project settings in Label Studio UI</p>"},{"location":"scripts/wildetect/#launch_mlflowbat","title":"launch_mlflow.bat","text":"<p>Purpose: Launch MLflow tracking server UI.</p> <p>Location: <code>scripts/launch_mlflow.bat</code></p> <p>Command: <pre><code>uv run mlflow server --backend-store-uri runs/mlflow --host 0.0.0.0 --port 5000\n</code></pre></p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\launch_mlflow.bat\n</code></pre></p> <p>Access: Opens browser at <code>http://localhost:5000</code></p> <p>Features: - View experiments and runs - Compare models - Model registry management - Metrics and artifacts</p> <p>Environment Variables: <pre><code># In .env\nMLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre></p>"},{"location":"scripts/wildetect/#launch_inference_serverbat","title":"launch_inference_server.bat","text":"<p>Purpose: Launch FastAPI inference server for remote detection.</p> <p>Location: <code>scripts/launch_inference_server.bat</code></p> <p>Command: <pre><code>uv run wildetect services inference-server --port 4141 --workers 2\n</code></pre></p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\launch_inference_server.bat\n</code></pre></p> <p>Access: - API: <code>http://localhost:4141</code> - Docs: <code>http://localhost:4141/docs</code></p> <p>API Endpoints: <pre><code># Health check\nGET /health\n\n# Run detection\nPOST /predict\nContent-Type: multipart/form-data\n{\n  \"file\": &lt;image_file&gt;,\n  \"confidence\": 0.5\n}\n\n# Batch detection\nPOST /predict/batch\n</code></pre></p> <p>Example Request: <pre><code>import requests\n\n# Single image\nwith open(\"image.jpg\", \"rb\") as f:\n    response = requests.post(\n        \"http://localhost:4141/predict\",\n        files={\"file\": f},\n        data={\"confidence\": 0.5}\n    )\n\ndetections = response.json()\n</code></pre></p>"},{"location":"scripts/wildetect/#model-management-scripts","title":"Model Management Scripts","text":""},{"location":"scripts/wildetect/#register_modelbat","title":"register_model.bat","text":"<p>Purpose: Register a trained model to MLflow model registry.</p> <p>Location: <code>scripts/register_model.bat</code></p> <p>Command: <pre><code>uv run wildtrain register detector config/detector_registration.yaml\n</code></pre></p> <p>Configuration: <code>config/detector_registration.yaml</code></p> <p>Key Parameters: - <code>model_path</code>: Path to model weights - <code>model_name</code>: Name for model registry - <code>model_type</code>: Model type (\"detector\" or \"classifier\") - <code>description</code>: Model description - <code>tags</code>: Metadata tags</p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\register_model.bat\n</code></pre></p> <p>Example Config: <pre><code>model_path: \"models/best.pt\"\nmodel_name: \"wildlife_detector\"\nmodel_type: \"detector\"\ndescription: \"YOLO model trained on aerial wildlife images\"\ntags:\n  framework: \"yolo\"\n  dataset: \"wildlife_v1\"\n  training_date: \"2024-01-15\"\n</code></pre></p> <p>Output: - Model registered in MLflow - Model version assigned - Artifacts logged</p>"},{"location":"scripts/wildetect/#testing-scripts","title":"Testing Scripts","text":""},{"location":"scripts/wildetect/#run_integration_testsbat","title":"run_integration_tests.bat","text":"<p>Purpose: Run integration tests for detection pipeline.</p> <p>Location: <code>scripts/run_integration_tests.bat</code></p> <p>Command: <pre><code># Detection pipeline tests\nuv run pytest tests/test_detection_pipeline.py::TestDetectionPipeline::test_detection_pipeline_with_real_images -v\n\n# Data loading tests\nuv run pytest tests/test_data_loading.py -v\n</code></pre></p> <p>Example Usage: <pre><code>cd wildetect\nscripts\\run_integration_tests.bat\n</code></pre></p> <p>Tests Covered: - Detection pipeline with real images - Data loading and preprocessing - Model loading and inference - Result formatting and export</p> <p>Requirements: - Test data in <code>tests/data/</code> - Test model available - pytest installed</p>"},{"location":"scripts/wildetect/#utility-scripts","title":"Utility Scripts","text":""},{"location":"scripts/wildetect/#load_envbat","title":"load_env.bat","text":"<p>Purpose: Load environment variables from <code>.env</code> file.</p> <p>Location: <code>scripts/load_env.bat</code></p> <p>Usage: Called automatically by other scripts</p> <p>Environment Variables: <pre><code># Example .env file\nMLFLOW_TRACKING_URI=http://localhost:5000\nLABEL_STUDIO_URL=http://localhost:8080\nLABEL_STUDIO_API_KEY=your_api_key\nDATA_ROOT=D:/data/\nCUDA_VISIBLE_DEVICES=0\n</code></pre></p>"},{"location":"scripts/wildetect/#common-workflows","title":"Common Workflows","text":""},{"location":"scripts/wildetect/#detection-workflow","title":"Detection Workflow","text":"<pre><code># 1. Start MLflow\nscripts\\launch_mlflow.bat\n\n# 2. Run detection\nscripts\\run_detection.bat\n\n# 3. View results\nscripts\\launch_fiftyone.bat\n</code></pre>"},{"location":"scripts/wildetect/#census-workflow","title":"Census Workflow","text":"<pre><code># 1. Configure census\n# Edit config/census.yaml\n\n# 2. Run census\nscripts\\run_census.bat\n\n# 3. View results\n# Open census_results/report.pdf\n</code></pre>"},{"location":"scripts/wildetect/#model-training-and-registration","title":"Model Training and Registration","text":"<pre><code># 1. Train model (in wildtrain)\ncd wildtrain\nscripts\\train_classifier.bat\n\n# 2. Register model\ncd ..\nscripts\\register_model.bat\n\n# 3. View in MLflow\nscripts\\launch_mlflow.bat\n</code></pre>"},{"location":"scripts/wildetect/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/#script-wont-run","title":"Script Won't Run","text":"<p>Issue: Script exits immediately or shows error</p> <p>Solutions: 1. Check Python environment is activated 2. Run <code>uv sync</code> to install dependencies 3. Verify <code>.env</code> file exists and is configured 4. Check paths in configuration files</p>"},{"location":"scripts/wildetect/#model-loading-error","title":"Model Loading Error","text":"<p>Issue: Can't load model from MLflow</p> <p>Solutions: 1. Ensure MLflow server is running (<code>launch_mlflow.bat</code>) 2. Check model name and alias in config 3. Verify <code>MLFLOW_TRACKING_URI</code> in <code>.env</code> 4. Use <code>mlflow models list</code> to see available models</p>"},{"location":"scripts/wildetect/#gpu-not-detected","title":"GPU Not Detected","text":"<p>Issue: Detection runs on CPU despite having GPU</p> <p>Solutions: 1. Check CUDA installation: <code>python -c \"import torch; print(torch.cuda.is_available())\"</code> 2. Set <code>device: \"cuda\"</code> in config file 3. Check <code>CUDA_VISIBLE_DEVICES</code> environment variable 4. Reinstall PyTorch with CUDA support</p>"},{"location":"scripts/wildetect/#out-of-memory","title":"Out of Memory","text":"<p>Issue: CUDA out of memory or system RAM exhausted</p> <p>Solutions: 1. Reduce <code>batch_size</code> in config 2. Reduce <code>tile_size</code> for raster detection 3. Close other applications 4. Use <code>pipeline_type: \"simple\"</code> for lower memory usage</p>"},{"location":"scripts/wildetect/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Reference</li> <li>CLI Reference</li> <li>Tutorials</li> </ul>"},{"location":"scripts/wildetect/extract_gps/","title":"GPS Extraction Script","text":"<p>Location: <code>scripts/extract_gps.bat</code></p> <p>Purpose: Extract GPS coordinates from images and create visualizations with geographic information. This script runs the GPS extraction and visualization command.</p>"},{"location":"scripts/wildetect/extract_gps/#usage","title":"Usage","text":"<pre><code>scripts\\extract_gps.bat\n</code></pre> <p>The script automatically: 1. Changes to the project root directory 2. Runs the GPS extraction command</p>"},{"location":"scripts/wildetect/extract_gps/#command-executed","title":"Command Executed","text":"<pre><code>uv run wildetect visualization extract-gps-coordinates -c config/extract-gps.yaml\n</code></pre>"},{"location":"scripts/wildetect/extract_gps/#configuration","title":"Configuration","text":"<p>Config File: <code>config/extract-gps.yaml</code></p> <p>See GPS Extraction Configuration for complete parameter reference.</p>"},{"location":"scripts/wildetect/extract_gps/#prerequisites","title":"Prerequisites","text":"<ol> <li>Images with GPS Data: Images must have EXIF GPS metadata, OR</li> <li>Label Studio Data: Label Studio project with GPS data, OR</li> <li> <p>CSV with GPS: CSV file with GPS coordinates matching image filenames</p> </li> <li> <p>Configuration: <code>config/extract-gps.yaml</code> must be properly configured</p> </li> </ol>"},{"location":"scripts/wildetect/extract_gps/#example-workflow","title":"Example Workflow","text":""},{"location":"scripts/wildetect/extract_gps/#1-configure-gps-extraction","title":"1. Configure GPS Extraction","text":"<p>Edit <code>config/extract-gps.yaml</code>:</p> <pre><code>labelstudio:\n  url: http://localhost:8080\n  api_key: ${LABEL_STUDIO_API_KEY}\n  project_id: 1\n  download_resources: false\n  dotenv_path: .env\n\ncsv_output_path: results/gps_coordinates.csv\ndetection_type: \"annotations\"\n\nflight_specs:\n  sensor_height: 15.6\n  focal_length: 16.0\n  flight_height: 120.0\n\nlogging:\n  verbose: true\n</code></pre>"},{"location":"scripts/wildetect/extract_gps/#2-run-gps-extraction","title":"2. Run GPS Extraction","text":"<pre><code>scripts\\extract_gps.bat\n</code></pre>"},{"location":"scripts/wildetect/extract_gps/#3-view-results","title":"3. View Results","text":"<ul> <li>CSV file with GPS coordinates at <code>csv_output_path</code></li> <li>Geographic visualizations (if enabled)</li> <li>Summary statistics</li> </ul>"},{"location":"scripts/wildetect/extract_gps/#output","title":"Output","text":"<p>The GPS extraction script generates:</p> <ul> <li>CSV File: GPS coordinates and detection data exported to CSV</li> <li>Geographic Visualizations: Maps showing detection locations (if configured)</li> <li>Statistics: Summary of GPS coverage and detection distribution</li> <li>Coverage Maps: Visualization of survey coverage area</li> </ul>"},{"location":"scripts/wildetect/extract_gps/#common-use-cases","title":"Common Use Cases","text":""},{"location":"scripts/wildetect/extract_gps/#extract-gps-from-label-studio-annotations","title":"Extract GPS from Label Studio Annotations","text":"<pre><code>labelstudio:\n  url: http://localhost:8080\n  api_key: ${LABEL_STUDIO_API_KEY}\n  project_id: 1\n\ncsv_output_path: results/annotations_gps.csv\ndetection_type: \"annotations\"\n</code></pre>"},{"location":"scripts/wildetect/extract_gps/#extract-gps-from-predictions","title":"Extract GPS from Predictions","text":"<pre><code>labelstudio:\n  json_path: results/predictions.json\n\ncsv_output_path: results/predictions_gps.csv\ndetection_type: \"predictions\"\n</code></pre>"},{"location":"scripts/wildetect/extract_gps/#extract-gps-from-images-with-exif","title":"Extract GPS from Images with EXIF","text":"<p>If images have EXIF GPS data, the script will extract it automatically when processing through Label Studio or detection results.</p>"},{"location":"scripts/wildetect/extract_gps/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/extract_gps/#label-studio-connection-failed","title":"Label Studio Connection Failed","text":"<p>Issue: Cannot connect to Label Studio server</p> <p>Solutions: 1. Start Label Studio: <code>scripts\\launch_labelstudio.bat</code> 2. Verify <code>url</code> in config is correct (default: <code>http://localhost:8080</code>) 3. Check API key is set in <code>.env</code> file 4. Verify network connectivity</p>"},{"location":"scripts/wildetect/extract_gps/#gps-coordinates-missing","title":"GPS Coordinates Missing","text":"<p>Issue: No GPS coordinates extracted</p> <p>Solutions: 1. Verify images have EXIF GPS data 2. Check image formats support EXIF (JPEG, TIFF) 3. Ensure images were taken with GPS-enabled camera 4. Verify Label Studio project contains GPS data 5. Check CSV format if using CSV input</p>"},{"location":"scripts/wildetect/extract_gps/#csv-output-not-created","title":"CSV Output Not Created","text":"<p>Issue: CSV file not generated</p> <p>Solutions: 1. Verify <code>csv_output_path</code> directory exists or can be created 2. Check file permissions for output directory 3. Ensure input data (annotations/predictions) is valid 4. Check logs for error messages 5. Verify sufficient disk space</p>"},{"location":"scripts/wildetect/extract_gps/#api-key-error","title":"API Key Error","text":"<p>Issue: Label Studio API key authentication fails</p> <p>Solutions: 1. Set <code>LABEL_STUDIO_API_KEY</code> in <code>.env</code> file 2. Get API key from Label Studio: Settings \u2192 Access Token 3. Verify API key has correct permissions 4. Check API key format is correct</p>"},{"location":"scripts/wildetect/extract_gps/#flight-specs-missing","title":"Flight Specs Missing","text":"<p>Issue: Geographic calculations fail</p> <p>Solutions: 1. Ensure <code>flight_specs</code> are set in config 2. Verify flight specifications match actual survey 3. Check sensor_height, focal_length, and flight_height are correct 4. Ensure GSD can be calculated from flight specs</p>"},{"location":"scripts/wildetect/extract_gps/#related-documentation","title":"Related Documentation","text":"<ul> <li>GPS Extraction Configuration</li> <li>Label Studio Launch Script</li> <li>Visualization Config</li> <li>CLI Reference</li> </ul>"},{"location":"scripts/wildetect/launch_fiftyone/","title":"FiftyOne Launch Script","text":"<p>Location: <code>scripts/launch_fiftyone.bat</code></p> <p>Purpose: Launch the FiftyOne app for interactive dataset visualization, detection review, and data exploration.</p>"},{"location":"scripts/wildetect/launch_fiftyone/#usage","title":"Usage","text":"<pre><code>scripts\\launch_fiftyone.bat\n</code></pre> <p>The script automatically: 1. Changes to the project root directory 2. Loads environment variables from <code>.env</code> file 3. Launches FiftyOne app</p>"},{"location":"scripts/wildetect/launch_fiftyone/#command-executed","title":"Command Executed","text":"<pre><code>uv run --no-sync --env-file .env fiftyone app launch\n</code></pre> <p>Note: The <code>--no-sync</code> flag prevents <code>uv</code> from syncing dependencies.</p>"},{"location":"scripts/wildetect/launch_fiftyone/#access","title":"Access","text":"<p>Once launched, the FiftyOne app will be available at: - URL: <code>http://localhost:5151</code> - Opens automatically in your default web browser</p>"},{"location":"scripts/wildetect/launch_fiftyone/#features","title":"Features","text":"<p>FiftyOne provides:</p> <ul> <li>Interactive Dataset Viewer: Browse images and detections</li> <li>Detection Visualization: View bounding boxes, labels, and confidence scores</li> <li>Filtering and Querying: Filter by class, confidence, location, etc.</li> <li>Statistics: View dataset statistics and distributions</li> <li>Export Capabilities: Export datasets in various formats</li> <li>Annotation Review: Review and validate detections</li> <li>Comparison: Compare different model predictions</li> </ul>"},{"location":"scripts/wildetect/launch_fiftyone/#prerequisites","title":"Prerequisites","text":"<ol> <li>FiftyOne Installed: FiftyOne should be installed (<code>pip install fiftyone</code>)</li> <li>Dataset Available: Detection results should be uploaded to FiftyOne (via detection/census scripts)</li> <li>Port Availability: Port 5151 should be available</li> <li>Environment: <code>.env</code> file should exist (for environment variables)</li> </ol>"},{"location":"scripts/wildetect/launch_fiftyone/#example-workflow","title":"Example Workflow","text":""},{"location":"scripts/wildetect/launch_fiftyone/#1-run-detection-with-fiftyone-export","title":"1. Run Detection with FiftyOne Export","text":"<p>First, ensure detection results are exported to FiftyOne:</p> <pre><code># In config/detection.yaml\noutput:\n  dataset_name: \"my_detections\"  # Set dataset name\n</code></pre> <p>Run detection: <pre><code>scripts\\run_detection.bat\n</code></pre></p>"},{"location":"scripts/wildetect/launch_fiftyone/#2-launch-fiftyone","title":"2. Launch FiftyOne","text":"<pre><code>scripts\\launch_fiftyone.bat\n</code></pre>"},{"location":"scripts/wildetect/launch_fiftyone/#3-explore-dataset","title":"3. Explore Dataset","text":"<ul> <li>Browse images and detections</li> <li>Filter by species, confidence, etc.</li> <li>View statistics and distributions</li> <li>Export results if needed</li> </ul>"},{"location":"scripts/wildetect/launch_fiftyone/#stopping-fiftyone","title":"Stopping FiftyOne","text":"<p>Press <code>Ctrl+C</code> in the terminal window to stop the FiftyOne server.</p>"},{"location":"scripts/wildetect/launch_fiftyone/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/launch_fiftyone/#port-already-in-use","title":"Port Already in Use","text":"<p>Issue: Port 5151 is already occupied</p> <p>Solutions: 1. Close other FiftyOne instances 2. Kill process using port 5151 3. FiftyOne will try to use next available port automatically</p>"},{"location":"scripts/wildetect/launch_fiftyone/#dataset-not-found","title":"Dataset Not Found","text":"<p>Issue: No datasets visible in FiftyOne</p> <p>Solutions: 1. Verify detection was run with <code>output.dataset_name</code> set 2. Check dataset name matches in config and FiftyOne 3. Ensure detection completed successfully 4. Check FiftyOne database is accessible</p>"},{"location":"scripts/wildetect/launch_fiftyone/#fiftyone-not-installed","title":"FiftyOne Not Installed","text":"<p>Issue: Command not found or import error</p> <p>Solutions: 1. Install FiftyOne: <code>uv run pip install fiftyone</code> 2. Verify installation: <code>uv run python -c \"import fiftyone\"</code> 3. Check Python environment is correct</p>"},{"location":"scripts/wildetect/launch_fiftyone/#connection-refused","title":"Connection Refused","text":"<p>Issue: Cannot connect to FiftyOne server</p> <p>Solutions: 1. Verify FiftyOne server started successfully 2. Check terminal for error messages 3. Try accessing <code>http://127.0.0.1:5151</code> instead 4. Check firewall settings</p>"},{"location":"scripts/wildetect/launch_fiftyone/#slow-performance","title":"Slow Performance","text":"<p>Issue: FiftyOne is slow or unresponsive</p> <p>Solutions: 1. Reduce dataset size (filter samples) 2. Use lower resolution images 3. Close other applications 4. Check system resources</p>"},{"location":"scripts/wildetect/launch_fiftyone/#related-documentation","title":"Related Documentation","text":"<ul> <li>Detection Script</li> <li>Census Script</li> <li>FiftyOne Documentation</li> </ul>"},{"location":"scripts/wildetect/launch_inference_server/","title":"Inference Server Launch Script","text":"<p>Location: <code>scripts/launch_inference_server.bat</code></p> <p>Purpose: Launch a FastAPI inference server for remote detection. Allows other applications or services to send images for detection via HTTP API.</p>"},{"location":"scripts/wildetect/launch_inference_server/#usage","title":"Usage","text":"<pre><code>scripts\\launch_inference_server.bat\n</code></pre> <p>The script automatically: 1. Changes to the project root directory 2. Launches the inference server</p>"},{"location":"scripts/wildetect/launch_inference_server/#command-executed","title":"Command Executed","text":"<pre><code>uv run wildetect services inference-server --port 4141 --workers 2\n</code></pre>"},{"location":"scripts/wildetect/launch_inference_server/#access","title":"Access","text":"<p>Once launched, the inference server will be available at: - API Base URL: <code>http://localhost:4141</code> - API Documentation: <code>http://localhost:4141/docs</code> (Swagger UI) - Alternative Docs: <code>http://localhost:4141/redoc</code> (ReDoc)</p>"},{"location":"scripts/wildetect/launch_inference_server/#features","title":"Features","text":"<p>The inference server provides:</p> <ul> <li>REST API: HTTP endpoints for detection</li> <li>Single Image Detection: Process individual images</li> <li>Batch Detection: Process multiple images</li> <li>Health Check: Server health and status endpoint</li> <li>Interactive Docs: Swagger UI for API exploration</li> <li>Model Loading: Loads models from MLflow or local paths</li> </ul>"},{"location":"scripts/wildetect/launch_inference_server/#prerequisites","title":"Prerequisites","text":"<ol> <li>Dependencies: All Python dependencies installed</li> <li>Port Availability: Port 4141 should be available</li> <li>Model Available: Model should be accessible (MLflow or local path)</li> </ol>"},{"location":"scripts/wildetect/launch_inference_server/#api-endpoints","title":"API Endpoints","text":""},{"location":"scripts/wildetect/launch_inference_server/#health-check","title":"Health Check","text":"<pre><code>GET /health\n</code></pre> <p>Returns server status and health information.</p>"},{"location":"scripts/wildetect/launch_inference_server/#single-image-detection","title":"Single Image Detection","text":"<pre><code>POST /predict\nContent-Type: multipart/form-data\n\n{\n  \"file\": &lt;image_file&gt;,\n  \"confidence\": 0.5  # optional\n}\n</code></pre>"},{"location":"scripts/wildetect/launch_inference_server/#batch-detection","title":"Batch Detection","text":"<pre><code>POST /predict/batch\nContent-Type: multipart/form-data\n\n{\n  \"files\": [&lt;image_file1&gt;, &lt;image_file2&gt;, ...],\n  \"confidence\": 0.5  # optional\n}\n</code></pre>"},{"location":"scripts/wildetect/launch_inference_server/#example-usage","title":"Example Usage","text":""},{"location":"scripts/wildetect/launch_inference_server/#python-client","title":"Python Client","text":"<pre><code>import requests\n\n# Single image detection\nwith open(\"image.jpg\", \"rb\") as f:\n    response = requests.post(\n        \"http://localhost:4141/predict\",\n        files={\"file\": f},\n        data={\"confidence\": 0.5}\n    )\n\ndetections = response.json()\nprint(detections)\n</code></pre>"},{"location":"scripts/wildetect/launch_inference_server/#curl","title":"cURL","text":"<pre><code>curl -X POST \"http://localhost:4141/predict\" \\\n  -F \"file=@image.jpg\" \\\n  -F \"confidence=0.5\"\n</code></pre>"},{"location":"scripts/wildetect/launch_inference_server/#using-in-detection-config","title":"Using in Detection Config","text":"<pre><code># In config/detection.yaml\ninference_service:\n  url: \"http://localhost:4141/predict\"\n  timeout: 60\n</code></pre>"},{"location":"scripts/wildetect/launch_inference_server/#stopping-the-server","title":"Stopping the Server","text":"<p>Press <code>Ctrl+C</code> in the terminal window to stop the inference server.</p>"},{"location":"scripts/wildetect/launch_inference_server/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/launch_inference_server/#port-already-in-use","title":"Port Already in Use","text":"<p>Issue: Port 4141 is already occupied</p> <p>Solutions: 1. Close other inference server instances 2. Kill process using port 4141 3. Use different port: <code>--port 4142</code></p>"},{"location":"scripts/wildetect/launch_inference_server/#model-loading-fails","title":"Model Loading Fails","text":"<p>Issue: Server cannot load model</p> <p>Solutions: 1. Verify model is available (MLflow or local path) 2. Check MLflow server is running (if using MLflow models) 3. Verify model path is correct 4. Check model file permissions</p>"},{"location":"scripts/wildetect/launch_inference_server/#api-requests-fail","title":"API Requests Fail","text":"<p>Issue: Detection requests return errors</p> <p>Solutions: 1. Verify server is running: <code>curl http://localhost:4141/health</code> 2. Check request format matches API specification 3. Verify image file is valid 4. Check server logs for error messages 5. Ensure image format is supported</p>"},{"location":"scripts/wildetect/launch_inference_server/#slow-response-times","title":"Slow Response Times","text":"<p>Issue: API responses are slow</p> <p>Solutions: 1. Increase <code>--workers</code> count (more parallel workers) 2. Use GPU if available 3. Reduce image size before sending 4. Check server resources (CPU, memory, GPU)</p>"},{"location":"scripts/wildetect/launch_inference_server/#connection-refused","title":"Connection Refused","text":"<p>Issue: Cannot connect to inference server</p> <p>Solutions: 1. Verify server started successfully 2. Check server is listening on correct port 3. Try accessing <code>http://127.0.0.1:4141</code> instead 4. Check firewall settings 5. Review terminal for error messages</p>"},{"location":"scripts/wildetect/launch_inference_server/#related-documentation","title":"Related Documentation","text":"<ul> <li>Detection Script</li> <li>Inference Service Config</li> <li>CLI Reference</li> </ul>"},{"location":"scripts/wildetect/launch_labelstudio/","title":"Label Studio Launch Script","text":"<p>Location: <code>scripts/launch_labelstudio.bat</code></p> <p>Purpose: Launch Label Studio for data annotation, detection review, and annotation management. This script activates a separate virtual environment and starts the Label Studio server.</p>"},{"location":"scripts/wildetect/launch_labelstudio/#usage","title":"Usage","text":"<pre><code>scripts\\launch_labelstudio.bat\n</code></pre> <p>The script automatically: 1. Changes to the scripts directory 2. Deactivates any active virtual environments 3. Activates the Label Studio virtual environment (<code>.venv-ls</code>) 4. Installs/updates Label Studio (version 1.20.0) 5. Launches Label Studio server with environment variables loaded</p>"},{"location":"scripts/wildetect/launch_labelstudio/#command-executed","title":"Command Executed","text":"<pre><code>call ..\\.venv-ls\\Scripts\\activate\ncall uv pip install label-studio==1.20.0\ncall uv run --active --no-sync --env-file ..\\.env label-studio start -p 8080\n</code></pre>"},{"location":"scripts/wildetect/launch_labelstudio/#access","title":"Access","text":"<p>Once launched, Label Studio will be available at: - URL: <code>http://localhost:8080</code> - Opens automatically in your default web browser</p>"},{"location":"scripts/wildetect/launch_labelstudio/#features","title":"Features","text":"<p>Label Studio provides:</p> <ul> <li>Data Annotation: Annotate images with bounding boxes, polygons, etc.</li> <li>Detection Review: Review and correct model predictions</li> <li>Project Management: Organize annotation projects</li> <li>Team Collaboration: Multiple annotators can work on projects</li> <li>Export Formats: Export annotations in various formats (COCO, YOLO, etc.)</li> <li>Import Predictions: Import model predictions for review</li> </ul>"},{"location":"scripts/wildetect/launch_labelstudio/#prerequisites","title":"Prerequisites","text":"<ol> <li>Virtual Environment: Label Studio virtual environment should exist at <code>.venv-ls/</code></li> <li>Port Availability: Port 8080 should be available</li> <li>Environment File: <code>.env</code> file should exist in project root</li> <li>Label Studio Config: XML configuration file (optional, for custom labeling interface)</li> </ol>"},{"location":"scripts/wildetect/launch_labelstudio/#setup","title":"Setup","text":""},{"location":"scripts/wildetect/launch_labelstudio/#first-time-setup","title":"First-Time Setup","text":"<p>If <code>.venv-ls</code> doesn't exist, create it:</p> <pre><code>python -m venv .venv-ls\n.venv-ls\\Scripts\\activate\npip install label-studio==1.20.0\n</code></pre>"},{"location":"scripts/wildetect/launch_labelstudio/#environment-variables","title":"Environment Variables","text":"<p>Set in <code>.env</code> file:</p> <pre><code>LABEL_STUDIO_URL=http://localhost:8080\nLABEL_STUDIO_API_KEY=your_api_key_here\n</code></pre>"},{"location":"scripts/wildetect/launch_labelstudio/#example-workflow","title":"Example Workflow","text":""},{"location":"scripts/wildetect/launch_labelstudio/#1-launch-label-studio","title":"1. Launch Label Studio","text":"<pre><code>scripts\\launch_labelstudio.bat\n</code></pre>"},{"location":"scripts/wildetect/launch_labelstudio/#2-create-project","title":"2. Create Project","text":"<ul> <li>Open <code>http://localhost:8080</code></li> <li>Create new project</li> <li>Configure labeling interface (or use XML config)</li> <li>Import images</li> </ul>"},{"location":"scripts/wildetect/launch_labelstudio/#3-annotate-or-review","title":"3. Annotate or Review","text":"<ul> <li>Annotate images manually, or</li> <li>Import model predictions for review</li> <li>Correct and validate annotations</li> </ul>"},{"location":"scripts/wildetect/launch_labelstudio/#4-export-annotations","title":"4. Export Annotations","text":"<ul> <li>Export in desired format (COCO, YOLO, etc.)</li> <li>Use exported annotations for training or evaluation</li> </ul>"},{"location":"scripts/wildetect/launch_labelstudio/#integration-with-wilddetect","title":"Integration with WildDetect","text":"<p>Label Studio can be integrated with WildDetect in several ways:</p>"},{"location":"scripts/wildetect/launch_labelstudio/#import-predictions-for-review","title":"Import Predictions for Review","text":"<pre><code># In config/detection.yaml or config/census.yaml\nlabelstudio:\n  url: http://localhost:8080\n  api_key: ${LABEL_STUDIO_API_KEY}\n  project_id: 1\n  download_resources: false\n</code></pre>"},{"location":"scripts/wildetect/launch_labelstudio/#export-to-label-studio","title":"Export to Label Studio","text":"<pre><code># In config/census.yaml\nexport:\n  export_to_labelstudio: true\n</code></pre>"},{"location":"scripts/wildetect/launch_labelstudio/#stopping-label-studio","title":"Stopping Label Studio","text":"<p>Press <code>Ctrl+C</code> in the terminal window to stop the Label Studio server.</p>"},{"location":"scripts/wildetect/launch_labelstudio/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/launch_labelstudio/#virtual-environment-not-found","title":"Virtual Environment Not Found","text":"<p>Issue: <code>.venv-ls</code> directory doesn't exist</p> <p>Solutions: 1. Create virtual environment (see Setup section) 2. Verify path is correct: <code>..\\.venv-ls\\Scripts\\activate</code> 3. Check script is run from <code>scripts/</code> directory</p>"},{"location":"scripts/wildetect/launch_labelstudio/#port-already-in-use","title":"Port Already in Use","text":"<p>Issue: Port 8080 is already occupied</p> <p>Solutions: 1. Close other Label Studio instances 2. Kill process using port 8080 3. Modify script to use different port: <code>-p 8081</code></p>"},{"location":"scripts/wildetect/launch_labelstudio/#label-studio-wont-start","title":"Label Studio Won't Start","text":"<p>Issue: Server fails to start</p> <p>Solutions: 1. Check virtual environment is activated correctly 2. Verify Label Studio is installed: <code>pip list | findstr label-studio</code> 3. Check Python version compatibility 4. Review terminal error messages</p>"},{"location":"scripts/wildetect/launch_labelstudio/#api-key-not-working","title":"API Key Not Working","text":"<p>Issue: Cannot authenticate with Label Studio API</p> <p>Solutions: 1. Verify <code>LABEL_STUDIO_API_KEY</code> in <code>.env</code> file 2. Get API key from Label Studio: Settings \u2192 Account &amp; Settings \u2192 Access Token 3. Check API key has correct permissions 4. Ensure Label Studio server is running</p>"},{"location":"scripts/wildetect/launch_labelstudio/#importexport-fails","title":"Import/Export Fails","text":"<p>Issue: Cannot import predictions or export annotations</p> <p>Solutions: 1. Verify Label Studio server is running 2. Check <code>url</code> and <code>api_key</code> in config are correct 3. Verify project ID exists 4. Check network connectivity 5. Review Label Studio logs</p>"},{"location":"scripts/wildetect/launch_labelstudio/#related-documentation","title":"Related Documentation","text":"<ul> <li>Label Studio Configuration (for XML config)</li> <li>Detection Script</li> <li>Census Script</li> <li>Label Studio Documentation</li> </ul>"},{"location":"scripts/wildetect/launch_mlflow/","title":"MLflow Launch Script","text":"<p>Location: <code>scripts/launch_mlflow.bat</code></p> <p>Purpose: Launch the MLflow tracking server UI for model registry management, experiment tracking, and model versioning.</p>"},{"location":"scripts/wildetect/launch_mlflow/#usage","title":"Usage","text":"<pre><code>scripts\\launch_mlflow.bat\n</code></pre> <p>The script automatically: 1. Changes to the project root directory 2. Launches MLflow tracking server</p>"},{"location":"scripts/wildetect/launch_mlflow/#command-executed","title":"Command Executed","text":"<pre><code>uv run mlflow server --backend-store-uri runs/mlflow --host 0.0.0.0 --port 5000\n</code></pre>"},{"location":"scripts/wildetect/launch_mlflow/#access","title":"Access","text":"<p>Once launched, MLflow UI will be available at: - URL: <code>http://localhost:5000</code> - Opens automatically in your default web browser</p>"},{"location":"scripts/wildetect/launch_mlflow/#features","title":"Features","text":"<p>MLflow provides:</p> <ul> <li>Model Registry: View and manage registered models</li> <li>Experiment Tracking: View training runs and metrics</li> <li>Model Versioning: Track model versions and aliases</li> <li>Model Comparison: Compare different model versions</li> <li>Artifacts: View model artifacts and files</li> <li>Metadata: View model tags, descriptions, and metadata</li> </ul>"},{"location":"scripts/wildetect/launch_mlflow/#prerequisites","title":"Prerequisites","text":"<ol> <li>MLflow Installed: MLflow should be installed (included in dependencies)</li> <li>Port Availability: Port 5000 should be available</li> <li>Storage: <code>runs/mlflow</code> directory will be created for backend storage</li> </ol>"},{"location":"scripts/wildetect/launch_mlflow/#example-workflow","title":"Example Workflow","text":""},{"location":"scripts/wildetect/launch_mlflow/#1-launch-mlflow","title":"1. Launch MLflow","text":"<pre><code>scripts\\launch_mlflow.bat\n</code></pre>"},{"location":"scripts/wildetect/launch_mlflow/#2-access-ui","title":"2. Access UI","text":"<p>Open browser to <code>http://localhost:5000</code> (usually opens automatically)</p>"},{"location":"scripts/wildetect/launch_mlflow/#3-view-models-and-experiments","title":"3. View Models and Experiments","text":"<ul> <li>Browse registered models</li> <li>View experiment runs</li> <li>Compare model versions</li> <li>Download model artifacts</li> </ul>"},{"location":"scripts/wildetect/launch_mlflow/#environment-variables","title":"Environment Variables","text":"<p>Set in <code>.env</code> file:</p> <pre><code>MLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre> <p>This URI is used by detection and census scripts to load models.</p>"},{"location":"scripts/wildetect/launch_mlflow/#integration-with-wilddetect","title":"Integration with WildDetect","text":"<p>MLflow is used throughout WildDetect for:</p> <ul> <li>Model Loading: Detection and census scripts load models from MLflow</li> <li>Model Registration: Register trained models via <code>register_model.bat</code></li> <li>Version Management: Use model aliases (e.g., \"production\", \"latest\")</li> </ul>"},{"location":"scripts/wildetect/launch_mlflow/#example-model-configuration","title":"Example Model Configuration","text":"<pre><code># In config/detection.yaml\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"  # Loads production version\n  device: \"cuda\"\n</code></pre>"},{"location":"scripts/wildetect/launch_mlflow/#stopping-mlflow","title":"Stopping MLflow","text":"<p>Press <code>Ctrl+C</code> in the terminal window to stop the MLflow server.</p>"},{"location":"scripts/wildetect/launch_mlflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/launch_mlflow/#port-already-in-use","title":"Port Already in Use","text":"<p>Issue: Port 5000 is already occupied</p> <p>Solutions: 1. Close other MLflow instances 2. Kill process using port 5000: <code>netstat -ano | findstr :5000</code> 3. Use different port: <code>--port 5001</code> (update <code>.env</code> accordingly)</p>"},{"location":"scripts/wildetect/launch_mlflow/#cannot-connect-to-mlflow","title":"Cannot Connect to MLflow","text":"<p>Issue: Detection scripts cannot connect to MLflow</p> <p>Solutions: 1. Verify MLflow server is running 2. Check <code>MLFLOW_TRACKING_URI</code> in <code>.env</code> matches server URL 3. Ensure server is accessible: <code>curl http://localhost:5000</code> 4. Check firewall settings</p>"},{"location":"scripts/wildetect/launch_mlflow/#models-not-visible","title":"Models Not Visible","text":"<p>Issue: Registered models don't appear in UI</p> <p>Solutions: 1. Verify models were registered successfully 2. Check backend storage directory exists: <code>runs/mlflow</code> 3. Refresh browser page 4. Check model name and version in registry 5. Verify backend storage is accessible</p>"},{"location":"scripts/wildetect/launch_mlflow/#backend-storage-issues","title":"Backend Storage Issues","text":"<p>Issue: Errors related to backend storage</p> <p>Solutions: 1. Verify <code>runs/mlflow</code> directory exists or can be created 2. Check file permissions for storage directory 3. Ensure sufficient disk space 4. Check for corrupted storage files</p>"},{"location":"scripts/wildetect/launch_mlflow/#related-documentation","title":"Related Documentation","text":"<ul> <li>Model Registration Script</li> <li>Detection Script</li> <li>Detector Registration Config</li> <li>MLflow Documentation</li> </ul>"},{"location":"scripts/wildetect/launch_ui/","title":"UI Launch Script","text":"<p>Location: <code>scripts/launch_ui.bat</code></p> <p>Purpose: Launch the WildDetect Streamlit web interface for interactive detection, configuration editing, and results visualization.</p>"},{"location":"scripts/wildetect/launch_ui/#usage","title":"Usage","text":"<pre><code>scripts\\launch_ui.bat\n</code></pre> <p>The script automatically: 1. Changes to the project root directory 2. Launches the Streamlit UI server</p>"},{"location":"scripts/wildetect/launch_ui/#command-executed","title":"Command Executed","text":"<pre><code>uv run wildetect services ui\n</code></pre>"},{"location":"scripts/wildetect/launch_ui/#access","title":"Access","text":"<p>Once launched, the UI will be available at: - URL: <code>http://localhost:8501</code> - Opens automatically in your default web browser</p>"},{"location":"scripts/wildetect/launch_ui/#features","title":"Features","text":"<p>The Streamlit UI provides:</p> <ul> <li>Interactive Detection: Upload images and run detection in real-time</li> <li>Configuration Editor: Edit and test configuration files through the web interface</li> <li>Results Visualization: View detection results with bounding boxes and statistics</li> <li>Real-time Processing: Monitor detection progress and see results as they're generated</li> <li>Model Selection: Choose models from MLflow registry</li> <li>Batch Processing: Process multiple images through the web interface</li> </ul>"},{"location":"scripts/wildetect/launch_ui/#prerequisites","title":"Prerequisites","text":"<ol> <li>Dependencies: All Python dependencies installed via <code>uv sync</code></li> <li>Streamlit: Streamlit should be installed (included in dependencies)</li> <li>Port Availability: Port 8501 should be available (default Streamlit port)</li> </ol>"},{"location":"scripts/wildetect/launch_ui/#example-workflow","title":"Example Workflow","text":""},{"location":"scripts/wildetect/launch_ui/#1-launch-ui","title":"1. Launch UI","text":"<pre><code>scripts\\launch_ui.bat\n</code></pre>"},{"location":"scripts/wildetect/launch_ui/#2-access-interface","title":"2. Access Interface","text":"<p>Open browser to <code>http://localhost:8501</code> (usually opens automatically)</p>"},{"location":"scripts/wildetect/launch_ui/#3-use-features","title":"3. Use Features","text":"<ul> <li>Upload images for detection</li> <li>Configure detection parameters</li> <li>View results and visualizations</li> <li>Export results</li> </ul>"},{"location":"scripts/wildetect/launch_ui/#stopping-the-ui","title":"Stopping the UI","text":"<p>Press <code>Ctrl+C</code> in the terminal window to stop the Streamlit server.</p>"},{"location":"scripts/wildetect/launch_ui/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/launch_ui/#port-already-in-use","title":"Port Already in Use","text":"<p>Issue: Port 8501 is already occupied</p> <p>Solutions: 1. Close other Streamlit instances 2. Kill process using port 8501: <code>netstat -ano | findstr :8501</code> 3. Use different port (modify script to add <code>--server.port 8502</code>)</p>"},{"location":"scripts/wildetect/launch_ui/#ui-wont-load","title":"UI Won't Load","text":"<p>Issue: Browser shows connection error</p> <p>Solutions: 1. Verify Streamlit server started successfully 2. Check firewall settings 3. Try accessing <code>http://127.0.0.1:8501</code> instead 4. Check terminal for error messages</p>"},{"location":"scripts/wildetect/launch_ui/#dependencies-missing","title":"Dependencies Missing","text":"<p>Issue: Import errors when launching UI</p> <p>Solutions: 1. Run <code>uv sync</code> to install dependencies 2. Verify Streamlit is installed: <code>uv run pip list | findstr streamlit</code> 3. Check Python environment is activated</p>"},{"location":"scripts/wildetect/launch_ui/#slow-performance","title":"Slow Performance","text":"<p>Issue: UI is slow or unresponsive</p> <p>Solutions: 1. Close other applications 2. Reduce image sizes for upload 3. Use smaller batch sizes in detection 4. Check system resources (CPU, memory)</p>"},{"location":"scripts/wildetect/launch_ui/#related-documentation","title":"Related Documentation","text":"<ul> <li>Detection Script</li> <li>Configuration Reference</li> <li>CLI Reference</li> </ul>"},{"location":"scripts/wildetect/load_env/","title":"Environment Load Script","text":"<p>Location: <code>scripts/load_env.bat</code></p> <p>Purpose: Utility script to load environment variables from <code>.env</code> file into the current shell session. This script is typically called automatically by other scripts, but can be used standalone.</p>"},{"location":"scripts/wildetect/load_env/#usage","title":"Usage","text":"<pre><code>scripts\\load_env.bat\n</code></pre> <p>Or called from other scripts: <pre><code>call scripts\\load_env.bat\n</code></pre></p>"},{"location":"scripts/wildetect/load_env/#functionality","title":"Functionality","text":"<p>The script: 1. Checks if <code>.env</code> file exists 2. Parses <code>.env</code> file line by line 3. Sets environment variables in the current shell 4. Skips comment lines (starting with <code>#</code>) 5. Skips empty lines</p>"},{"location":"scripts/wildetect/load_env/#environment-file-format","title":"Environment File Format","text":"<p>The <code>.env</code> file should contain key-value pairs:</p> <pre><code># MLflow Configuration\nMLFLOW_TRACKING_URI=http://localhost:5000\n\n# Label Studio Configuration\nLABEL_STUDIO_URL=http://localhost:8080\nLABEL_STUDIO_API_KEY=your_api_key_here\n\n# Data Paths\nDATA_ROOT=D:/data/\nMODELS_ROOT=D:/models/\n\n# GPU Configuration\nCUDA_VISIBLE_DEVICES=0\n\n# Other settings\nLOG_LEVEL=INFO\n</code></pre>"},{"location":"scripts/wildetect/load_env/#common-environment-variables","title":"Common Environment Variables","text":""},{"location":"scripts/wildetect/load_env/#mlflow","title":"MLflow","text":"<pre><code>MLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre>"},{"location":"scripts/wildetect/load_env/#label-studio","title":"Label Studio","text":"<pre><code>LABEL_STUDIO_URL=http://localhost:8080\nLABEL_STUDIO_API_KEY=your_api_key\n</code></pre>"},{"location":"scripts/wildetect/load_env/#data-paths","title":"Data Paths","text":"<pre><code>DATA_ROOT=D:/data/\nRESULTS_ROOT=D:/results/\n</code></pre>"},{"location":"scripts/wildetect/load_env/#gpu","title":"GPU","text":"<pre><code>CUDA_VISIBLE_DEVICES=0\n</code></pre>"},{"location":"scripts/wildetect/load_env/#integration-with-other-scripts","title":"Integration with Other Scripts","text":"<p>Most WildDetect scripts automatically call this script or use <code>--env-file .env</code> with <code>uv run</code>. The script is useful when:</p> <ul> <li>Running commands manually</li> <li>Debugging environment issues</li> <li>Setting up environment for manual operations</li> </ul>"},{"location":"scripts/wildetect/load_env/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/load_env/#env-file-not-found","title":".env File Not Found","text":"<p>Issue: Script reports <code>.env</code> file not found</p> <p>Solutions: 1. Verify <code>.env</code> file exists in project root 2. Check script is run from correct directory 3. Create <code>.env</code> file from <code>example.env</code> if needed 4. Verify file name is exactly <code>.env</code> (not <code>.env.txt</code>)</p>"},{"location":"scripts/wildetect/load_env/#variables-not-set","title":"Variables Not Set","text":"<p>Issue: Environment variables not available after running script</p> <p>Solutions: 1. Verify <code>.env</code> file format is correct (KEY=VALUE) 2. Check for syntax errors in <code>.env</code> file 3. Ensure no spaces around <code>=</code> sign 4. Verify script executed successfully (no errors) 5. Note: Variables are set in the script's shell session only</p>"},{"location":"scripts/wildetect/load_env/#syntax-errors","title":"Syntax Errors","text":"<p>Issue: Script fails to parse <code>.env</code> file</p> <p>Solutions: 1. Check for invalid characters in variable names 2. Ensure values with spaces are properly quoted (if needed) 3. Verify line endings are correct (Windows CRLF) 4. Check for special characters that need escaping</p>"},{"location":"scripts/wildetect/load_env/#path-issues","title":"Path Issues","text":"<p>Issue: Path variables not working correctly</p> <p>Solutions: 1. Use forward slashes or escaped backslashes in paths 2. Use absolute paths instead of relative paths 3. Verify paths exist and are accessible 4. Check for trailing slashes (may cause issues)</p>"},{"location":"scripts/wildetect/load_env/#best-practices","title":"Best Practices","text":"<ol> <li>Template File: Keep <code>example.env</code> as a template</li> <li>Don't Commit Secrets: Add <code>.env</code> to <code>.gitignore</code></li> <li>Document Variables: Document required variables in README</li> <li>Use Absolute Paths: Prefer absolute paths for reliability</li> <li>Validate Format: Ensure <code>.env</code> file follows standard format</li> </ol>"},{"location":"scripts/wildetect/load_env/#related-documentation","title":"Related Documentation","text":"<ul> <li>Environment Setup</li> <li>Installation Guide</li> <li>Configuration Files</li> </ul>"},{"location":"scripts/wildetect/profile_census/","title":"Census Profiling Script","text":"<p>Location: <code>scripts/profile_census.bat</code></p> <p>Purpose: Run census campaign with detailed performance profiling enabled. This script executes the census command with profiling flags to analyze performance bottlenecks, memory usage, and execution time.</p>"},{"location":"scripts/wildetect/profile_census/#usage","title":"Usage","text":"<pre><code>scripts\\profile_census.bat\n</code></pre> <p>The script automatically: 1. Changes to the project root directory 2. Runs census with profiling enabled</p>"},{"location":"scripts/wildetect/profile_census/#command-executed","title":"Command Executed","text":"<pre><code>uv run wildetect detection census -c config/census.yaml --profile --gpu-profile --line-profile\n</code></pre>"},{"location":"scripts/wildetect/profile_census/#profiling-flags","title":"Profiling Flags","text":"<p>The script enables three types of profiling:</p> <ul> <li><code>--profile</code>: General profiling (timing, function calls)</li> <li><code>--gpu-profile</code>: GPU memory and utilization profiling</li> <li><code>--line-profile</code>: Line-by-line execution time profiling</li> </ul>"},{"location":"scripts/wildetect/profile_census/#configuration","title":"Configuration","text":"<p>Config File: <code>config/census.yaml</code></p> <p>See Census Configuration for complete parameter reference.</p>"},{"location":"scripts/wildetect/profile_census/#prerequisites","title":"Prerequisites","text":"<ol> <li>Census Config: <code>config/census.yaml</code> must be properly configured</li> <li>Environment: <code>.env</code> file should exist</li> <li>Model Available: Model should be accessible</li> <li>Profiling Tools: Profiling dependencies should be installed</li> </ol>"},{"location":"scripts/wildetect/profile_census/#example-workflow","title":"Example Workflow","text":""},{"location":"scripts/wildetect/profile_census/#1-configure-census","title":"1. Configure Census","text":"<p>Edit <code>config/census.yaml</code> (see Census Config)</p>"},{"location":"scripts/wildetect/profile_census/#2-run-profiling","title":"2. Run Profiling","text":"<pre><code>scripts\\profile_census.bat\n</code></pre>"},{"location":"scripts/wildetect/profile_census/#3-analyze-results","title":"3. Analyze Results","text":"<p>Profiling results will be saved to: - Profile reports in output directory - Performance metrics and statistics - Memory usage analysis - GPU utilization data - Line-by-line timing information</p>"},{"location":"scripts/wildetect/profile_census/#output","title":"Output","text":"<p>The profiling script generates:</p> <ul> <li>Profile Reports: Detailed timing and performance reports</li> <li>Memory Analysis: Memory usage statistics and peak usage</li> <li>GPU Profiling: GPU memory and utilization metrics</li> <li>Line Profiles: Line-by-line execution time breakdown</li> <li>Bottleneck Identification: Identification of slow functions and operations</li> <li>Performance Metrics: Throughput, latency, and efficiency metrics</li> </ul>"},{"location":"scripts/wildetect/profile_census/#use-cases","title":"Use Cases","text":""},{"location":"scripts/wildetect/profile_census/#performance-optimization","title":"Performance Optimization","text":"<p>Use profiling to identify bottlenecks: - Slow functions or operations - Memory-intensive operations - GPU utilization issues - Inefficient data loading</p>"},{"location":"scripts/wildetect/profile_census/#resource-planning","title":"Resource Planning","text":"<p>Analyze resource requirements: - Memory usage patterns - GPU memory requirements - CPU utilization - Disk I/O patterns</p>"},{"location":"scripts/wildetect/profile_census/#comparison","title":"Comparison","text":"<p>Compare different configurations: - Different batch sizes - Different pipeline types - Different hardware setups</p>"},{"location":"scripts/wildetect/profile_census/#interpreting-results","title":"Interpreting Results","text":""},{"location":"scripts/wildetect/profile_census/#general-profile","title":"General Profile","text":"<ul> <li>Function call counts</li> <li>Total time per function</li> <li>Cumulative time</li> <li>Call graph</li> </ul>"},{"location":"scripts/wildetect/profile_census/#gpu-profile","title":"GPU Profile","text":"<ul> <li>GPU memory allocation</li> <li>Memory peak usage</li> <li>GPU utilization percentage</li> <li>Memory leaks</li> </ul>"},{"location":"scripts/wildetect/profile_census/#line-profile","title":"Line Profile","text":"<ul> <li>Time spent per line of code</li> <li>Hot spots in code</li> <li>Inefficient operations</li> </ul>"},{"location":"scripts/wildetect/profile_census/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/profile_census/#profiling-overhead","title":"Profiling Overhead","text":"<p>Issue: Profiling significantly slows down execution</p> <p>Solutions: 1. This is expected - profiling adds overhead 2. Use smaller test dataset for profiling 3. Disable line profiling for faster runs 4. Profile only specific sections if needed</p>"},{"location":"scripts/wildetect/profile_census/#memory-profiling-fails","title":"Memory Profiling Fails","text":"<p>Issue: Memory profiling errors</p> <p>Solutions: 1. Verify memory profiling tools are installed 2. Check Python version compatibility 3. Ensure sufficient system memory 4. Try disabling other profiling types</p>"},{"location":"scripts/wildetect/profile_census/#gpu-profiling-not-available","title":"GPU Profiling Not Available","text":"<p>Issue: GPU profiling doesn't work</p> <p>Solutions: 1. Verify GPU is available and accessible 2. Check CUDA is properly installed 3. Ensure PyTorch has GPU support 4. Verify GPU profiling tools are installed</p>"},{"location":"scripts/wildetect/profile_census/#results-not-saved","title":"Results Not Saved","text":"<p>Issue: Profiling completes but no reports</p> <p>Solutions: 1. Check output directory exists 2. Verify file permissions 3. Review logs for error messages 4. Ensure sufficient disk space</p>"},{"location":"scripts/wildetect/profile_census/#too-much-data","title":"Too Much Data","text":"<p>Issue: Profiling generates too much data</p> <p>Solutions: 1. Use smaller test dataset 2. Disable line profiling (most verbose) 3. Profile only specific time periods 4. Filter results to relevant sections</p>"},{"location":"scripts/wildetect/profile_census/#best-practices","title":"Best Practices","text":"<ol> <li>Test Dataset: Use representative but smaller dataset for profiling</li> <li>Selective Profiling: Enable only needed profiling types</li> <li>Baseline First: Run without profiling first to establish baseline</li> <li>Compare Runs: Compare profiling results across different configurations</li> <li>Document Findings: Document performance bottlenecks and optimizations</li> </ol>"},{"location":"scripts/wildetect/profile_census/#related-documentation","title":"Related Documentation","text":"<ul> <li>Census Configuration</li> <li>Census Script</li> <li>Benchmark Config</li> <li>Performance Optimization</li> </ul>"},{"location":"scripts/wildetect/register_model/","title":"Model Registration Script","text":"<p>Location: <code>scripts/register_model.bat</code></p> <p>Purpose: Register a trained detector or classifier model to the MLflow model registry. This makes the model available for use in detection and census pipelines.</p>"},{"location":"scripts/wildetect/register_model/#usage","title":"Usage","text":"<pre><code>scripts\\register_model.bat\n</code></pre> <p>The script automatically: 1. Changes to the project root directory 2. Runs the model registration command</p>"},{"location":"scripts/wildetect/register_model/#command-executed","title":"Command Executed","text":"<pre><code>uv run wildtrain register detector config/detector_registration.yaml\n</code></pre> <p>Note: This script registers a detector model. For classifier registration, modify the script or use the CLI directly.</p>"},{"location":"scripts/wildetect/register_model/#configuration","title":"Configuration","text":"<p>Config File: <code>config/detector_registration.yaml</code></p> <p>See Detector Registration Configuration for complete parameter reference.</p>"},{"location":"scripts/wildetect/register_model/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>MLflow Server Running: MLflow server must be running    <pre><code>scripts\\launch_mlflow.bat\n</code></pre></p> </li> <li> <p>Model Files Available: Model weights files must exist at paths specified in config</p> </li> <li> <p>Configuration: <code>config/detector_registration.yaml</code> must be properly configured</p> </li> <li> <p>Environment: <code>.env</code> file should have <code>MLFLOW_TRACKING_URI</code> set</p> </li> </ol>"},{"location":"scripts/wildetect/register_model/#example-workflow","title":"Example Workflow","text":""},{"location":"scripts/wildetect/register_model/#1-configure-registration","title":"1. Configure Registration","text":"<p>Edit <code>config/detector_registration.yaml</code>:</p> <pre><code>localizer:\n  yolo:\n    weights: \"models/best.pt\"\n    imgsz: 800\n    device: \"cuda\"\n    conf_thres: 0.25\n    iou_thres: 0.45\n    max_det: 300\n    overlap_metric: \"IOU\"\n    task: \"detect\"\n  processing:\n    export_format: \"pt\"\n    batch_size: 32\n\nprocessing:\n  name: \"wildlife_detector_v2\"\n  mlflow_tracking_uri: \"http://localhost:5000\"\n</code></pre>"},{"location":"scripts/wildetect/register_model/#2-start-mlflow","title":"2. Start MLflow","text":"<pre><code>scripts\\launch_mlflow.bat\n</code></pre>"},{"location":"scripts/wildetect/register_model/#3-register-model","title":"3. Register Model","text":"<pre><code>scripts\\register_model.bat\n</code></pre>"},{"location":"scripts/wildetect/register_model/#4-verify-registration","title":"4. Verify Registration","text":"<ul> <li>Open MLflow UI: <code>http://localhost:5000</code></li> <li>Navigate to Models section</li> <li>Verify model appears with correct name and version</li> </ul>"},{"location":"scripts/wildetect/register_model/#output","title":"Output","text":"<p>The registration script:</p> <ul> <li>Registers Model: Adds model to MLflow model registry</li> <li>Creates Version: Assigns a new version number</li> <li>Stores Artifacts: Saves model files and metadata</li> <li>Sets Metadata: Stores tags, descriptions, and configuration</li> </ul>"},{"location":"scripts/wildetect/register_model/#model-usage-after-registration","title":"Model Usage After Registration","text":"<p>Once registered, use the model in detection/census configs:</p> <pre><code># In config/detection.yaml or config/census.yaml\nmodel:\n  mlflow_model_name: \"wildlife_detector_v2\"  # Match name from registration\n  mlflow_model_alias: \"latest\"  # or \"production\"\n  device: \"cuda\"\n</code></pre>"},{"location":"scripts/wildetect/register_model/#registering-classifier","title":"Registering Classifier","text":"<p>To register a classifier instead of detector, modify the script or use CLI directly:</p> <pre><code>uv run wildtrain register classifier config/detector_registration.yaml\n</code></pre> <p>Ensure config has <code>classifier</code> section configured.</p>"},{"location":"scripts/wildetect/register_model/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/register_model/#mlflow-not-running","title":"MLflow Not Running","text":"<p>Issue: Cannot connect to MLflow server</p> <p>Solutions: 1. Start MLflow: <code>scripts\\launch_mlflow.bat</code> 2. Verify <code>MLFLOW_TRACKING_URI</code> in <code>.env</code> matches server URL 3. Check MLflow server is accessible: <code>curl http://localhost:5000</code></p>"},{"location":"scripts/wildetect/register_model/#model-file-not-found","title":"Model File Not Found","text":"<p>Issue: Cannot find model weights file</p> <p>Solutions: 1. Verify <code>weights</code> path in config is correct 2. Use absolute paths instead of relative paths 3. Check file permissions 4. Ensure model file exists before registration</p>"},{"location":"scripts/wildetect/register_model/#registration-fails","title":"Registration Fails","text":"<p>Issue: Registration process errors</p> <p>Solutions: 1. Check model file format is correct (<code>.pt</code> for YOLO, <code>.ckpt</code> for classifier) 2. Verify model can be loaded independently 3. Check MLflow server logs for detailed errors 4. Ensure sufficient disk space for artifacts 5. Verify MLflow database is accessible</p>"},{"location":"scripts/wildetect/register_model/#model-name-already-exists","title":"Model Name Already Exists","text":"<p>Issue: Model name conflicts with existing model</p> <p>Solutions: 1. Use different model name in <code>processing.name</code> 2. Or let MLflow create new version automatically 3. Check existing models in MLflow UI</p>"},{"location":"scripts/wildetect/register_model/#export-format-not-supported","title":"Export Format Not Supported","text":"<p>Issue: Model export/conversion fails</p> <p>Solutions: 1. Verify model framework supports the export format 2. Check required dependencies are installed 3. Try different export format 4. Ensure model is in correct state (eval mode)</p>"},{"location":"scripts/wildetect/register_model/#related-documentation","title":"Related Documentation","text":"<ul> <li>Detector Registration Configuration</li> <li>MLflow Launch Script</li> <li>Main Config</li> <li>WildTrain CLI</li> </ul>"},{"location":"scripts/wildetect/run_census/","title":"Census Script","text":"<p>Location: <code>scripts/run_census.bat</code></p> <p>Purpose: Run a complete wildlife census campaign with detection, statistics, geographic analysis, and report generation. This script executes the census pipeline configured in <code>config/census.yaml</code>.</p>"},{"location":"scripts/wildetect/run_census/#usage","title":"Usage","text":"<pre><code>scripts\\run_census.bat\n</code></pre> <p>The script automatically: 1. Changes to the project root directory 2. Loads environment variables from <code>.env</code> file 3. Runs the census command with profiling disabled (for production use)</p>"},{"location":"scripts/wildetect/run_census/#command-executed","title":"Command Executed","text":"<pre><code>uv run --env-file .env --no-sync wildetect detection census -c config/census.yaml\n</code></pre> <p>Note: The <code>--no-sync</code> flag prevents <code>uv</code> from syncing dependencies, speeding up execution.</p>"},{"location":"scripts/wildetect/run_census/#configuration","title":"Configuration","text":"<p>Config File: <code>config/census.yaml</code></p> <p>See Census Configuration for complete parameter reference.</p>"},{"location":"scripts/wildetect/run_census/#prerequisites","title":"Prerequisites","text":"<ol> <li>Environment Setup:</li> <li><code>.env</code> file exists in project root</li> <li> <p>Environment variables configured</p> </li> <li> <p>Model Availability:</p> </li> <li>Model registered in MLflow (or model path specified)</li> <li> <p>MLflow server running (if using MLflow models)</p> </li> <li> <p>Input Images:</p> </li> <li>Images specified in <code>config/census.yaml</code></li> <li> <p>GPS data available (for geographic analysis)</p> </li> <li> <p>Dependencies:</p> </li> <li>All Python dependencies installed</li> </ol>"},{"location":"scripts/wildetect/run_census/#example-workflow","title":"Example Workflow","text":""},{"location":"scripts/wildetect/run_census/#1-configure-census-campaign","title":"1. Configure Census Campaign","text":"<p>Edit <code>config/census.yaml</code>:</p> <pre><code>campaign:\n  id: \"Summer_2024_Survey\"\n  pilot_name: \"John Doe\"\n  target_species: [\"elephant\", \"giraffe\", \"zebra\"]\n\ndetection:\n  image_dir: \"D:/census_images/2024/\"\n\n  model:\n    mlflow_model_name: \"detector\"\n    mlflow_model_alias: \"production\"\n    device: \"cuda\"\n\n  processing:\n    batch_size: 32\n    pipeline_type: \"mt\"\n\n  flight_specs:\n    flight_height: 120.0\n    gsd: 2.38\n\nexport:\n  to_fiftyone: true\n  create_map: true\n  export_to_labelstudio: false\n</code></pre>"},{"location":"scripts/wildetect/run_census/#2-start-mlflow-if-using-mlflow-models","title":"2. Start MLflow (if using MLflow models)","text":"<pre><code>scripts\\launch_mlflow.bat\n</code></pre>"},{"location":"scripts/wildetect/run_census/#3-run-census","title":"3. Run Census","text":"<pre><code>scripts\\run_census.bat\n</code></pre>"},{"location":"scripts/wildetect/run_census/#4-view-results","title":"4. View Results","text":"<p>Results will be saved to the directory specified in <code>export.output_directory</code> (or <code>campaign.id</code> if not specified):</p> <ul> <li>Census statistics and counts</li> <li>Geographic visualizations and maps</li> <li>PDF reports (if enabled)</li> <li>FiftyOne dataset (if enabled)</li> </ul>"},{"location":"scripts/wildetect/run_census/#output","title":"Output","text":"<p>The census script generates:</p> <ul> <li>Detection Results: All detections with coordinates and metadata</li> <li>Statistics: Species counts, densities, distributions</li> <li>Geographic Maps: Interactive maps showing detection locations</li> <li>Reports: PDF reports with analysis (if configured)</li> <li>FiftyOne Dataset: Dataset for interactive exploration (if enabled)</li> <li>Label Studio Export: Annotations for review (if enabled)</li> </ul>"},{"location":"scripts/wildetect/run_census/#common-use-cases","title":"Common Use Cases","text":""},{"location":"scripts/wildetect/run_census/#basic-census","title":"Basic Census","text":"<pre><code>campaign:\n  id: \"Quick_Survey_2024\"\n  target_species: [\"elephant\"]\n\ndetection:\n  image_dir: \"D:/survey/\"\n\nexport:\n  create_map: true\n</code></pre>"},{"location":"scripts/wildetect/run_census/#comprehensive-census-with-analysis","title":"Comprehensive Census with Analysis","text":"<pre><code>campaign:\n  id: \"Annual_Census_2024\"\n  target_species: [\"elephant\", \"giraffe\", \"zebra\", \"buffalo\"]\n\ndetection:\n  image_dir: \"D:/annual_census/images/\"\n\nexport:\n  to_fiftyone: true\n  create_map: true\n  export_to_labelstudio: true\n</code></pre>"},{"location":"scripts/wildetect/run_census/#census-with-gps-update","title":"Census with GPS Update","text":"<pre><code>campaign:\n  id: \"GPS_Updated_Census\"\n\ndetection:\n  exif_gps_update:\n    image_folder: \"D:/images/cam0\"\n    csv_path: \"D:/gps_data.csv\"\n    filename_col: \"filename\"\n    lat_col: \"latitude\"\n    lon_col: \"longitude\"\n</code></pre>"},{"location":"scripts/wildetect/run_census/#differences-from-detection-script","title":"Differences from Detection Script","text":"<p>The census script differs from <code>run_detection.bat</code> in several ways:</p> <ol> <li>Campaign Metadata: Includes campaign ID, pilot name, target species</li> <li>Statistics: Generates population statistics and analysis</li> <li>Geographic Analysis: Creates maps and geographic visualizations</li> <li>Export Options: Supports FiftyOne, Label Studio, and report generation</li> <li>Nested Config: Detection config is nested under <code>detection:</code> key</li> </ol>"},{"location":"scripts/wildetect/run_census/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/run_census/#campaign-id-conflicts","title":"Campaign ID Conflicts","text":"<p>Issue: Output directory already exists</p> <p>Solutions: 1. Use unique campaign IDs 2. Specify custom <code>export.output_directory</code> 3. Remove or rename existing output directory</p>"},{"location":"scripts/wildetect/run_census/#no-detections-found","title":"No Detections Found","text":"<p>Issue: Census completes but finds no animals</p> <p>Solutions: 1. Check <code>target_species</code> matches model class names 2. Verify model is appropriate for survey area 3. Lower confidence thresholds in processing 4. Check images actually contain wildlife</p>"},{"location":"scripts/wildetect/run_census/#map-generation-fails","title":"Map Generation Fails","text":"<p>Issue: Geographic maps not created</p> <p>Solutions: 1. Ensure GPS data is available (EXIF or CSV) 2. Verify <code>flight_specs</code> are set correctly 3. Check <code>export.create_map: true</code> is enabled 4. Ensure sufficient disk space</p>"},{"location":"scripts/wildetect/run_census/#memory-issues","title":"Memory Issues","text":"<p>Issue: Out of memory errors</p> <p>Solutions: 1. Reduce <code>batch_size</code> in detection config 2. Use smaller <code>tile_size</code> 3. Process images in smaller batches 4. Use <code>pipeline_type: \"simple\"</code> for lower memory</p>"},{"location":"scripts/wildetect/run_census/#slow-processing","title":"Slow Processing","text":"<p>Issue: Census takes very long</p> <p>Solutions: 1. Use <code>pipeline_type: \"mt\"</code> for multi-threading 2. Increase <code>batch_size</code> if memory allows 3. Use GPU (<code>device: \"cuda\"</code>) 4. Reduce number of images processed at once</p>"},{"location":"scripts/wildetect/run_census/#related-documentation","title":"Related Documentation","text":"<ul> <li>Census Configuration</li> <li>Detection Script</li> <li>Census Campaign Tutorial</li> <li>Profiling Script</li> </ul>"},{"location":"scripts/wildetect/run_detection/","title":"Detection Script","text":"<p>Location: <code>scripts/run_detection.bat</code></p> <p>Purpose: Run wildlife detection on images using a trained model. This script executes the main detection pipeline configured in <code>config/detection.yaml</code>.</p>"},{"location":"scripts/wildetect/run_detection/#usage","title":"Usage","text":"<pre><code>scripts\\run_detection.bat\n</code></pre> <p>The script automatically: 1. Changes to the project root directory 2. Loads environment variables from <code>.env</code> file 3. Runs the detection command with the configuration file</p>"},{"location":"scripts/wildetect/run_detection/#command-executed","title":"Command Executed","text":"<pre><code>uv run --env-file .env wildetect detection detect -c config/detection.yaml\n</code></pre>"},{"location":"scripts/wildetect/run_detection/#configuration","title":"Configuration","text":"<p>Config File: <code>config/detection.yaml</code></p> <p>See Detection Configuration for complete parameter reference.</p>"},{"location":"scripts/wildetect/run_detection/#prerequisites","title":"Prerequisites","text":"<ol> <li>Environment Setup:</li> <li><code>.env</code> file exists in project root</li> <li> <p>Environment variables configured (MLflow URI, etc.)</p> </li> <li> <p>Model Availability:</p> </li> <li>Model registered in MLflow (or model path specified)</li> <li> <p>MLflow server running (if using MLflow models)</p> </li> <li> <p>Input Images:</p> </li> <li>Images specified in <code>config/detection.yaml</code></li> <li> <p>Images accessible at specified paths</p> </li> <li> <p>Dependencies:</p> </li> <li>All Python dependencies installed via <code>uv sync</code></li> </ol>"},{"location":"scripts/wildetect/run_detection/#example-workflow","title":"Example Workflow","text":""},{"location":"scripts/wildetect/run_detection/#1-configure-detection","title":"1. Configure Detection","text":"<p>Edit <code>config/detection.yaml</code>:</p> <pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nimage_dir: \"D:/survey_images/2024/\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  pipeline_type: \"raster\"\n\noutput:\n  directory: \"results/detections\"\n</code></pre>"},{"location":"scripts/wildetect/run_detection/#2-start-mlflow-if-using-mlflow-models","title":"2. Start MLflow (if using MLflow models)","text":"<pre><code>scripts\\launch_mlflow.bat\n</code></pre>"},{"location":"scripts/wildetect/run_detection/#3-run-detection","title":"3. Run Detection","text":"<pre><code>scripts\\run_detection.bat\n</code></pre>"},{"location":"scripts/wildetect/run_detection/#4-view-results","title":"4. View Results","text":"<p>Results will be saved to the directory specified in <code>output.directory</code>. You can also view in FiftyOne:</p> <pre><code>scripts\\launch_fiftyone.bat\n</code></pre>"},{"location":"scripts/wildetect/run_detection/#output","title":"Output","text":"<p>The detection script generates:</p> <ul> <li>Detection Results: JSON and CSV files with detection coordinates, classes, and confidence scores</li> <li>Visualizations: Images with bounding boxes overlaid (if enabled)</li> <li>FiftyOne Dataset: Dataset uploaded to FiftyOne (if <code>output.dataset_name</code> is set)</li> <li>Logs: Processing logs saved to log file or console</li> </ul>"},{"location":"scripts/wildetect/run_detection/#common-use-cases","title":"Common Use Cases","text":""},{"location":"scripts/wildetect/run_detection/#single-image-detection","title":"Single Image Detection","text":"<pre><code># In config/detection.yaml\nimage_paths:\n  - \"D:/images/single_image.tif\"\n</code></pre>"},{"location":"scripts/wildetect/run_detection/#directory-of-images","title":"Directory of Images","text":"<pre><code># In config/detection.yaml\nimage_dir: \"D:/survey_images/\"\n</code></pre>"},{"location":"scripts/wildetect/run_detection/#large-raster-geotiff","title":"Large Raster (GeoTIFF)","text":"<pre><code># In config/detection.yaml\nimage_paths:\n  - \"D:/orthomosaics/large_ortho.tif\"\n\nprocessing:\n  pipeline_type: \"raster\"\n\nflight_specs:\n  gsd: 2.38  # Required for raster\n</code></pre>"},{"location":"scripts/wildetect/run_detection/#with-gps-update","title":"With GPS Update","text":"<pre><code># In config/detection.yaml\nexif_gps_update:\n  image_folder: \"D:/images/\"\n  csv_path: \"D:/gps_data.csv\"\n  filename_col: \"filename\"\n  lat_col: \"latitude\"\n  lon_col: \"longitude\"\n</code></pre>"},{"location":"scripts/wildetect/run_detection/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/run_detection/#script-exits-immediately","title":"Script Exits Immediately","text":"<p>Issue: Script closes without running</p> <p>Solutions: 1. Check Python environment is set up: <code>uv sync</code> 2. Verify <code>.env</code> file exists 3. Run from project root directory 4. Check script file has correct line endings (Windows CRLF)</p>"},{"location":"scripts/wildetect/run_detection/#model-not-found","title":"Model Not Found","text":"<p>Issue: Cannot load model from MLflow</p> <p>Solutions: 1. Ensure MLflow server is running: <code>scripts\\launch_mlflow.bat</code> 2. Verify model name and alias in config match MLflow registry 3. Check <code>MLFLOW_TRACKING_URI</code> in <code>.env</code> file 4. List available models: <code>mlflow models list</code></p>"},{"location":"scripts/wildetect/run_detection/#images-not-found","title":"Images Not Found","text":"<p>Issue: Cannot find input images</p> <p>Solutions: 1. Verify image paths in config are correct 2. Use absolute paths instead of relative paths 3. Check file permissions 4. Ensure images exist at specified locations</p>"},{"location":"scripts/wildetect/run_detection/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Issue: GPU memory errors</p> <p>Solutions: 1. Reduce <code>batch_size</code> in config (try 16, 8, or 4) 2. Reduce <code>tile_size</code> (try 640 or 512) 3. Set <code>device: \"cpu\"</code> to use CPU 4. Close other GPU applications</p>"},{"location":"scripts/wildetect/run_detection/#detection-takes-too-long","title":"Detection Takes Too Long","text":"<p>Issue: Processing is very slow</p> <p>Solutions: 1. Increase <code>batch_size</code> if memory allows 2. Use <code>pipeline_type: \"mt\"</code> for multi-threading 3. Use GPU (<code>device: \"cuda\"</code>) instead of CPU 4. Reduce <code>tile_size</code> for faster processing 5. Process fewer images at once</p>"},{"location":"scripts/wildetect/run_detection/#related-documentation","title":"Related Documentation","text":"<ul> <li>Detection Configuration</li> <li>Census Script</li> <li>End-to-End Detection Tutorial</li> <li>CLI Reference</li> </ul>"},{"location":"scripts/wildetect/run_integration_tests/","title":"Integration Tests Script","text":"<p>Location: <code>scripts/run_integration_tests.bat</code></p> <p>Purpose: Run integration tests for the detection pipeline to verify end-to-end functionality with real images and data.</p>"},{"location":"scripts/wildetect/run_integration_tests/#usage","title":"Usage","text":"<pre><code>scripts\\run_integration_tests.bat\n</code></pre> <p>The script automatically: 1. Changes to the project root directory 2. Runs multiple integration test suites</p>"},{"location":"scripts/wildetect/run_integration_tests/#commands-executed","title":"Commands Executed","text":"<p>The script runs two test suites:</p> <pre><code># Detection pipeline tests\nuv run pytest tests/test_detection_pipeline.py::TestDetectionPipeline::test_detection_pipeline_with_real_images -v\n\n# Data loading tests\nuv run pytest tests/test_data_loading.py -v\n</code></pre>"},{"location":"scripts/wildetect/run_integration_tests/#test-coverage","title":"Test Coverage","text":""},{"location":"scripts/wildetect/run_integration_tests/#detection-pipeline-tests","title":"Detection Pipeline Tests","text":"<p>Tests the complete detection pipeline: - Detection with real images - Model loading and inference - Result formatting - Output generation</p>"},{"location":"scripts/wildetect/run_integration_tests/#data-loading-tests","title":"Data Loading Tests","text":"<p>Tests data loading and preprocessing: - Image loading - Data preprocessing - Batch creation - Data transformations</p>"},{"location":"scripts/wildetect/run_integration_tests/#prerequisites","title":"Prerequisites","text":"<ol> <li>Test Data: Test images should be available in <code>tests/data/</code> (if required)</li> <li>Model Available: Test model should be accessible (MLflow or local)</li> <li>Dependencies: All test dependencies installed via <code>uv sync</code></li> <li>pytest: pytest should be installed and available</li> </ol>"},{"location":"scripts/wildetect/run_integration_tests/#example-workflow","title":"Example Workflow","text":""},{"location":"scripts/wildetect/run_integration_tests/#1-prepare-test-data","title":"1. Prepare Test Data","text":"<p>Ensure test data is available: - Test images in appropriate location - Test model accessible - Test configurations ready</p>"},{"location":"scripts/wildetect/run_integration_tests/#2-run-tests","title":"2. Run Tests","text":"<pre><code>scripts\\run_integration_tests.bat\n</code></pre>"},{"location":"scripts/wildetect/run_integration_tests/#3-review-results","title":"3. Review Results","text":"<ul> <li>Check test output for pass/fail status</li> <li>Review any error messages</li> <li>Verify all tests complete successfully</li> </ul>"},{"location":"scripts/wildetect/run_integration_tests/#output","title":"Output","text":"<p>The test script provides:</p> <ul> <li>Test Results: Pass/fail status for each test</li> <li>Verbose Output: Detailed test execution information (<code>-v</code> flag)</li> <li>Error Messages: Detailed error information for failed tests</li> <li>Execution Time: Time taken for each test</li> </ul>"},{"location":"scripts/wildetect/run_integration_tests/#running-individual-tests","title":"Running Individual Tests","text":"<p>You can also run tests individually:</p> <pre><code># Run all detection pipeline tests\nuv run pytest tests/test_detection_pipeline.py -v\n\n# Run all data loading tests\nuv run pytest tests/test_data_loading.py -v\n\n# Run specific test\nuv run pytest tests/test_detection_pipeline.py::TestDetectionPipeline::test_detection_pipeline_with_real_images -v\n</code></pre>"},{"location":"scripts/wildetect/run_integration_tests/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildetect/run_integration_tests/#tests-fail","title":"Tests Fail","text":"<p>Issue: Tests fail with errors</p> <p>Solutions: 1. Check test data is available and accessible 2. Verify model is available (MLflow or local path) 3. Check test configuration is correct 4. Review error messages for specific issues 5. Ensure all dependencies are installed</p>"},{"location":"scripts/wildetect/run_integration_tests/#test-data-not-found","title":"Test Data Not Found","text":"<p>Issue: Cannot find test data files</p> <p>Solutions: 1. Verify test data exists in expected location 2. Check file paths in test files 3. Ensure test data is included in repository 4. Check file permissions</p>"},{"location":"scripts/wildetect/run_integration_tests/#model-loading-fails-in-tests","title":"Model Loading Fails in Tests","text":"<p>Issue: Tests fail when loading model</p> <p>Solutions: 1. Verify test model is available 2. Check MLflow server is running (if using MLflow) 3. Verify model path/name in test configuration 4. Ensure model file format is correct</p>"},{"location":"scripts/wildetect/run_integration_tests/#tests-take-too-long","title":"Tests Take Too Long","text":"<p>Issue: Integration tests run very slowly</p> <p>Solutions: 1. Use smaller test images 2. Reduce number of test images 3. Use CPU instead of GPU for faster startup 4. Skip slow tests if not needed: <code>-k \"not slow\"</code></p>"},{"location":"scripts/wildetect/run_integration_tests/#import-errors","title":"Import Errors","text":"<p>Issue: Import errors when running tests</p> <p>Solutions: 1. Verify Python environment is correct 2. Run <code>uv sync</code> to install dependencies 3. Check PYTHONPATH includes project directories 4. Verify test files are in correct location</p>"},{"location":"scripts/wildetect/run_integration_tests/#best-practices","title":"Best Practices","text":"<ol> <li>Run Before Commits: Run integration tests before committing changes</li> <li>Fix Failing Tests: Don't commit with failing tests</li> <li>Add New Tests: Add tests for new functionality</li> <li>Keep Tests Updated: Update tests when functionality changes</li> <li>Use Real Data: Use representative real data in tests</li> </ol>"},{"location":"scripts/wildetect/run_integration_tests/#related-documentation","title":"Related Documentation","text":"<ul> <li>Testing Documentation</li> <li>Detection Pipeline</li> <li>pytest Documentation</li> </ul>"},{"location":"scripts/wildtrain/","title":"WildTrain Scripts Reference","text":"<p>This page documents all batch scripts available in the WildTrain package for model training and evaluation.</p>"},{"location":"scripts/wildtrain/#overview","title":"Overview","text":"<p>All scripts are located in <code>wildtrain/scripts/</code> directory.</p>"},{"location":"scripts/wildtrain/#quick-reference","title":"Quick Reference","text":"Script Purpose Config File train_classifier.bat Train classification model <code>configs/classification/classification_train.yaml</code> eval_classifier.bat Evaluate classifier <code>configs/classification/classification_eval.yaml</code> eval_detector.bat Evaluate detector Various detector configs train_yolo.bat Train YOLO detector <code>configs/detection/yolo_configs/yolo.yaml</code> train_mmdet.bat Train MMDetection model <code>configs/detection/mmdet_configs/mmdet.yaml</code> register_model.bat Register model to MLflow <code>configs/registration/*.yaml</code> run_classification_pipeline.bat Full classification pipeline <code>configs/classification/classification_pipeline_config.yaml</code> run_detection_pipeline.bat Full detection pipeline <code>configs/detection/yolo_configs/yolo_pipeline_config.yaml</code> run_server.bat Run inference server <code>configs/inference.yaml</code> visualize_predictions.bat Visualize predictions Various configs create_dataset.bat Create/prepare dataset <code>configs/datapreparation/*.yaml</code>"},{"location":"scripts/wildtrain/#classification-scripts","title":"Classification Scripts","text":""},{"location":"scripts/wildtrain/#train_classifierbat","title":"train_classifier.bat","text":"<p>Purpose: Train an image classification model using PyTorch Lightning.</p> <p>Location: <code>wildtrain/scripts/train_classifier.bat</code></p> <p>Command: <pre><code>uv run wildtrain train classifier -c configs\\classification\\classification_train.yaml\n</code></pre></p> <p>Configuration: <code>wildtrain/configs/classification/classification_train.yaml</code></p> <p>Key Parameters: <pre><code>model:\n  architecture: \"resnet50\"  # resnet18, resnet50, efficientnet_b0, etc.\n  num_classes: 10\n  pretrained: true\n  learning_rate: 0.001\n\ndata:\n  root_data_directory: \"D:/data/roi_dataset\"\n  split: \"train\"\n  batch_size: 32\n  num_workers: 4\n  image_size: 224\n\ntraining:\n  max_epochs: 100\n  accelerator: \"gpu\"  # gpu, cpu\n  devices: 1\n  precision: 16  # 16, 32\n  gradient_clip_val: 1.0\n\ncallbacks:\n  early_stopping:\n    monitor: \"val_loss\"\n    patience: 10\n  model_checkpoint:\n    monitor: \"val_acc\"\n    mode: \"max\"\n    save_top_k: 3\n\nmlflow:\n  experiment_name: \"classification\"\n  tracking_uri: \"http://localhost:5000\"\n</code></pre></p> <p>Example Usage: <pre><code>cd wildtrain\n\n# Edit config\nnotepad configs\\classification\\classification_train.yaml\n\n# Train\nscripts\\train_classifier.bat\n</code></pre></p> <p>Output: - Trained model checkpoints - MLflow run with metrics - Training logs - Best model weights</p>"},{"location":"scripts/wildtrain/#eval_classifierbat","title":"eval_classifier.bat","text":"<p>Purpose: Evaluate a trained classification model.</p> <p>Location: <code>wildtrain/scripts/eval_classifier.bat</code></p> <p>Command: <pre><code>uv run wildtrain eval classifier -c configs\\classification\\classification_eval.yaml\n</code></pre></p> <p>Configuration: <code>wildtrain/configs/classification/classification_eval.yaml</code></p> <p>Key Parameters: <pre><code>model:\n  checkpoint_path: \"checkpoints/best.ckpt\"\n  # or load from MLflow\n  mlflow_model_name: \"wildlife_classifier\"\n  mlflow_model_version: \"latest\"\n\ndata:\n  root_data_directory: \"D:/data/roi_dataset\"\n  split: \"test\"\n  batch_size: 64\n\nevaluation:\n  save_predictions: true\n  generate_confusion_matrix: true\n  class_metrics: true\n</code></pre></p> <p>Example Usage: <pre><code>cd wildtrain\nscripts\\eval_classifier.bat\n</code></pre></p> <p>Output: - Evaluation metrics (accuracy, precision, recall, F1) - Confusion matrix - Per-class metrics - Predictions file (if enabled)</p>"},{"location":"scripts/wildtrain/#detection-scripts","title":"Detection Scripts","text":""},{"location":"scripts/wildtrain/#train_yolobat","title":"train_yolo.bat","text":"<p>Purpose: Train YOLO object detection model.</p> <p>Location: <code>wildtrain/scripts/train_yolo.bat</code> (or similar)</p> <p>Command: <pre><code>uv run wildtrain train detector -c configs\\detection\\yolo_configs\\yolo.yaml\n</code></pre></p> <p>Configuration: <code>wildtrain/configs/detection/yolo_configs/yolo.yaml</code></p> <p>Key Parameters: <pre><code>model:\n  framework: \"yolo\"\n  size: \"n\"  # n, s, m, l, x\n  pretrained: true\n\ndata:\n  data_yaml: \"D:/data/wildlife/data.yaml\"\n  # data.yaml contains:\n  # - train: path to train images\n  # - val: path to val images\n  # - nc: number of classes\n  # - names: class names\n\ntraining:\n  epochs: 100\n  imgsz: 640\n  batch: 16\n  optimizer: \"AdamW\"\n  lr0: 0.001\n  device: 0  # GPU device\n\naugmentation:\n  hsv_h: 0.015\n  hsv_s: 0.7\n  hsv_v: 0.4\n  degrees: 0.0\n  translate: 0.1\n  scale: 0.5\n  shear: 0.0\n  perspective: 0.0\n  flipud: 0.0\n  fliplr: 0.5\n  mosaic: 1.0\n\nmlflow:\n  experiment_name: \"yolo_detection\"\n</code></pre></p> <p>Example Usage: <pre><code>cd wildtrain\nscripts\\train_yolo.bat\n</code></pre></p> <p>YOLO Data Format: <pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 val/\n\u251c\u2500\u2500 labels/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 val/\n\u2514\u2500\u2500 data.yaml\n</code></pre></p> <p>data.yaml: <pre><code>train: ./images/train\nval: ./images/val\nnc: 3\nnames: ['elephant', 'giraffe', 'zebra']\n</code></pre></p>"},{"location":"scripts/wildtrain/#eval_detectorbat","title":"eval_detector.bat","text":"<p>Purpose: Evaluate object detection model.</p> <p>Location: <code>wildtrain/scripts/eval_detector.bat</code></p> <p>Command: <pre><code>uv run wildtrain eval detector -c configs\\detection\\yolo_configs\\yolo_eval.yaml\n</code></pre></p> <p>Example Usage: <pre><code>cd wildtrain\nscripts\\eval_detector.bat\n</code></pre></p> <p>Output: - mAP (mean Average Precision) - mAP@50, mAP@75 - Per-class AP - Precision-Recall curves - Detection visualizations</p>"},{"location":"scripts/wildtrain/#pipeline-scripts","title":"Pipeline Scripts","text":""},{"location":"scripts/wildtrain/#run_classification_pipelinebat","title":"run_classification_pipeline.bat","text":"<p>Purpose: Run complete classification training pipeline.</p> <p>Location: <code>wildtrain/scripts/run_classification_pipeline.bat</code></p> <p>Configuration: <code>wildtrain/configs/classification/classification_pipeline_config.yaml</code></p> <p>Pipeline Steps: 1. Data validation 2. Model initialization 3. Training 4. Evaluation 5. Model registration 6. Export for deployment</p> <p>Example Config: <pre><code>pipeline:\n  name: \"wildlife_classification_v1\"\n\ndata_preparation:\n  validate_data: true\n  augmentation: true\n\ntraining:\n  config_file: \"configs/classification/classification_train.yaml\"\n\nevaluation:\n  config_file: \"configs/classification/classification_eval.yaml\"\n\nregistration:\n  register_to_mlflow: true\n  model_name: \"wildlife_classifier\"\n  stage: \"Staging\"\n\nexport:\n  export_onnx: true\n  export_torchscript: true\n</code></pre></p>"},{"location":"scripts/wildtrain/#run_detection_pipelinebat","title":"run_detection_pipeline.bat","text":"<p>Purpose: Run complete detection training pipeline.</p> <p>Location: <code>wildtrain/scripts/run_detection_pipeline.bat</code></p> <p>Configuration: <code>wildtrain/configs/detection/yolo_configs/yolo_pipeline_config.yaml</code></p> <p>Pipeline Includes: - Data preparation - Training - Validation - Hyperparameter tuning (optional) - Model registration - Export</p>"},{"location":"scripts/wildtrain/#model-management-scripts","title":"Model Management Scripts","text":""},{"location":"scripts/wildtrain/#register_modelbat","title":"register_model.bat","text":"<p>Purpose: Register trained model to MLflow model registry.</p> <p>Location: <code>wildtrain/scripts/register_model.bat</code></p> <p>Command: <pre><code>uv run wildtrain register &lt;model_type&gt; &lt;config_file&gt;\n</code></pre></p> <p>Example Configs:</p>"},{"location":"scripts/wildtrain/#classifier-registration","title":"Classifier Registration","text":"<pre><code># configs/registration/classifier_registration_example.yaml\nmodel_path: \"checkpoints/best.ckpt\"\nmodel_name: \"wildlife_classifier\"\nmodel_type: \"classifier\"\n\ndescription: \"ResNet50 classifier for wildlife ROI\"\n\ntags:\n  architecture: \"resnet50\"\n  dataset: \"wildlife_roi_v1\"\n  num_classes: \"10\"\n  training_date: \"2024-01-15\"\n  accuracy: \"0.95\"\n\naliases:\n  - \"production\"\n  - \"latest\"\n</code></pre>"},{"location":"scripts/wildtrain/#detector-registration","title":"Detector Registration","text":"<pre><code># configs/registration/detector_registration_example.yaml\nmodel_path: \"runs/detect/train/weights/best.pt\"\nmodel_name: \"wildlife_detector\"\nmodel_type: \"detector\"\n\ndescription: \"YOLO11n detector for aerial wildlife\"\n\ntags:\n  framework: \"yolo\"\n  version: \"11n\"\n  dataset: \"wildlife_aerial_v2\"\n  map50: \"0.89\"\n  training_date: \"2024-01-15\"\n</code></pre> <p>Example Usage: <pre><code>cd wildtrain\nscripts\\register_model.bat\n</code></pre></p>"},{"location":"scripts/wildtrain/#inference-scripts","title":"Inference Scripts","text":""},{"location":"scripts/wildtrain/#run_serverbat","title":"run_server.bat","text":"<p>Purpose: Run inference server for model deployment.</p> <p>Location: <code>wildtrain/scripts/run_server.bat</code></p> <p>Command: <pre><code>uv run wildtrain serve -c configs\\inference.yaml\n</code></pre></p> <p>Configuration: <code>wildtrain/configs/inference.yaml</code></p> <p>Key Parameters: <pre><code>server:\n  host: \"0.0.0.0\"\n  port: 8000\n  workers: 2\n\nmodel:\n  mlflow_model_name: \"wildlife_detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\ninference:\n  batch_size: 8\n  confidence_threshold: 0.5\n  nms_threshold: 0.45\n</code></pre></p> <p>Example Usage: <pre><code>cd wildtrain\nscripts\\run_server.bat\n</code></pre></p> <p>API Access: - Endpoint: <code>http://localhost:8000</code> - Docs: <code>http://localhost:8000/docs</code></p> <p>Example Request: <pre><code>import requests\n\nwith open(\"image.jpg\", \"rb\") as f:\n    response = requests.post(\n        \"http://localhost:8000/predict\",\n        files={\"file\": f}\n    )\n\npredictions = response.json()\n</code></pre></p>"},{"location":"scripts/wildtrain/#utility-scripts","title":"Utility Scripts","text":""},{"location":"scripts/wildtrain/#visualize_predictionsbat","title":"visualize_predictions.bat","text":"<p>Purpose: Visualize model predictions on images.</p> <p>Location: <code>wildtrain/scripts/visualize_predictions.bat</code></p> <p>Configuration: Various visualization configs</p> <p>Features: - Draw bounding boxes - Show confidence scores - Save annotated images - Generate prediction gallery</p> <p>Example Usage: <pre><code>cd wildtrain\nscripts\\visualize_predictions.bat\n</code></pre></p>"},{"location":"scripts/wildtrain/#common-workflows","title":"Common Workflows","text":""},{"location":"scripts/wildtrain/#training-workflow-classification","title":"Training Workflow (Classification)","text":"<pre><code>cd wildtrain\n\n# 1. Prepare data (if needed)\nscripts\\create_dataset.bat\n\n# 2. Train model\nscripts\\train_classifier.bat\n\n# 3. Evaluate\nscripts\\eval_classifier.bat\n\n# 4. Register to MLflow\nscripts\\register_model.bat\n</code></pre>"},{"location":"scripts/wildtrain/#training-workflow-detection","title":"Training Workflow (Detection)","text":"<pre><code>cd wildtrain\n\n# 1. Train YOLO\nscripts\\train_yolo.bat\n\n# 2. Evaluate\nscripts\\eval_detector.bat\n\n# 3. Visualize predictions\nscripts\\visualize_predictions.bat\n\n# 4. Register model\nscripts\\register_model.bat\n</code></pre>"},{"location":"scripts/wildtrain/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code># Use Optuna for HPO\ncd wildtrain\nuv run wildtrain tune classifier -c configs/classification/classification_sweep.yaml\n</code></pre>"},{"location":"scripts/wildtrain/#model-deployment","title":"Model Deployment","text":"<pre><code># 1. Register model\nscripts\\register_model.bat\n\n# 2. Start inference server\nscripts\\run_server.bat\n\n# 3. Test API\ncurl -X POST \"http://localhost:8000/predict\" -F \"file=@test.jpg\"\n</code></pre>"},{"location":"scripts/wildtrain/#configuration-examples","title":"Configuration Examples","text":""},{"location":"scripts/wildtrain/#complete-training-config","title":"Complete Training Config","text":"<pre><code># configs/classification/classification_train.yaml\nmodel:\n  architecture: \"resnet50\"\n  num_classes: 10\n  pretrained: true\n  learning_rate: 0.001\n  weight_decay: 0.0001\n  dropout: 0.5\n\ndata:\n  root_data_directory: \"D:/data/roi_dataset\"\n  split: \"train\"\n  batch_size: 32\n  num_workers: 4\n  image_size: 224\n  normalize: true\n  mean: [0.485, 0.456, 0.406]\n  std: [0.229, 0.224, 0.225]\n\naugmentation:\n  random_flip: 0.5\n  random_rotation: 15\n  color_jitter:\n    brightness: 0.2\n    contrast: 0.2\n    saturation: 0.2\n  random_crop: true\n\ntraining:\n  max_epochs: 100\n  accelerator: \"gpu\"\n  devices: 1\n  precision: 16\n  accumulate_grad_batches: 1\n  gradient_clip_val: 1.0\n  log_every_n_steps: 10\n\noptimizer:\n  type: \"Adam\"\n  lr: 0.001\n  betas: [0.9, 0.999]\n\nscheduler:\n  type: \"CosineAnnealingLR\"\n  T_max: 100\n  eta_min: 0.00001\n\ncallbacks:\n  early_stopping:\n    monitor: \"val_loss\"\n    patience: 10\n    mode: \"min\"\n  model_checkpoint:\n    monitor: \"val_acc\"\n    mode: \"max\"\n    save_top_k: 3\n    filename: \"epoch{epoch:02d}-acc{val_acc:.4f}\"\n\nmlflow:\n  experiment_name: \"wildlife_classification\"\n  tracking_uri: \"http://localhost:5000\"\n  log_models: true\n</code></pre>"},{"location":"scripts/wildtrain/#troubleshooting","title":"Troubleshooting","text":""},{"location":"scripts/wildtrain/#training-crashes","title":"Training Crashes","text":"<p>Issue: Training crashes with CUDA out of memory</p> <p>Solutions: 1. Reduce batch size 2. Use mixed precision (<code>precision: 16</code>) 3. Reduce image size 4. Enable gradient accumulation</p>"},{"location":"scripts/wildtrain/#mlflow-connection-error","title":"MLflow Connection Error","text":"<p>Issue: Can't connect to MLflow tracking server</p> <p>Solutions: 1. Start MLflow server: <code>mlflow server --port 5000</code> 2. Check <code>MLFLOW_TRACKING_URI</code> environment variable 3. Verify network connection 4. Check firewall settings</p>"},{"location":"scripts/wildtrain/#data-loading-slow","title":"Data Loading Slow","text":"<p>Issue: Data loading is bottleneck</p> <p>Solutions: 1. Increase <code>num_workers</code> (max 4 on Windows) 2. Enable <code>pin_memory: true</code> 3. Use SSD for data storage 4. Reduce data preprocessing complexity</p>"},{"location":"scripts/wildtrain/#model-wont-load","title":"Model Won't Load","text":"<p>Issue: Can't load trained model</p> <p>Solutions: 1. Check checkpoint path is correct 2. Verify model architecture matches 3. Check for version compatibility 4. Try loading with <code>strict=False</code></p>"},{"location":"scripts/wildtrain/#next-steps","title":"Next Steps","text":"<ul> <li>WildTrain Configuration Reference</li> <li>Model Training Tutorial</li> <li>WildTrain CLI Reference</li> <li>MLflow Integration Guide</li> </ul>"},{"location":"tutorials/census-campaign/","title":"Census Campaign Tutorial","text":"<p>Learn how to conduct a complete wildlife census campaign using WildDetect.</p>"},{"location":"tutorials/census-campaign/#overview","title":"Overview","text":"<p>A census campaign includes detection, population statistics, geographic analysis, and comprehensive reporting.</p>"},{"location":"tutorials/census-campaign/#prerequisites","title":"Prerequisites","text":"<ul> <li>WildDetect installed</li> <li>Aerial survey images with GPS data</li> <li>Trained detection model</li> <li>MLflow server (optional)</li> </ul>"},{"location":"tutorials/census-campaign/#step-1-organize-survey-data","title":"Step 1: Organize Survey Data","text":"<pre><code>census_2024/\n\u251c\u2500\u2500 images/              # Survey images with GPS EXIF\n\u2502   \u251c\u2500\u2500 flight1/\n\u2502   \u251c\u2500\u2500 flight2/\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 config/\n    \u2514\u2500\u2500 census.yaml\n</code></pre>"},{"location":"tutorials/census-campaign/#step-2-configure-census","title":"Step 2: Configure Census","text":"<p>Create <code>config/census.yaml</code>:</p> <pre><code>campaign:\n  name: \"Summer_2024_Survey\"\n  target_species: [\"elephant\", \"giraffe\", \"zebra\"]\n  area_name: \"Serengeti_North\"\n  start_date: \"2024-06-01\"\n  end_date: \"2024-06-15\"\n\nmodel:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nimage_dir: \"D:/census_2024/images/\"\n\nflight_specs:\n  flight_height: 120.0\n  gsd: 2.38\n\nanalysis:\n  calculate_density: true\n  detect_hotspots: true\n  create_maps: true\n\noutput:\n  directory: \"census_results\"\n  generate_pdf_report: true\n</code></pre>"},{"location":"tutorials/census-campaign/#step-3-run-census","title":"Step 3: Run Census","text":"<pre><code>cd wildetect\n\n# Edit config\nnotepad config\\census.yaml\n\n# Run\nscripts\\run_census.bat\n</code></pre>"},{"location":"tutorials/census-campaign/#step-4-review-results","title":"Step 4: Review Results","text":"<pre><code>census_results/\n\u251c\u2500\u2500 detections.json              # All detections\n\u251c\u2500\u2500 statistics.json              # Population stats\n\u251c\u2500\u2500 census_report.pdf            # PDF report\n\u251c\u2500\u2500 maps/                        # Geographic maps\n\u2502   \u251c\u2500\u2500 distribution_map.html\n\u2502   \u251c\u2500\u2500 density_heatmap.html\n\u2502   \u2514\u2500\u2500 flight_path.html\n\u2514\u2500\u2500 visualizations/              # Annotated images\n</code></pre>"},{"location":"tutorials/census-campaign/#step-5-analyze-statistics","title":"Step 5: Analyze Statistics","text":"<p>The census generates:</p> <ul> <li>Total counts per species</li> <li>Population density (animals/km\u00b2)</li> <li>Species distribution analysis</li> <li>Hotspot locations</li> <li>Coverage area statistics</li> </ul>"},{"location":"tutorials/census-campaign/#geographic-analysis","title":"Geographic Analysis","text":"<p>View interactive maps:</p> <pre><code># Open in browser\nexplorer census_results\\maps\\distribution_map.html\n</code></pre> <p>Features: - Animal locations plotted on map - Density heatmaps - Flight path overlay - Filterable by species</p>"},{"location":"tutorials/census-campaign/#generate-custom-reports","title":"Generate Custom Reports","text":"<pre><code>from wildetect.analysis import ReportGenerator\n\ngenerator = ReportGenerator(\"census_results/detections.json\")\n\n# Custom report\nreport = generator.generate_report(\n    output_path=\"custom_report.pdf\",\n    include_maps=True,\n    include_statistics=True,\n    target_species=[\"elephant\"]\n)\n</code></pre>"},{"location":"tutorials/census-campaign/#example-census-output","title":"Example Census Output","text":"<pre><code>{\n  \"campaign\": \"Summer_2024_Survey\",\n  \"survey_area\": 25.5,  # km\u00b2\n  \"total_images\": 450,\n  \"total_detections\": 1234,\n\n  \"species_counts\": {\n    \"elephant\": 423,\n    \"giraffe\": 612,\n    \"zebra\": 199\n  },\n\n  \"density\": {\n    \"elephant\": 16.6,  # per km\u00b2\n    \"giraffe\": 24.0,\n    \"zebra\": 7.8\n  }\n}\n</code></pre> <p>Next Steps: - End-to-End Detection - Census Configuration</p>"},{"location":"tutorials/dataset-preparation/","title":"Dataset Preparation Tutorial","text":"<p>Learn how to prepare datasets for training using WilData.</p>"},{"location":"tutorials/dataset-preparation/#overview","title":"Overview","text":"<p>This tutorial covers importing, transforming, and exporting datasets for wildlife detection training.</p>"},{"location":"tutorials/dataset-preparation/#prerequisites","title":"Prerequisites","text":"<ul> <li>WilData installed</li> <li>Annotated images (COCO, YOLO, or Label Studio format)</li> </ul>"},{"location":"tutorials/dataset-preparation/#step-1-import-dataset","title":"Step 1: Import Dataset","text":""},{"location":"tutorials/dataset-preparation/#option-a-using-config-file","title":"Option A: Using Config File","text":"<p>Create <code>config.yaml</code>:</p> <pre><code>source_path: \"D:/annotations/dataset.json\"\nsource_format: \"coco\"\ndataset_name: \"wildlife_train\"\nroot: \"D:/data\"\nsplit_name: \"train\"\n\ntransformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n    min_visibility: 0.7\n</code></pre> <p>Run import:</p> <pre><code>cd wildata\nwildata import-dataset --config config.yaml\n</code></pre>"},{"location":"tutorials/dataset-preparation/#option-b-direct-cli","title":"Option B: Direct CLI","text":"<pre><code>wildata import-dataset annotations.json \\\n    --format coco \\\n    --name wildlife_train \\\n    --enable-tiling \\\n    --tile-size 800\n</code></pre>"},{"location":"tutorials/dataset-preparation/#step-2-apply-transformations","title":"Step 2: Apply Transformations","text":""},{"location":"tutorials/dataset-preparation/#tiling-for-large-images","title":"Tiling for Large Images","text":"<pre><code>transformations:\n  enable_tiling: true\n  tiling:\n    tile_size: 800\n    stride: 640\n    min_visibility: 0.7\n</code></pre>"},{"location":"tutorials/dataset-preparation/#bbox-clipping","title":"Bbox Clipping","text":"<pre><code>transformations:\n  enable_bbox_clipping: true\n  bbox_clipping:\n    tolerance: 5\n    skip_invalid: false\n</code></pre>"},{"location":"tutorials/dataset-preparation/#step-3-create-roi-dataset","title":"Step 3: Create ROI Dataset","text":"<p>For classification training:</p> <pre><code>cd wildata\nscripts\\create-roi-dataset.bat\n\n# Or with CLI\nwildata create-roi-dataset --config configs/roi-create-config.yaml\n</code></pre>"},{"location":"tutorials/dataset-preparation/#step-4-visualize","title":"Step 4: Visualize","text":"<pre><code># Launch FiftyOne\nwildata visualize-dataset --dataset wildlife_train --split train\n</code></pre>"},{"location":"tutorials/dataset-preparation/#step-5-export-for-training","title":"Step 5: Export for Training","text":"<pre><code># Export to YOLO format\nwildata dataset export wildlife_train --format yolo --output exports/yolo\n\n# Export to COCO\nwildata dataset export wildlife_train --format coco --output exports/coco\n</code></pre>"},{"location":"tutorials/dataset-preparation/#complete-example","title":"Complete Example","text":"<pre><code>from wildata import DataPipeline\n\n# Initialize\npipeline = DataPipeline(\"data\")\n\n# Import with transformations\nresult = pipeline.import_dataset(\n    source_path=\"annotations.json\",\n    source_format=\"coco\",\n    dataset_name=\"wildlife_train\",\n    transformations={\n        \"enable_tiling\": True,\n        \"tiling\": {\n            \"tile_size\": 800,\n            \"stride\": 640\n        }\n    }\n)\n\n# Export for training\npipeline.export_dataset(\"wildlife_train\", \"yolo\")\n</code></pre> <p>Next Steps: - Model Training Tutorial - WilData Scripts Reference</p>"},{"location":"tutorials/end-to-end-detection/","title":"End-to-End Detection Tutorial","text":"<p>This tutorial walks you through a complete wildlife detection workflow, from images to analysis results.</p>"},{"location":"tutorials/end-to-end-detection/#prerequisites","title":"Prerequisites","text":"<ul> <li>WildDetect installed (Installation Guide)</li> <li>Aerial images with wildlife</li> <li>Trained model or access to MLflow registry</li> <li>MLflow server running (optional but recommended)</li> </ul>"},{"location":"tutorials/end-to-end-detection/#workflow-overview","title":"Workflow Overview","text":"<pre><code>graph LR\n    A[Aerial Images] --&gt; B[Run Detection]\n    B --&gt; C[View Results]\n    C --&gt; D[Analyze]\n    D --&gt; E[Export]\n\n    style A fill:#e3f2fd\n    style C fill:#fff3e0\n    style E fill:#e8f5e9</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-1-prepare-your-environment","title":"Step 1: Prepare Your Environment","text":""},{"location":"tutorials/end-to-end-detection/#11-setup-directory-structure","title":"1.1 Setup Directory Structure","text":"<pre><code>mkdir D:\\wildlife_detection\ncd D:\\wildlife_detection\n\n# Create directories\nmkdir images\nmkdir results\nmkdir models\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#12-organize-your-images","title":"1.2 Organize Your Images","text":"<pre><code>D:\\wildlife_detection\\\n\u251c\u2500\u2500 images\\\n\u2502   \u251c\u2500\u2500 drone_001.jpg\n\u2502   \u251c\u2500\u2500 drone_002.jpg\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 results\\\n\u2514\u2500\u2500 models\\\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#13-start-mlflow-optional","title":"1.3 Start MLflow (Optional)","text":"<pre><code>cd wildetect\nscripts\\launch_mlflow.bat\n</code></pre> <p>Access at: <code>http://localhost:5000</code></p>"},{"location":"tutorials/end-to-end-detection/#step-2-configure-detection","title":"Step 2: Configure Detection","text":""},{"location":"tutorials/end-to-end-detection/#21-create-configuration-file","title":"2.1 Create Configuration File","text":"<p>Create <code>config/my_detection.yaml</code>:</p> <pre><code>model:\n  mlflow_model_name: \"detector\"\n  mlflow_model_alias: \"production\"\n  device: \"cuda\"\n\nimage_dir: \"D:/wildlife_detection/images/\"\n\nprocessing:\n  batch_size: 32\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"simple\"  # or \"raster\" for large images\n  confidence_threshold: 0.5\n\nflight_specs:\n  flight_height: 120.0\n  gsd: 2.38  # Ground Sample Distance (cm/pixel)\n\noutput:\n  directory: \"D:/wildlife_detection/results\"\n  dataset_name: \"my_detections\"  # FiftyOne dataset name\n  save_visualizations: true\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#22-alternative-use-model-file","title":"2.2 Alternative: Use Model File","text":"<p>If not using MLflow:</p> <pre><code>model:\n  model_path: \"D:/wildlife_detection/models/detector.pt\"\n  device: \"cuda\"\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-3-run-detection","title":"Step 3: Run Detection","text":""},{"location":"tutorials/end-to-end-detection/#option-a-using-script","title":"Option A: Using Script","text":"<pre><code>cd wildetect\n\n# Edit config/detection.yaml\nnotepad config\\detection.yaml\n\n# Run\nscripts\\run_detection.bat\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#option-b-using-cli","title":"Option B: Using CLI","text":"<pre><code>wildetect detect D:/wildlife_detection/images/ \\\n    --model detector.pt \\\n    --output D:/wildlife_detection/results/ \\\n    --device cuda \\\n    --batch-size 32\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#option-c-using-python","title":"Option C: Using Python","text":"<pre><code>from wildetect.core.pipeline import DetectionPipeline\nfrom pathlib import Path\n\n# Initialize pipeline\npipeline = DetectionPipeline(\n    model_path=\"detector.pt\",\n    device=\"cuda\"\n)\n\n# Run detection\nimage_dir = Path(\"D:/wildlife_detection/images\")\nresults = pipeline.detect_batch(image_dir)\n\n# Save results\npipeline.save_results(results, \"D:/wildlife_detection/results/detections.json\")\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#expected-output","title":"Expected Output","text":"<pre><code>Processing images: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:45&lt;00:00,  1.11it/s]\nDetection complete!\nResults saved to: D:/wildlife_detection/results/detections.json\nTotal detections: 1,234\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-4-review-results","title":"Step 4: Review Results","text":""},{"location":"tutorials/end-to-end-detection/#41-results-structure","title":"4.1 Results Structure","text":"<pre><code>results/\n\u251c\u2500\u2500 detections.json          # All detections\n\u251c\u2500\u2500 detections.csv          # CSV format\n\u251c\u2500\u2500 summary.txt             # Summary statistics\n\u251c\u2500\u2500 visualizations/         # Annotated images\n\u2502   \u251c\u2500\u2500 drone_001.jpg\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 fiftyone/              # FiftyOne dataset (if enabled)\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#42-detection-json-format","title":"4.2 Detection JSON Format","text":"<pre><code>{\n  \"image_path\": \"D:/wildlife_detection/images/drone_001.jpg\",\n  \"image_size\": [1920, 1080],\n  \"processing_time\": 0.5,\n  \"detections\": [\n    {\n      \"class_name\": \"elephant\",\n      \"confidence\": 0.95,\n      \"bbox\": [100, 200, 150, 180],\n      \"bbox_normalized\": [0.052, 0.185, 0.078, 0.167]\n    }\n  ]\n}\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-5-visualize-results","title":"Step 5: Visualize Results","text":""},{"location":"tutorials/end-to-end-detection/#option-a-using-fiftyone","title":"Option A: Using FiftyOne","text":"<pre><code># Launch FiftyOne\ncd wildetect\nscripts\\launch_fiftyone.bat\n\n# Or with CLI\nwildetect fiftyone --action launch --dataset my_detections\n</code></pre> <p>Features: - Interactive viewing - Filtering by confidence - Filtering by species - Export capabilities</p>"},{"location":"tutorials/end-to-end-detection/#option-b-view-saved-visualizations","title":"Option B: View Saved Visualizations","text":"<pre><code># Open visualizations folder\nexplorer D:\\wildlife_detection\\results\\visualizations\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#option-c-using-web-ui","title":"Option C: Using Web UI","text":"<pre><code>cd wildetect\nscripts\\launch_ui.bat\n</code></pre> <p>Navigate to results viewer.</p>"},{"location":"tutorials/end-to-end-detection/#step-6-analyze-results","title":"Step 6: Analyze Results","text":""},{"location":"tutorials/end-to-end-detection/#61-basic-statistics","title":"6.1 Basic Statistics","text":"<pre><code>wildetect analyze D:/wildlife_detection/results/detections.json \\\n    --output D:/wildlife_detection/results/analysis/\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#62-python-analysis","title":"6.2 Python Analysis","text":"<pre><code>import json\nimport pandas as pd\nfrom collections import Counter\n\n# Load results\nwith open(\"results/detections.json\") as f:\n    results = json.load(f)\n\n# Count by species\nspecies_counts = Counter()\nfor result in results:\n    for det in result[\"detections\"]:\n        species_counts[det[\"class_name\"]] += 1\n\nprint(\"Species Counts:\")\nfor species, count in species_counts.items():\n    print(f\"  {species}: {count}\")\n\n# Calculate average confidence\nconfidences = []\nfor result in results:\n    for det in result[\"detections\"]:\n        confidences.append(det[\"confidence\"])\n\nprint(f\"\\nAverage Confidence: {sum(confidences)/len(confidences):.2f}\")\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#63-generate-report","title":"6.3 Generate Report","text":"<pre><code>from wildetect.analysis import ReportGenerator\n\ngenerator = ReportGenerator(results_path=\"results/detections.json\")\nreport = generator.generate_report(\n    output_path=\"results/report.pdf\",\n    include_maps=True,\n    include_statistics=True\n)\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-7-filter-and-refine","title":"Step 7: Filter and Refine","text":""},{"location":"tutorials/end-to-end-detection/#71-filter-by-confidence","title":"7.1 Filter by Confidence","text":"<pre><code># Filter detections by confidence threshold\nfiltered_results = []\nconfidence_threshold = 0.7\n\nfor result in results:\n    filtered_dets = [\n        det for det in result[\"detections\"]\n        if det[\"confidence\"] &gt;= confidence_threshold\n    ]\n    if filtered_dets:\n        filtered_results.append({\n            **result,\n            \"detections\": filtered_dets\n        })\n\n# Save filtered results\nwith open(\"results/filtered_detections.json\", \"w\") as f:\n    json.dump(filtered_results, f, indent=2)\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#72-filter-by-species","title":"7.2 Filter by Species","text":"<pre><code># Keep only specific species\ntarget_species = [\"elephant\", \"giraffe\"]\n\nspecies_results = []\nfor result in results:\n    species_dets = [\n        det for det in result[\"detections\"]\n        if det[\"class_name\"] in target_species\n    ]\n    if species_dets:\n        species_results.append({\n            **result,\n            \"detections\": species_dets\n        })\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-8-export-results","title":"Step 8: Export Results","text":""},{"location":"tutorials/end-to-end-detection/#81-export-to-csv","title":"8.1 Export to CSV","text":"<pre><code>import csv\n\n# Convert to CSV\ncsv_data = []\nfor result in results:\n    for det in result[\"detections\"]:\n        csv_data.append({\n            \"image\": result[\"image_path\"],\n            \"species\": det[\"class_name\"],\n            \"confidence\": det[\"confidence\"],\n            \"x\": det[\"bbox\"][0],\n            \"y\": det[\"bbox\"][1],\n            \"width\": det[\"bbox\"][2],\n            \"height\": det[\"bbox\"][3]\n        })\n\n# Save CSV\nwith open(\"results/detections.csv\", \"w\", newline=\"\") as f:\n    writer = csv.DictWriter(f, fieldnames=csv_data[0].keys())\n    writer.writeheader()\n    writer.writerows(csv_data)\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#82-export-to-excel","title":"8.2 Export to Excel","text":"<pre><code>import pandas as pd\n\ndf = pd.DataFrame(csv_data)\n\n# Create Excel with multiple sheets\nwith pd.ExcelWriter(\"results/detections.xlsx\") as writer:\n    df.to_excel(writer, sheet_name=\"All Detections\", index=False)\n\n    # Summary by species\n    summary = df.groupby(\"species\").agg({\n        \"confidence\": [\"count\", \"mean\"]\n    })\n    summary.to_excel(writer, sheet_name=\"Summary\")\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#83-export-to-coco-format","title":"8.3 Export to COCO Format","text":"<pre><code>from wildetect.export import COCOExporter\n\nexporter = COCOExporter(results)\nexporter.export(\"results/detections_coco.json\")\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#step-9-advanced-processing","title":"Step 9: Advanced Processing","text":""},{"location":"tutorials/end-to-end-detection/#91-large-raster-detection","title":"9.1 Large Raster Detection","text":"<p>For large GeoTIFF files:</p> <pre><code># config/raster_detection.yaml\nmodel:\n  mlflow_model_name: \"detector\"\n  device: \"cuda\"\n\nimage_paths:\n  - \"D:/orthomosaics/large_ortho.tif\"\n\nprocessing:\n  tile_size: 800\n  overlap_ratio: 0.2\n  pipeline_type: \"raster\"\n  nms_threshold: 0.5\n\nflight_specs:\n  gsd: 2.38  # Required for raster detection\n\noutput:\n  directory: \"results/raster\"\n</code></pre> <pre><code>wildetect detect --config config/raster_detection.yaml\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#92-batch-processing-multiple-folders","title":"9.2 Batch Processing Multiple Folders","text":"<pre><code>from pathlib import Path\n\n# Process multiple folders\nfolders = [\n    \"D:/surveys/site_a/\",\n    \"D:/surveys/site_b/\",\n    \"D:/surveys/site_c/\"\n]\n\nfor folder in folders:\n    folder_name = Path(folder).name\n    results = pipeline.detect_batch(folder)\n    pipeline.save_results(results, f\"results/{folder_name}_detections.json\")\n</code></pre>"},{"location":"tutorials/end-to-end-detection/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/end-to-end-detection/#detection-is-slow","title":"Detection is Slow","text":"<p>Solutions: 1. Increase batch size (if GPU memory allows) 2. Use smaller tile size 3. Enable GPU acceleration 4. Use multithreaded pipeline</p>"},{"location":"tutorials/end-to-end-detection/#out-of-memory","title":"Out of Memory","text":"<p>Solutions: 1. Reduce batch size 2. Reduce tile size 3. Use CPU instead of GPU 4. Close other applications</p>"},{"location":"tutorials/end-to-end-detection/#low-detection-accuracy","title":"Low Detection Accuracy","text":"<p>Solutions: 1. Check model is appropriate for your data 2. Adjust confidence threshold 3. Verify image quality 4. Check GSD matches training data</p>"},{"location":"tutorials/end-to-end-detection/#model-wont-load","title":"Model Won't Load","text":"<p>Solutions: 1. Verify MLflow server is running 2. Check model name and alias 3. Verify model path if using file 4. Check CUDA availability</p>"},{"location":"tutorials/end-to-end-detection/#next-steps","title":"Next Steps","text":"<ul> <li>Census Campaign Tutorial - Run a full census</li> <li>Dataset Preparation - Prepare your own training data</li> <li>Model Training - Train custom models</li> <li>Geographic Visualization - Create maps</li> </ul>"},{"location":"tutorials/end-to-end-detection/#complete-example-script","title":"Complete Example Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nComplete detection workflow example\n\"\"\"\nfrom pathlib import Path\nfrom wildetect.core.pipeline import DetectionPipeline\nimport json\n\ndef main():\n    # Configuration\n    image_dir = Path(\"D:/wildlife_detection/images\")\n    output_dir = Path(\"D:/wildlife_detection/results\")\n    model_path = \"detector.pt\"\n\n    # Initialize pipeline\n    print(\"Initializing detection pipeline...\")\n    pipeline = DetectionPipeline(\n        model_path=model_path,\n        device=\"cuda\"\n    )\n\n    # Run detection\n    print(f\"Processing images in {image_dir}...\")\n    results = pipeline.detect_batch(image_dir)\n\n    # Save results\n    output_dir.mkdir(exist_ok=True)\n    results_file = output_dir / \"detections.json\"\n    pipeline.save_results(results, results_file)\n    print(f\"Results saved to {results_file}\")\n\n    # Print summary\n    total_detections = sum(len(r[\"detections\"]) for r in results)\n    print(f\"\\nSummary:\")\n    print(f\"  Images processed: {len(results)}\")\n    print(f\"  Total detections: {total_detections}\")\n\n    # Species breakdown\n    from collections import Counter\n    species = Counter()\n    for result in results:\n        for det in result[\"detections\"]:\n            species[det[\"class_name\"]] += 1\n\n    print(f\"\\nSpecies breakdown:\")\n    for name, count in species.items():\n        print(f\"  {name}: {count}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Save as <code>run_detection.py</code> and run:</p> <pre><code>python run_detection.py\n</code></pre> <p>Congratulations! You've completed the end-to-end detection tutorial. You now know how to run detection, visualize results, and export data for further analysis.</p>"},{"location":"tutorials/model-training/","title":"Model Training Tutorial","text":"<p>Learn how to train detection and classification models using WildTrain.</p>"},{"location":"tutorials/model-training/#prerequisites","title":"Prerequisites","text":"<ul> <li>WildTrain installed</li> <li>Prepared dataset (see Dataset Preparation)</li> <li>MLflow server running</li> </ul>"},{"location":"tutorials/model-training/#training-a-yolo-detector","title":"Training a YOLO Detector","text":""},{"location":"tutorials/model-training/#step-1-prepare-data","title":"Step 1: Prepare Data","text":"<p>Ensure YOLO format:</p> <pre><code>data/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 val/\n\u251c\u2500\u2500 labels/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 val/\n\u2514\u2500\u2500 data.yaml\n</code></pre> <p><code>data.yaml</code>: <pre><code>train: ./images/train\nval: ./images/val\nnc: 3\nnames: ['elephant', 'giraffe', 'zebra']\n</code></pre></p>"},{"location":"tutorials/model-training/#step-2-configure-training","title":"Step 2: Configure Training","text":"<p>Create <code>configs/yolo_train.yaml</code>:</p> <pre><code>model:\n  framework: \"yolo\"\n  size: \"n\"  # n, s, m, l, x\n  pretrained: true\n\ndata:\n  data_yaml: \"D:/data/wildlife/data.yaml\"\n\ntraining:\n  epochs: 100\n  imgsz: 640\n  batch: 16\n  device: 0\n\nmlflow:\n  experiment_name: \"yolo_wildlife\"\n</code></pre>"},{"location":"tutorials/model-training/#step-3-train","title":"Step 3: Train","text":"<pre><code>cd wildtrain\nscripts\\train_yolo.bat\n\n# Or with CLI\nwildtrain train detector -c configs/yolo_train.yaml\n</code></pre>"},{"location":"tutorials/model-training/#step-4-evaluate","title":"Step 4: Evaluate","text":"<pre><code>wildtrain eval detector -c configs/yolo_eval.yaml\n</code></pre>"},{"location":"tutorials/model-training/#step-5-register-model","title":"Step 5: Register Model","text":"<pre><code>wildtrain register detector configs/registration/detector_registration.yaml\n</code></pre>"},{"location":"tutorials/model-training/#training-a-classifier","title":"Training a Classifier","text":""},{"location":"tutorials/model-training/#step-1-prepare-roi-dataset","title":"Step 1: Prepare ROI Dataset","text":"<p>Use WilData to create ROI dataset from detections.</p>"},{"location":"tutorials/model-training/#step-2-configure","title":"Step 2: Configure","text":"<p><code>configs/classification_train.yaml</code>:</p> <pre><code>model:\n  architecture: \"resnet50\"\n  num_classes: 10\n  pretrained: true\n  learning_rate: 0.001\n\ndata:\n  root_data_directory: \"D:/data/roi_dataset\"\n  batch_size: 32\n\ntraining:\n  max_epochs: 100\n  accelerator: \"gpu\"\n</code></pre>"},{"location":"tutorials/model-training/#step-3-train_1","title":"Step 3: Train","text":"<pre><code>cd wildtrain\nscripts\\train_classifier.bat\n</code></pre>"},{"location":"tutorials/model-training/#monitor-with-mlflow","title":"Monitor with MLflow","text":"<pre><code># Start MLflow\nscripts\\launch_mlflow.bat\n\n# Access at http://localhost:5000\n</code></pre> <p>View: - Training metrics - Model performance - Hyperparameters - Artifacts</p>"},{"location":"tutorials/model-training/#complete-python-example","title":"Complete Python Example","text":"<pre><code>from wildtrain import Trainer\n\n# Configure\nconfig = Trainer.load_config(\"configs/yolo_train.yaml\")\n\n# Train\ntrainer = Trainer(config)\nmodel = trainer.train()\n\n# Evaluate\nmetrics = trainer.evaluate()\n\n# Register\ntrainer.register_model(\n    model_name=\"wildlife_detector\",\n    tags={\"dataset\": \"wildlife_v1\"}\n)\n</code></pre> <p>Next Steps: - End-to-End Detection - WildTrain Scripts</p>"}]}